\documentclass[a4paper,11pt]{amsbook}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm,amssymb,latexsym}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{a4wide}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\renewcommand{\familydefault}{bch}
\renewcommand{\baselinestretch}{1.15}
\usepackage{lipsum}
\usepackage{cancel}
\usepackage{array}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage{xifthen}
\usepackage{xcolor}
\usepackage{stackrel}
\usepackage{diagbox}
\usepackage{hyperref}

%%% Color Section Headers
\usepackage{enumitem}
\definecolor{darkblue}{cmyk}{.9,.7,.3,.15}
\definecolor{darkred}{cmyk}{.3,.9,.80,.2}

%%% Section headers
\makeatletter
\def\section{\@startsection{section}{2}%
    \z@{1\linespacing\@plus1\linespacing}{.5\linespacing}%
    {\large\normalfont\bfseries\centering\color{darkblue}}}
\makeatother

%%% Subsection headers
\makeatletter
\def\subsection{\@startsection{subsection}{2}%
    \z@{.5\linespacing\@plus.7\linespacing}{-.5em}%
    {\normalfont\bfseries\color{darkblue}}}
\makeatother

%%% Subsubsection headers
\makeatletter
\def\subsubsection{\@startsection{subsubsection}{2}%
    \z@{.5\linespacing\@plus.7\linespacing}{-.5em}%
    {\normalfont\itshape\color{darkblue}}}
\makeatother

\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
    \pushQED{\qed}%
    \normalfont \topsep6\p@\@plus6\p@\relax
    \trivlist
    \itemindent\z@ % original has \normalparindent
    \item[\hskip\labelsep
          \scshape
      #1\@addpunct{.}]\ignorespaces
}{%
    \popQED\endtrivlist\@endpefalse
}
\makeatother

%%% \widebar implementation
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{%
    \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
    \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
    \ifdim\ht0=\ht2 #3\else #2\fi
    }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
    \begingroup
    \def\mathaccent##1##2{%
%Enable nesting of accents:
    \let\mathaccent\save@mathaccent
%If there's more than a single symbol, use the first character instead (see below):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = (italic correction / 3) - (Breite / 10)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
        \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
        \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern (-\dimen@) if it is >0 or if a superscript follows:
        \let\final@kern#2%
        \ifdim\dimen@<\z@ \let\final@kern1\fi
        \if\final@kern1 \kern-\dimen@\fi
    \else
        \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
}%
    \macc@depth\@ne
    \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
    \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
    \macc@set@skewchar\relax
    \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
    \if#31
        \macc@nested@a\relax111{#1}%
    \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
        \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
\fi
\endgroup
}
\makeatother


%%% Theorems, Definitions and Proofs
\newtheorem{theorem}{\hspace{-2em} \color{darkblue} Theorem}[chapter]
\newtheorem{proposition}{\hspace{-2em} \color{darkblue} Proposition}[chapter]
\newtheorem{lemma}{\hspace{-2em} \color{darkblue} Lemma}[chapter]
\newtheorem{corollary}{\hspace{-2em} \color{darkblue} Corollary}[chapter]
\newtheorem{conjecture}{\hspace{-2em} \color{darkblue} Conjecture}[chapter]
\newtheorem{claim}{\hspace{-2em} \color{darkblue} Claim}[chapter]
\newtheorem{fact}{\hspace{-2em} \color{darkblue} Fact}[chapter]
\renewcommand{\thetheorem}{\arabic{theorem}}
\renewcommand{\theproposition}{\arabic{proposition}}
\renewcommand{\thelemma}{\arabic{lemma}}
\renewcommand{\thecorollary}{\arabic{corollary}}
\renewcommand{\theconjecture}{\arabic{conjecture}}
\renewcommand{\theclaim}{\arabic{claim}}
\renewcommand{\thefact}{\arabic{fact}}

\theoremstyle{definition}
\newtheorem{definition}{\hspace{-2em} \color{darkblue} Definition}[chapter]
\newtheorem{example}{\hspace{-2em} \color{darkblue} Example}[chapter]
\newtheorem{exercise}{\hspace{-2em} \color{darkblue} Exercise}[chapter]
\newtheorem{question}{\hspace{-2em} \color{darkblue} Question}[chapter]
\renewcommand{\thedefinition}{\arabic{definition}}
\renewcommand{\theexample}{\arabic{example}}
\renewcommand{\theexercise}{\arabic{exercise}}
\renewcommand{\thequestion}{\arabic{question}}

\theoremstyle{remark}
\newtheorem{remark}{\hspace{-2em} \color{darkblue} Remark}[chapter]
\renewcommand{\theremark}{\arabic{remark}}

\newcommand{\mref}{{\color{darkred} \textbf{[missing]}}} % missing reference
\newcommand{\mcontent}{{\color{darkred} \textbf{MISSING CONTENT}}} % missing content

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Qbar}{\overline{\mathbb{Q}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Zbar}{\overline{\mathbb{Z}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\rk}{rank}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\kr}{Ker}
\DeclareMathOperator\Aut{Aut}
\DeclareMathOperator\Ind{Ind}
\DeclareMathOperator\End{End}
\DeclareMathOperator\Hom{Hom}
\DeclareMathOperator\chr{char}
\DeclareMathOperator\Spec{Spec}
\DeclareMathOperator\Var{Var}
\DeclareMathOperator\Cov{Cov}
% \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% \DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
% \DeclarePairedDelimiter{\abs}{|}{|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\Id}{\text{Id}}
\newcommand{\ch}{\text{ch}}
\newcommand{\m}{\text{m}}
\newcommand{\ord}{\text{ord}}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\PSL}{\text{PSL}}
\newcommand\sgn{\text{sgn}}
\newcommand\triv{\text{triv}}
\newcommand\der[1]{#1^{\text{der}}}
\renewcommand\mod{\ \text{mod}\ }
\newcommand\Zn[1]{\Z / #1 \Z}
\newcommand\inc\subseteq
\newcommand\cni\supseteq
\newcommand\exc\setminus
\newcommand\divs{\mathbin{|}}
\newcommand\notdivs{\mathbin{\not|}}
\newcommand\inj{\xhookrightarrow{}}
\newcommand\surj{\twoheadrightarrow}
\newcommand\imp\Rightarrow
\newcommand\eq\Leftrightarrow
\newcommand\contradiction{ _{\ \Rightarrow\!\Leftarrow}}
\newcommand\Wlog{\text{W.L.O.G.}}
\newcommand\tensor[1]{\stackrel[#1]{}{\otimes}}
\newcommand\sto\rightsquigarrow
\newcommand\overtext[2]{%
  \mathrel{\vbox{\offinterlineskip\ialign{%
    \hfil##\hfil\cr
    \normalfont\tiny\text{#1}\cr
    \noalign{\kern4pt}
    $#2$\cr
}}}}
\newcommand\0{\varnothing}
\newcommand\dx[1][]{\ifthenelse{\isempty{#1}}{\,}{}\mathrm{d}x}
\newcommand\dy[1][]{\ifthenelse{\isempty{#1}}{\,}{}\mathrm{d}y}
\newcommand\dz[1][]{\ifthenelse{\isempty{#1}}{\,}{}\mathrm{d}z}
\newcommand\dt[1][t]{\,\mathrm{d}#1}
\newcommand\Bin{\text{Bin}}
\newcommand\Ber{\text{Ber}}
\newcommand\Poi{\text{Poi}}
\newcommand\Geom{\text{Geom}}
\newcommand\NB{\text{NB}}
\newcommand\Unif{\text{Unif}}
\newcommand\Exp{\text{Exp}}

% \set[condition]{set}
\newcommand\set[2][]
{
    \ifthenelse{\isempty{#1}}
    {
        \left\{#2\right\}
    }
    {
        \left\{#2\left|\,#1\right.\right\}
    }
}
\newcommand\fset[2][]
{
    \ifthenelse{\isempty{#1}}
    {
        \left\{#2\right\}
    }
    {
        \left\{\left.#2\,\right|#1\right\}
    }
}

% \diff[function]{variable}
\newcommand\diff[2][]
{
    \frac{\mathrm{d}#1}{\mathrm{d}#2}
}

% \pdiff[function]{variable}
\newcommand\pdiff[2][]
{
    \frac{\partial#1}{\partial#2}
}

\setenumerate{label=(\alph*)}

% \disj and \bigdisj 
\newcommand\Disj
{
    \setbox0\hbox{$\cup$}
    \rlap{\hbox to \wd0{\hss\raisebox{3.5pt}{$\cdot$}\hss}}\box0
}
\newcommand\disj
{
    \mathbin{\mathchoice{\Disj}{\Disj}{\scalebox{0.5}{$\Disj$}}{\scalebox{0.4}{$\Disj$}}}
}
\newcommand\BigDisj
{
    \setbox0\hbox{$\displaystyle\bigcup$}
    \rlap{\hbox to \wd0{\hss\raisebox{7.8pt}{$\cdot$}\hss}}\box0
}
\newcommand\bigdisj
{
    \mathop{\mathchoice{\BigDisj}{\scalebox{0.7}{$\BigDisj$}}{\scalebox{0.5}{$\BigDisj$}}{\scalebox{0.4}{$\BigDisj$}}}
}

%%% Further Styling
% \newcommand\Tstrut{\rule{0pt}{2.7ex}}         
% \newcommand\Bstrut{\rule[-1.3ex]{0pt}{0pt}} 
% \newcommand{\SP}{\vspace{0.2cm}\par}
% \newcommand{\SPP}{\vspace{0.05cm}\par}  

\title{Introduction to Probability Theory}

\author[]{Lecturer: Shagnik Das \\ Scribe: Jenny} %%% ENTER YOUR NAME HERE %%%

\hypersetup{
    pdfborder={0 0 0}
}

\begin{document}


\begin{abstract}
These are notes from the course `Introduction to Probability Theory,' given by Shagnik Das in the Spring Semester of 2025 at the National Taiwan University. Nobody bears any responsibility for mistakes in the script, but I would be grateful to be notified of any errors found.

This note lacks content of the lecture on 2/18, 2/20, 4/17, and 4/22.
\end{abstract}

\clearpage\maketitle

\tableofcontents
\clearpage
\hypersetup{
    pdfborder={0 0 1}
}

\chapter{Axioms of Probability}

    Given a sample space $S$, 
    \begin{enumerate}[label=(\arabic*)]
        \item For any event $E\inc S$, $0\leq\P(E)\leq1$.
        \item $\P(S)=1$.
        \item For mutually exclusive events $E_1,E_2,\ldots$, $\P\left(\bigcup_{i=1}^\infty E_i\right)=\sum_{i=1}^\infty\P(E_i)$. 
    \end{enumerate}
    
    Define $\varnothing=\left\{\right\}$ as the empty set.\\

    \begin{claim}
        $\P(\varnothing)=0$.
    \end{claim}
    \begin{proof}
        Consider the sequence of events $E_1=S$, $E_2=\varnothing$ for all $i\geq2$.
        These events are mutually exclusive.
        By Axiom 3, $$\P\left(\bigcup_{i=1}^\infty E_i\right)=\sum_{i=1}^\infty\P(E_i).$$
        $$\bigcup_{i=1}^\infty E_i=S\cup\varnothing\cup\varnothing\cup\cdots=S$$ 
        $$\P(S)=\sum_{i=1}^\infty\P(E_i)=\P(S)+\sum_{i=2}^\infty\P(\varnothing)$$
        $$\Rightarrow\sum_{i=2}^\infty\P(\varnothing)\Rightarrow\P(\varnothing)=0$$
    \end{proof}

    \begin{corollary} \label{1.1}
        For any finite sequence of mutually exclusive events $E_1,E_2,\ldots,E_n$, $$\P\left(\bigcup_{i=1}^n E_i\right)=\sum_{i=1}^n\P(E_i).$$
    \end{corollary}
    \begin{proof}
        Extend to an infinite sequence of exclusive events by adding  the empty set $E_i=\varnothing$ for all $i\geq n+1$.
        Then $\bigcup_{i=1}^n E_i=\bigcup_{i=1}^\infty E_i$.

        By Axiom 3, \begin{align*}
            \P\left(\bigcup_{i=1}^n E_i\right)&=\P\left( \bigcup_{i=1}^\infty E_i \right) \\
            &=\sum_{i=1}^n\P(E_i)+\sum_{i=n+1}^\infty\P(\varnothing) \\
            &=\sum_{i=1}^n\P(E_i) \tag{since $\P(\varnothing)=0$}
        \end{align*}
    \end{proof}

    \begin{proposition}
        Given a probability space $(S,\P)$, where $S$ is the \emph{sample space} and $\P$ is the \emph{probability function},
        we have $$\P(E^c)=1-\P(E).$$
    \end{proposition}
    \begin{proof}
        Note that \begin{itemize}
            \item $E\cap E^c=\varnothing$
            \item $E\cup E^c=S$
        \end{itemize}

        By Corollary, $1=\P(S)=\P(E\cup E^c)=\P(E)+\P(E^c)$. %Axiom 2
    \end{proof}

    \begin{proposition}
        Given a probability space $(S,\P)$, and nested sets $E\inc F\inc S$, then $\P(E)\leq\P(F)$.
    \end{proposition}
    \begin{proof}
        Venn diagrams
        \begin{center}
            \begin{tikzpicture}[fill=gray]
                \draw (0,0) circle (0.5) (0.5,0)  node [text=black,right] {$E$}
                      (0.5,0) ellipse (1.5 and 1) (2,0.7)  node [text=black,above] {$F$}
                      (-2,-1.5) rectangle (3,1.5) node [text=black,above] {$S$};
            \end{tikzpicture}
        \end{center}

        Note that $E\cap F=E$ and $E^c\cap F$ are exclusive events ($E\cap(E^c\cap F)=(E\cap E^c)\cap F=\varnothing\cap F=\varnothing$),
        and $(E\cap F)\cup(E^c\cap F)=(E\cup E^c)\cap F=S\cap F=F$.

        By Corollary, $\P(F)=\P(E\cap F)+\P(E^c\cap F)=\P(E)+\P(E^c\cap F)\geq\P(E)$.
    \end{proof}

    \begin{example}
        Rolling a fair six-sided dice.
        $$\Rightarrow\P(\text{rolling a 6})\leq\P(\text{rolling an even number})$$
    \end{example}

    For arbitrary events, we observe: \begin{center}
        \begin{tikzpicture}
            \draw (-0.5,0) ellipse (2 and 1) (-2,1)  node [text=black,above] {$E$}
                  (1.5,0) ellipse (2 and 1) (3,1)  node [text=black,above] {$F$};
        \end{tikzpicture}
    \end{center}
    
    \begin{proposition}
        In a probability space $(S,\P)$, given any events $E,F\inc S$, $$\P(E\cup F)=\P(E)+\P(F)-\P(E\cap F).$$
    \end{proposition}

    \begin{corollary}[Union bound] $\P(E\cup F)\leq\P(E)+\P(F)$.
    \end{corollary}
    \begin{proof} (Cor.)
        $\P(E\cup F)=\P(E)+\P(F)-\P(E\cap F)\leq\P(E)+\P(F)$
    \end{proof}

    \begin{proof} (Prop.)
        \begin{center}
            \begin{tikzpicture}
                \draw (-0.5,0) ellipse (2 and 1) (-2,1)  node [text=black,above] {$E$}
                      (1.5,0) ellipse (2 and 1) (3,1)  node [text=black,above] {$F$};
                \node at (0.5,0){$E\cap F$};
                \node at (-1.5,0) {$E\cap F^C$};
                \node at (2.5,0) {$E^C\cap F$};
            \end{tikzpicture}
        \end{center}

        We have unions of exclusive events
        \begin{itemize}
            \item $E\cup F=(E\cap F^c)\disj(E\cap F)\disj(E^c\cap F)$
            \item $E=(E\cap F^c)\disj(E\cap F)$, $F=(E\cap F)\disj(E^c\cap F)$
        \end{itemize}

        By Corollary \ref{1.1}, \begin{itemize}
            \item $\P(E\cup F)=\P(E\cap F^c)+\P(E\cap F)+\F(E^c\cap F)$
            \item $\P(E)=\P(E\cap F^c)+\P(E\cap F)$
            \item $\P(F)=\P(E\cap F)+\P(E^c\cap F)$
        \end{itemize}
        \begin{align*}
            \Rightarrow\P(E)+\P(F)&=\P(E\cap F^c)+\P(E\cap F)+\P(E\cap F)+\P(E^c\cap F)\\
            &=\P(E\cap F^c)+\P(E\cap F)+\P(E^c\cap F)+\P(E\cap F) \\
            &=\P(E\cup F)+\P(E\cap F)
        \end{align*}
    \end{proof}

    \begin{example}
        Play a game against Real Madrid.
        \begin{itemize}
            \item $\P(\text{Mbapp\'e scores})=0.5$ 
            \item $\P(\text{Vinicius scores})=0.4$
            \item $\P(\text{Mbapp\'e and Vinicius both scores})=0.2$
        \end{itemize}
        \underline{Q.} $\P(\text{Mbapp\'e or Vinicius scores})=\text{?}$\\[5pt]
        \textbf{Solution.} Define events \begin{itemize}
            \item $E=\left\{\text{Mbapp\'e scores}\right\}$ 
            \item $F=\left\{\text{Vinicius scores}\right\}$
        \end{itemize}
        $$\P(E)=0.5,\P(F)=0.4,\P(E\cap F)=0.2$$
        $$\overtext{Prop 3}{\Rightarrow}\;\P(E\cup F)=\P(E)+\P(F)-\P(E\cap F)=0.7$$
        $$\overtext{Prop 1}{\Rightarrow}\;\P(E^c\cap F^c)=\P((E\cup F)^c)=1-\P(E\cap F)=0.3$$
    \end{example}
    \underline{Q.} What can we say about $\P(E\cup F\cup G)$? \begin{align*}
        &\P(E\cup F\cup G) \\
        &=\P((E\cup F)\cup G) \\
        &=\P(E\cup F)+\P(G)-\P((E\cup F)\cap G) \\
        &=\P(E)+\P(F)-\P(E\cap F)+\P(G)-\P((E\cup F)\cap G)
    \end{align*}     
    where \begin{align*}
        \P((E\cup F)\cap G)&=\P((E\cap G)\cup(F\cap G)) \\ 
        &=\P(E\cap G)+\P(F\cap G)-\P((E\cap G)\cap(F\cap G)) \\
        &=\P(E\cap G)+\P(F\cap G)-\P(E\cap F\cap G)
    \end{align*}

    Therefore $$\P(E\cup F\cup G)=\P(E)+\P(F)+\P(G)-\P(E\cap F)-\P(E\cap G)-\P(F\cap G)+\P(E\cap F\cap G).$$

    \begin{example}
        Roll a 60-sided dice. $\P(\text{roll in divisible by 2, 3, or 5})$?\\
        \textbf{Solution.} Let $E=\left\{\text{div. by 2}\right\}$, $F=\left\{\text{div. by 3}\right\}$, $G=\left\{\text{div. by 5}\right\}$.
        $$\P(E)=\frac{\#\text{even numbers in }1,2,\ldots,60}{60}=\frac{30}{60}=\frac12.$$
        $$\P(F)=\frac13,\quad\P(G)=\frac15.$$
        \begin{align*}
            \P(E\cap F)&=\P(\text{div by 2 of div by 3}) \\
            &=\P(\text{div by 6})=\frac16 \\
            \P(E\cap G)&=\P(\text{div by 10})=\frac1{10} \\
            \P(F\cap G)&=\P(\text{div by 15})=\frac1{15} \\
            \P(E\cap F\cap G)&=\P(\text{div by 30})=\frac1{30}
        \end{align*}
        \begin{align*}
            &\P(E\cup F\cup G) \\
            &=\P(E)+\P(F)+\P(G)-\P(E\cap F)-\P(E\cap G)-\P(F\cap G)+\P(E\cap F\cap G) \\
            &=\frac12+\frac13+\frac15-\frac16-\frac1{10}-\frac1{15}+\frac1{30}=\frac{22}{30}
        \end{align*}
    \end{example}

\chapter{Inclusion-Exclusion} 
    
    \section{Inclusion-Exclusion Formula}
    
    What is $\P\left(\bigcup_{i=1}^n E_i\right)$?

    Use induction, we can get \begin{align*}
        \P\left(\bigcup_{i=1}^n E_i\right)&=\P\left(\left( \bigcup_{i=1}^{n-1} E_i \right)\cup E_n\right) \\
        &=\sum_{i=1}^n\P(E_i)-\sum_{1\leq i_1<i_2\leq n}\P(E_{i_1}\cap E_{i_2})+\sum_{i_1<i_2<i_3}\P(E_{i_1}\cap E_{i_2}\cap E_{i_3})-\cdots
    \end{align*}

    Formally, $$\P\left(\bigcup_{i=1}^n E_i\right)=\sum_{r=1}^n\sum_{1\leq i_1<i_2<\cdots<i_r\leq n}(-1)^{r+1}\P\left(\bigcap_{j=1}^r E_{i_j}\right).$$
    \begin{proof} (Inclusion-Exclusion Formula)
        \begin{center}
            \begin{tikzpicture}
                \draw (0,1) circle (1) (-1,1)  node [text=black,left] {$E_1$}
                      (1,1) circle (1) (2,1)  node [text=black,right] {$E_2$}
                      (0.5,0) circle (1) (1,-1)  node [text=black,right] {$E_3$}
                      (-2,-1.5) rectangle (3,2.5);
            \end{tikzpicture}
        \end{center}

        We can write all the events as mutually exclusive unions
        $$E_I=\left(\bigcap_{i\in I}E_i\right)\cap\left(\bigcap_{i\notin I}E_i^C\right)\text{ for }I\inc\left[n\right].$$
        $$E_I=\left\{\text{outcomes where }E_i\text{ happens }\iff i\in I\right\}$$

        For example, $\bigcup_{i=1}^n E_i=\bigdisj_{I:I\neq\varnothing}E_I$.
        $$\Rightarrow\P\left(\bigcup_{i=1}^n E_i\right)=\sum_{I\neq\varnothing}\P(E_I)\quad(*)$$

        Given every $J\inc\left[n\right]$, $\P\left(\bigcap_{j\inc J} E_j\right)$,
        $\bigcap_{j\inc J} E_j=\bigdisj_{I:J\inc I}E_I$.

        RHS: \begin{align*}
            &\sum_{r=1}^n\sum_{\substack{J\inc\left[n\right] \\ |J|=r}}(-1)^{r+1}\P\left(\bigcap_{j\inc J} E_j\right) \\
            &=\sum_{r=1}^n\sum_{\substack{J\inc\left[n\right] \\ |J|=r}}(-1)^{r+1}\P\left(\bigdisj_{I:J\inc I}E_I\right) \\
            &=\sum_{r=1}^n(-1)^{r+1}\sum_{\substack{J\inc\left[n\right]|J|=r}}\sum_{I:J\inc I}\P\left(E_I\right) \tag{mutually exclusive} \\
            &=\sum_{\substack{I\inc\left[n\right] \\ I\neq\varnothing}}\left(\sum_{r=1}^n\sum_{\substack{J\inc\left[n\right] \\ |J|=r}}(-1)^{r+1}\right)\P(E_I)
        \end{align*}

        Recall that the number of choices of $J$, $J\inc I$, $|J|=r$ is $\binom{|I|}r$.
        \begin{align*}
            \Rightarrow\sum_{r=1}^n\sum_{\substack{J\inc\left[n\right] \\ |J|=r}}(-1)^{r+1}
            &=\sum_{r=1}^n\binom{|I|}r(-1)^{r+1} \\
            &=\sum_{r=1}^{|I|}\binom{|I|}r(-1)^{r+1} \\
            &=\sum_{r=0}^{|I|}\binom{|I|}r(-1)^{r+1}-\binom{|I|}0(-1)^{0+1} \\
            &=-\sum_{r=0}^{|I|}\binom{|I|}r(-1)^r-(-1) \\
            &=-(-1+1)^{|I|}+1=1 \tag{binomial thm}
        \end{align*}
        \begin{align*}
            \therefore&\sum_{r=1}^n(-1)^{r+1}\sum_{\substack{J\inc\left[n\right] \\ |J|=r}}\sum_{I:J\inc I}\P\left(E_I\right) \\
            &=\sum_{\substack{I\inc\left[n\right] \\ I\neq\varnothing}}1\cdot\P(E_I) \\ 
            &=\P\left(\bigcup_{i=1}^nE_i\right) \tag{*}
        \end{align*}
    \end{proof}

    \noindent \textbf{Warm-up.} Randomly shuffle a deck of cards.
    Turn them over, one-by-one, until the first Ace.\\
    \underline{Q.} What is the probability that the next card is
    \begin{enumerate}
        \item Ace of spades?
        \item Two of clubs?
    \end{enumerate}
    Attempt to answer:
    \begin{enumerate}
        \item We remove A$\spadesuit$, shuffle remaining 51 cards, and place A$\spadesuit$ in a random position.\\
        $\Rightarrow51!$ ways to shuffle other cards\\
        $\Rightarrow52$ positions available for A$\spadesuit$\\
        For the event to occur, we must place the A$\spadesuit$ directly after the first ace.\\
        $\Rightarrow\P(a)=\frac1{52}$
        \item Similarly, $\P(b)=\frac1{52}$.
    \end{enumerate}

    \begin{example} \label{party} (Inclusion-Exclusion)
        There are a party with $n$ people.
        They put their hats in a rack.
        When leaving, everybody takes a random hat from the rack.\\
        \underline{Q.} What is the probability that nobody gets their own hat?\\
        \textbf{Solution.}
        $S=\left\{\text{bijection from hats to people}\right\}$, $|S|=n!$.

        Let $E=\left\{\text{nobody gets their own hat}\right\}$.

        To make things simpler, let $E_i=\left\{i\text{th person gets their own hat}\right\}$. Then

        $$E=\bigcap_{i=1}^nE_i^C=\left(\bigcup_{i=1}^nE_i\right)^C$$
        $$\Rightarrow\P(E)=1-\P\left(\bigcup_{i=1}^nE_i\right)$$

        Therefore
        $$\P(E_i)=\frac1n,\,\P(E_i\cap E_j)=\frac{(n-2)!}{n!},\,
        \P(E_{i_1}\cap E_{i_2}\cap\cdots\cap E_{i_r})=\frac{(n-r)!}{n!}$$

        Plug into Inclusion-Exclusion:
        \begin{align*}
            \P\left(\bigcup_{i=1}^nE_i\right)&=\sum_{r=1}^n(-1)^{r+1}\sum_{1\leq i_1<i_2<\cdots<i_r\leq n}\P(E_{i_1}\cap\cdots\cap E_{i_r}) \\
            &=\sum_{r=1}^n(-1)^{r+1}\sum_{1\leq i_1<i_2<\cdots<i_r\leq n}\frac{(n-r)!}{n!} \\
            &=\sum_{r=1}^n(-1)^{r+1}\binom nr\frac{(n-r)!}{n!} \\
            &=\sum_{r=1}^n\frac{(-1)^{r+1}}{r!}
        \end{align*}
        $$\P(E)=1-\P\left(\bigcup_{i=1}^nE_i\right)=1-\sum_{r=1}^n\frac{(-1)^{r+1}}{r!}=\sum_{r=0}^n\frac{(-1)^r}{r!}$$
        
        As $n\to\infty$, $\P(E)\to\sum_{r=0}^\infty\frac{(-1)^r}{r!}=e^{-1}$.
    \end{example}

    \section{Bonferroni Inequalities}

    Inclusion-Exclusion: $$\P\left(\bigcup_{i=1}^nE_i\right)=\sum_i\P(E_i)-\sum_{i_1<i_2}\P(E_{i_1}\cap E_{i_2})+\sum_{i_1<i_2<i_3}\P(E_{i_1}\cap E_{i_2}\cap E_{i_3})-\cdots$$
    \begin{proposition}
        If $t$ is odd, then $$\P\left(\bigcup_{i=1}^nE_i\right)\leq\sum_{r=1}^t(-1)^{r+1}\sum_{1\leq i_1<i_2<\cdots<i_r\leq n}\P(E_{i_1}\cap\cdots\cap E_{i_r})$$

        If $t$ is even, then $$\P\left(\bigcup_{i=1}^nE_i\right)\geq\sum_{r=1}^t(-1)^{r+1}\sum_{1\leq i_1<i_2<\cdots<i_r\leq n}\P(E_{i_1}\cap\cdots\cap E_{i_r})$$
        
        In particular, the case $t=1$ is called the \emph{union bound}: $$\P\left(\bigcup_{i=1}^nE_i\right)\leq\sum_{i=1}^n\P(E_i).$$
    \end{proposition}
    \begin{proof}
        Proof by induction on $t$.

        $\bigcup_{i=1}^nE_i\rightarrow$ want to write as a union of mutually exclusive events
        $$\bigcup_{i=1}^nE_i=E_1\disj(E_2\cap E_1^C)\disj(E_3\cap E_1^C\cap E_2^C)\disj\cdots\disj(E_n\cap E_1^C\cap E_2^C\cap\cdots\cap E_{n-1})$$
        $$\Rightarrow\P\left(\bigcup_{i=1}^nE_i\right)=\P\left(\bigdisj_{i=1}^n\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right)\right)$$
        \begin{equation*}
            \Rightarrow\P\left(\bigcup_{i=1}^nE_i\right)=\sum_{i=1}^n\P\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right) \tag{*}
        \end{equation*}

        \textbf{Base case.} ($t=1$) For each $i$, $E_i\cap\left(\bigcap_{j<i}E_j^C\right)\inc E_i$.

        $\overtext{Prop 2}{\Rightarrow}\;\P\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right)\leq\P(E_i)$ by (*).

        \textbf{Induction step.} \begin{align*}
            E_i&=\left( E_i\cap\left(\bigcap_{j<i}E_j^C\right) \right)\disj \left( E_i\cap\left(\bigcap_{j<i}E_j^C\right)^C \right) \\
            &=\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right)\disj \left(E_i\cap\left(\bigcup_{j<i}E_j\right)\right)
        \end{align*}
        $$\Rightarrow\P\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right)=\P(E_i)-\P\left(E_i\cap\left(\bigcup_{j<i}E_j\right)\right)$$
        $$\Rightarrow\P\left(E_i\cap\left(\bigcap_{j<i}E_j^C\right)\right)=\P(E_i)-\underbrace{\P\left(\bigcup_{j<i}(E_i\cap E_j)\right)}_{(\dagger)}$$
        
        Apply the $(t-1)$-Bonferroni Inequality to $(\dagger)$.
        
        For example: ($t=2$) By the case of $t=1$, $$\P\left(\bigcup_{j<i}(E_i\cap E_j)\right)\leq\sum_{j<i}\P(E_i\cap E_j)$$
        
        plug (*) $\to$ ($\dagger$)
        $$\Rightarrow\P\left(E_i\cap\left(\bigcup_{j<i}E_j\right)\right)\geq\P(E_i)-\sum_{j<i}\P(E_i\cap E_j)$$
        $$\overtext{(*)}{\Rightarrow}\P\left(\bigcup_{i=1}^nE_i\right)\geq\sum_{i}\left(\P(E_i)-\sum_{j<i}\P(E_i\cap E_j)\right)
        =\sum_i\P(E_i)-\sum_{j<i}\P(E_i\cap E_j)$$
    \end{proof}

    \chapter{Continuity of Probability}

    \section{Increasing and Decreasing Sequences of Events}

    \begin{definition}
        Let $E_1,E_2,E_3,\ldots$ be a sequence of sets. We say the sequence is \emph{increasing} if $E_1\inc E_2\inc E_3\inc\cdots$
        and define $\lim_{n\to\infty} E_n=\bigcup_{n=1}^\infty E_n$.

        The sequence is \emph{decreasing} if $E_1\cni E_2\cni E_3\cni\cdots$
        and define $\lim_{n\to\infty} E_n=\bigcap_{n=1}^\infty E_n$.
    \end{definition}

    \begin{proposition}
        If $E_1,E_2,E_3,\ldots$ is increasing or decreasing, then $\P\left(\lim_{n\to\infty} E_n\right)=\lim_{n\to\infty}\P(E_n)$.
    \end{proposition}
    \begin{proof}
        Suppose $E_1\inc E_2\inc E_3\inc\cdots$. Then $\lim_{n\to\infty} E_n=\bigcup_{n=1}^\infty E_n$.
        \begin{center}
            \begin{tikzpicture}
                \draw (0,0) circle (0.5) (0.5,0)  node [text=black,right] {$E_1$}
                      (0.5,0) ellipse (1.2 and 0.8) (1.7,0)  node [text=black,right] {$E_2$}
                      (1,0) ellipse (1.9 and 1) (2.9,0)  node [text=black,right] {$E_3$}
                      (-1.5,-1.5) rectangle (4,1.5);
            \end{tikzpicture}
        \end{center}

        Let $F_n=E_n\exc\left(\bigcup_{i=1}^{n-1}E_i\right)$.

        Then $F_1,F_2,\ldots$ are mutually exclusive.
        Therefore $\bigcup_{i=1}^nF_i=E_n=\bigcup_{i=1}^nE_i$
        \begin{align*}
            \P\left(\lim_{n\to\infty} E_n\right)&=\P\left(\bigcup_{i=1}^\infty E_i\right) \\
            &=\P\left(\bigcup_{i=1}^\infty F_i\right) \tag{Axiom 3} \\
            &=\lim_{n\to\infty}\sum_{i=1}^n\P(F_i) \tag{def. of infinite sum}\\
            &=\lim_{n\to\infty}\P\left(\bigcup_{i=1}^nF_i\right) \tag{Axiom 3}\\
            &=\lim_{n\to\infty}\P(E_n)
        \end{align*}

        If $E_1\cni E_2\cni E_3\cni\cdots$ is decreasing, then $E_1^C\inc E_2^C\inc E_3^C\inc\cdots$ is increasing
        and $\left(\lim_{n\to\infty}E_n\right)^C=\lim_{n\to\infty}E_n^C$.
        \begin{align*}
            \Rightarrow\P\left(\lim_{n\to\infty}E_n\right)&=1-\P\left(\left(\lim_{n\to\infty}E_n\right)^C\right) \\
            &=1-\P\left(\lim_{n\to\infty}E_n^C\right) \\
            &=1-\lim_{n\to\infty}\P(E_n^C) \tag{above result} \\
            &=1-\lim_{n\to\infty}(1-\P(E_n)) \\
            &=\lim_{n\to\infty}\P(E_n)
        \end{align*}
    \end{proof}

    Given any sequence of sets $E_1,E_2,E_3,\ldots$, we define $$\limsup_{n\to\infty}E_n:=\bigcap_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_i\right)
    =\lim_{n\to\infty}\underbrace{\bigcup_{i=n}^\infty E_i}_\text{decreasing sequence}.$$
    \begin{remark} 
        $\limsup_{n\to\infty}E_n:=\bigcap_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_i\right)$ is the event that infinitely many of events of the events $E_n$ occur.
    \end{remark}

    \section{1st Borel-Cantelli Lemma}
    \begin{theorem}[1st Borel-Cantelli Lemma]{}
        If $E_1,E_2,E_3,\ldots$ is a sequence of events and $\sum_{n=1}^\infty\P(E_n)<\infty$,
        then $\P\left(\limsup_{n\to\infty}E_n\right)=0$.
    \end{theorem}
    \begin{proof}
        \begin{align*}
            &\P\left(\limsup_{n\to\infty}E_n\right) \\
            &=\P\left(\lim_{n\to\infty}\left(\bigcup_{i=n}^\infty E_i\right)\right) \tag{continuity} \\
            &=\lim_{n\to\infty}\P\left(\bigcup_{i=n}^\infty E_i\right) \\
            &\leq\lim_{n\to\infty}\sum_{i=n}^\infty\P(E_i)\to0\text{ since }\sum_{n=1}^\infty\P(E_n)<\infty
        \end{align*}
    \end{proof}
    \noindent\textbf{Application.} (1st Borel-Cantelli Lemma)\\
    (1) Promotion in a restaurant: the $n$th customer rolls $n$ dice.
    If all rolls are even, then they get free food for life!

    Let $E_n=\left\{n\text{th customer gets free food for life}\right\}$.
    $S=\left\{1,2,\ldots,6\right\}^n$, $E_n=\left\{2,4,6\right\}^n$.

    $$\P(E_n)=\frac{\abs{\left\{2,4,6\right\}^n}}{\abs{\left\{1,2,\ldots,6\right\}^n}}=\frac{3^n}{6^n}=2^{-n}.$$

    Since $\sum_{n=1}^\infty\P(E_n)=\sum_{n=1}^\infty2^{-n}=1<\infty$,
    the 1st Borel Cantelli Lemma states $\P(\limsup_{n\to\infty}E_n)=0$.
    Therefore almost surely, only have to give finitely many customers free food!\\
    (2) Roll a die infinitely many times. We are interested in the no. of even numbers.
    
    Let $e_n=\frac{\#\left\{\text{even rolls in first $n$ rolls}\right\}}{n}$.
    
    Fix $\varepsilon>0$. Let $E_n=\left\{e_n\geq\frac12+\varepsilon\right\}$.

    $S=\left\{1,2,3,4,5,6\right\}^n$.
    Count $E_n$: \begin{enumerate}
        \item Choose how many even rolls $r$: $\left(\frac12+\varepsilon\right)n\leq r\leq n$
        (Apply the sum rule over choice of $r$).
        \item Choose which rolls are even: $\binom nr$ choices.
        \item Each roll has 3 choice $\left\{2,4,6\right\}$ if even, $\left\{1,3,5\right\}$ if odd.
        Product rule $\Rightarrow 3^n$ choice.\\
    \end{enumerate}

    Putting it all togeher: \let\Ho\H \renewcommand\H{\mathcal H} 
    $$|E_n|=\sum_{r=\ceil*{\left(\frac12+\varepsilon\right)n}}^n\binom nr3^n$$
    $$\P(E_n)=\frac{|E_n|}{|S_n|}=\frac{\sum_{r=\ceil*{\left(\frac12+\varepsilon\right)n}}^n\binom nr3^n}{6^n}=\frac{\sum_{r=\ceil*{\left(\frac12+\varepsilon\right)n}}^n\binom nr}{2^n}$$
    Approximation. If $\frac12\leq\alpha\leq1$,
    $$\sum_{r=\ceil{\alpha n}}^n\binom nr\leq2^{n\H(\alpha)}$$

    where $\H$ is the binary entropy function, defined as
    $\H(\alpha)=-\alpha\log_2\alpha-(1-\alpha)\log_2(1-\alpha)$.
    
    $0\leq\H(\alpha)\leq1$ with $\H(\alpha)=1$ iff $\alpha=\frac12$.
    $$\P(E_n)=\frac{\sum_{r=\ceil*{\left(\frac12+\varepsilon\right)n}}^n\binom nr}{2^n}\leq
    \frac{2^{n\H\left(\frac12+\varepsilon\right)}}{2^n}=2^{-\delta n}$$ 

    where $\H\left(\frac12+\varepsilon\right)=(1-\delta)n$ for some $\delta=\delta(\varepsilon)>0$.
    
    $\Rightarrow\P(E_n)\leq2^{-\delta n}$
    
    $\Rightarrow\sum_{n=1}^\infty\P(E_n)<\infty$
    
    1st Borel Cantelli $\Rightarrow\P(\limsup_{n\to\infty}E_n)=0$.
    
    $\Rightarrow$ almost surely, there exists $N$ such that for all $n\geq N$, $E_n$ doesn't happen
    $e_n<\frac12+\varepsilon$.
    
    By symmetry, same is true for ratio of odd numbers.

    $\Rightarrow$ exists $N'$ such that for all $n\geq N'$, $e_n>\frac12-\varepsilon$.
    
    $\Rightarrow$ exists $N''$ such that for all $n\geq N''$, $\frac12-\varepsilon<e_n<\frac12+\varepsilon$.
    
    Since $\varepsilon>0$ is arbitrary, $\lim_{n\to\infty}e_n=\frac12$.

\chapter{Conditional Probabilities}
    \begin{example}
        Know that a die roll is prime. What is the probability that it is even?\\[5pt]
        $1:0\quad$ $2:\frac13\quad$ $3:\frac13\quad$ $4:0\quad$ $5:\frac13\quad$ $6:0\quad$
        $\P(\text{even})=\frac13$.
    \end{example}
    \begin{center}
        \begin{tikzpicture}
            \draw (-0.5,0) ellipse (2 and 1) (-2,1)  node [text=black,above] {$E$}
                  (1.5,0) ellipse (2 and 1) (3,1)  node [text=black,above] {$F$};
        \end{tikzpicture}
    \end{center}

    Interested in probability of $E$.

    $\rightarrow$ told that event $F$ ocurrs

    $\rightarrow$ for $E$ to happen, $E\cap F$ must happen

    Outcomes outside $F$ now have zero probability $\Rightarrow$ to make total probability 1, we divide by $\P(F)$.

    \begin{definition}
        The \emph{conditional probability} of $E$ given $F$ is $$\P(E|F)=\frac{\P(E\cap F)}{\P(F)}.$$
    \end{definition}
    \noindent Observation. \begin{itemize}
        \item $E\cap F\inc F\Rightarrow0\leq\P(E\cap F)\leq\P(F)\Rightarrow0\leq\P(E|F)\leq1$.
        \item If $E$, $F$ are disjoint, then $\P(E|F)=0$.
        \item $\P(E\cap F)=\P(E|F)\P(F)$. 
    \end{itemize}

    \begin{example}
        (See Example \ref{party}.)
        There are a party with $n$ people and $n$ hats.
        What is the probability that nobody gets their own hat?\\
        \textbf{Solution.} Before: calculated inclusion-exclusion $$\P(\text{0 people get own hats})=\sum_{k=0}^n\frac{(-1)^k}{k!}\to e^{-1}$$
        $$\P(n\text{ people get own hats})=\frac{1}{n!}$$

        Fix a set $R$ of $r$ people. Let $E_R=\left\{\text{people in $R$ get own hats and people not in $R$ don't}\right\}$.
        \begin{align*}
            \P(\text{exactly $r$ people get own hats})&=\P\left(\bigdisj_{R:|R|=r}E_R\right) \\
            &=\sum_{R:|R|=r}\P(E_R) \\
            &=\binom nr\P(E_{\left\{1,\ldots,r\right\}})
        \end{align*}
        $$E_R=\underbrace{\left\{r+1,r+2,\ldots,n\text{ don't get own hats}\right\}}_E\cap\underbrace{\left\{1,2,\ldots,r\text{ do get own hats}\right\}}_F$$
        
        Use $\P(E\cap F)=\P(E|F)\P(F)$.
        \begin{align*}
            \P(E|F)&=\P(\left\{ \text{nobody gets own hate in a party of $n-r$ people} \right\}) \\
            &=\sum_{k=1}^{n-r}\frac{(-1)^k}{k!}\to e^{-1}\text{ if }n-r\to\infty
        \end{align*}
        
        Let $F_i=\left\{i\text{th person gets own hat}\right\}$. $F=F_1\cap F_2\cap\cdots\cap F_r$.
        \begin{align*}
            \P(F)&=\P((F_1\cap F_2\cap\cdots\cap F_{r-1})\cap F_r) \\
            &=\P(F_r|F_1\cap F_2\cap\cdots\cap F_{r-1})\P((F_1\cap F_2\cap\cdots\cap F_{r-2})\cap F_{r-1}) \\
            &=\cdots=\P(F_r|F_1\cap F_2\cap\cdots\cap F_{r-1})\P(F_{r-1}|F_1\cap F_2\cap\cdots\cap F_{r-2})\cdots\P(F_1)
        \end{align*}

        Observe that $\P(F_1)=\frac1n$, $\P(F_2|F_1)=\frac1{n-1}$,..., $\P(F_i|F_1\cap F_2\cap\cdots\cap F_{i-1})=\frac1{n-i+1}$\\
        $\Rightarrow\P(F)=\frac1n\cdot\frac1{n-1}\cdot\ldots\cdot\frac1{n-r+1}=\frac{(n-r)!}{n!}$.
        \begin{align*}
            &\P(\text{exactly $r$ people get own hats})=\binom nr\P(E_{\left\{1,\ldots,r\right\}})\approx\binom nr\frac1e\cdot\frac{(n-r)!}{n!}=\frac1{r!e}
        \end{align*}
    \end{example}

    Suppose we can partition the sample space $$S=F_1\disj F_2\disj\cdots\disj F_n$$
    
    Then for any event $E\inc S$, $$E=E\cap S=E\cap\left(\bigdisj_{i=1}^n F_i\right)=\bigdisj_{i=1}^n(E\cap F_i)$$
    $$\Rightarrow\P(E)\;\overtext{Axiom 3}=\;\sum_{i=1}^n\P(E\cap F_i)$$
    $$\Rightarrow\P(E)=\sum_{i=1}^n\P(E|F_i)\P(F_i)$$

    This is the \emph{Law of Total Probability}.

    \begin{example} \label{swim}
        Go on holiday to Australia. Want to go to the beach.
        Maybe go swimming depending on the weather.
        \begin{itemize}
            \item if sunny: go swimming with probability 70\%
            \item if not sunny: go swimming with probability 30\%
        \end{itemize}

        The weather forecast says there is 80\% chance of sunny. What is $\P(\text{swimming})$?\\
        \textbf{Solution.}
        \begin{align*}
            &\P(\text{swimming}) \\
            &=\P(\text{swimming}|\text{sunny})\P(\text{sunny})+\P(\text{swimming}|\text{not sunny})\P(\text{not sunny}) \\
            &=0.7\times0.8+0.3\times0.2=\fbox{0.62}
        \end{align*}
    \end{example}

    \noindent \textbf{Warm-up.} Game show (Monty Hall)
    \begin{itemize}
        \item Three doors: behind one door is a car, behind the other two are goats.
        \item You choose one, then the host open another door that he knows has a goat.
        \item Offer you the option to switch doors. Should you?
    \end{itemize}

    \begin{example} (See Example \ref{swim}.)
        Let $\P(\text{sunny})=0.8$,
        $$\P(\text{swim}|\text{sunny})=0.7,\quad\P(\text{swim}|\text{not sunny})=0.3,$$
        $$\P(\text{bite}|\text{swim})=0.5,\quad\P(\text{bite}|\text{not swim})=0.01.$$

        By law of total probability, $\P(\text{bite})=0.3138$.\\
        \underline{Q.} If I do get bitten by a shark, what is the probability it was sunny?\\
        \textbf{Solution.}
        $$\P(\text{sunny}|\text{bite})=\frac{\P(\text{sunny}\cap\text{bite})}{\P(\text{bite})}$$
        $$\P(\text{sunny}\cap\text{bite})=\P(\text{bite}\cap\text{sunny})=\P(\text{bite}|\text{sunny})\P(\text{sunny})$$
        \begin{center}
            \begin{tikzpicture}[fill=gray]
                \draw (0,0) circle (0.5) (0.5,0)  node [text=black,right] {$E$}
                      (0.5,0) ellipse (1.5 and 1) (2,0.7)  node [text=black,above] {$F$}
                      (-2,-1.5) rectangle (3,1.5) node [text=black,above] {$S$};
            \end{tikzpicture}
        \end{center}
        \begin{align*}
            \P(\text{bite}|\text{sunny})&=\P(\text{bite}|\text{swim, sunny})\P(\text{swim}|\text{sunny}) \\
            &\quad+\P(\text{bite}|\text{not swim, sunny})\P(\text{not swim}|\text{sunny}) \\
            &=\P(\text{bite}|\text{swim})\P(\text{swim}|\text{sunny})+\P(\text{bite}|\text{not swim})\P(\text{not swim}|\text{sunny}) \\
            &=0.5\times0.7+0.01\times0.3=0.353
        \end{align*}        
        \begin{align*}
            \P(\text{sunny}|\text{bite})&=\frac{\P(\text{sunny}\cap\text{bite})}{\P(\text{bite})} \\
            &=\frac{0.353\times0.8}{0.3138}=\fbox{0.8999\ldots}
        \end{align*}
    \end{example}

    \begin{theorem}[Bayes' Rule]{}
        If we have a partition $S=F_1\disj F_2\disj\cdots\disj F_n$ and an event $E\inc S$, then 
        $$\P(F_i|E)=\frac{\P(E|F_i)\P(F_i)}{\sum_{j=1}^n\P(E|F_j)\P(F_j)}.$$
    \end{theorem}
    \begin{proof}
        By definition, $\P(F_i|E)=\frac{\P(F_i\cap E)}{\P(E)}$.
        By Law of total probability, $\P(E)=\sum_{j=1}^n\P(E|F_j)\P(F_j)$.
        This follows from $\P(F_i\cap E)=\P(E\cap F_i)=\P(E|F_i)\P(F_i)$.
    \end{proof}

    \begin{example}
        1\% of the population has COVID. Rapid test for COVID has 95\% accuracy, with
        5\% chance of ``false positive'' and 5\% chance of ``false negative''.\\
        \underline{Q.} A random person tests positive. What is the probability they have COVID?\\
        \textbf{Solution.} Let $S$ be the population. Let \begin{align*}
            &F_1=\left\{\text{people with COVID}\right\},&&\P(F_1)=0.01 \\
            &F_2=\left\{\text{people without COVID}\right\},&&\P(F_1)=0.99
        \end{align*}
        \begin{align*}
            &E=\left\{\text{test positive}\right\},&&\P(E|F_1)=0.95 \\
            &&&\P(E|F_2)=0.05
        \end{align*}
        \begin{align*}
            \P(F_1|E)&=\frac{\P(E|F_1)\P(F_1)}{\P(E|F_1)\P(F_1)+\P(E|F_2)\P(F_2)} \tag{Bayes'} \\
            &=\frac{0.95\times0.01}{0.95\times0.01+0.05\times0.99} \\
            &=\fbox{0.1610}
        \end{align*}
    \end{example}

    \begin{example}
        DNA test: \begin{itemize}
            \item $\P(\text{positive}|\text{match})=1$
            \item $\P(\text{positive}|\text{not match})=0.0001$
        \end{itemize}
        \begin{itemize}
            \item City of population 2500000
            \item Random person $\to$ DNA matches sample from the crime scene
        \end{itemize}
        
        What is $\P(\text{guilty})$?\\
        \textbf{Solution.} Let $S=\left\{\text{all people in the city}\right\}$, $F_1=\left\{\text{guilty}\right\}$, $F_2=\left\{\text{not guilty}\right\}$.
        
        $\P(F_1)=\frac1{2500000}$, $\P(F_2)=\frac{2499999}{2500000}$.
        
        Let $E=\left\{\text{match on DNA test}\right\}$. $\P(E|F_1)=1$, $\P(E|F_2)=0.0001$.
        \begin{align*}
            \P(F_1|E)&=\frac{\P(E|F_1)\P(F_1)}{\P(E|F_1)\P(F_1)+\P(E|F_2)\P(F_1)} \tag{Bayes'} \\
            &=\frac{1\times\frac{1}{2500000}}{1\times\frac{1}{2500000}+\frac{1}{10000}\left(1-\frac{1}{2500000}\right)} \\[5pt]
            &=\fbox{0.003984\ldots}
        \end{align*}
    \end{example}

\chapter{Independent Events}

    \begin{definition}
        If $\P(E|F)=\P(E)$, then we say $E$ and $F$ are \emph{independent}.
        Otherwise they are \emph{dependent}.
    \end{definition}
    Equivalently, $E$ and $F$ are independent iff $$\P(E\cap F)=\P(E)\P(F).$$
    
    \begin{corollary}
        Independence is symmetric in $E$, $F$.
    \end{corollary}

    \noindent Quiz. Which of the following pairs of events can be independent?
    \begin{center}
        \begin{tikzpicture}[fill=gray]
            \draw (-0.5,0) circle (0.75) (-1.25,0.75)  node [text=black,left] {$E$}
                  (1.5,0) circle (0.75) (2.25,0.75)  node [text=black,right] {$F$}
                  (-2,-1.5) rectangle (3,1.5) node [text=black,above] {$S$};
        \end{tikzpicture}
        \begin{tikzpicture}[fill=gray]
            \draw (0,0) circle (0.75) (-0.75,0.75)  node [text=black,left] {$E$}
                  (1,0) circle (0.75) (1.75,0.75)  node [text=black,right] {$F$}
                  (-2,-1.5) rectangle (3,1.5) node [text=black,above] {$S$};
        \end{tikzpicture}
        \begin{tikzpicture}[fill=gray]
            \draw (0,0) circle (0.5) (0.5,0)  node [text=black,right] {$E$}
                  (0.5,0) ellipse (1.5 and 1) (2,0.7)  node [text=black,above] {$F$}
                  (-2,-1.5) rectangle (3,1.5) node [text=black,above] {$S$};
        \end{tikzpicture}
    \end{center}

    \begin{example}
        Roll two dices. 
        $$E_1=\set{\text{first roll is a 4}},\;E_2=\set{\text{second roll is a 3}}$$
        $$F_1=\set{\text{sum is 6}},\;F_2=\set{\text{sum is 7}}$$

        Which pairs are independent?\\
        \textbf{Solution.}
        $$S=\set{(1,1),\ldots(1,6),(2,1),\ldots,(2,6),\ldots,(6,1),\ldots,(6,6)}$$
        $$\begin{matrix}
            E_1=\set{(4,1),(4,2),\ldots,(4,6)}, & \P(E_1)=\frac6{36}=\frac16. \\[10pt]
            E_2=\set{(1,3),(2,3),\ldots,(6,3)}, & \P(E_2)=\frac6{36}=\frac16.
        \end{matrix}$$
        $$E_1\cap E_2=\set{(4,3)},\P(E_1\cap E_2)=\frac1{36}=\P(E_1)\P(E_2).$$

        $\Rightarrow E_1,E_2$ are independent.

        $$\begin{matrix}
            F_1=\set{(1,5),(2,4),(3,3),(4,2),(5,1)}, & \P(F_1)=\frac5{36}.
        \end{matrix}$$
        $$E_1\cap F_1=\set{(4,2)},\P(E_1\cap F_1)=\frac1{36}\neq\frac5{36}\cdot\frac16=\P(E_1)\P(E_2).$$
        
        $\Rightarrow E_1,F_1$ are not independent.

        $F_1$, $F_2$ not independent. They are disjoint.

        $$\begin{matrix}
            F_2=\set{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)}, & \P(F_1)=\frac6{36}=\frac16.
        \end{matrix}$$
        $$E_i\cap F_2=\set{(4,3)},\P(E_i\cap F_2)=\frac1{36}=\frac16\cdot\frac16=\P(E_i)\P(F_2).$$

        $\Rightarrow E_1,E_2$ are both independent of $F_2$.
    \end{example}

    \begin{claim}
        If $E$, $F$ are independent, then $E$, $F^C$ are independent.
    \end{claim}
    \begin{proof}
        \begin{align*}
            \P(E\cap F^C)&=P(E)-\P(E\cap F) \\
            &=\P(E)-\P(E)\P(F) \tag{independence} \\
            &=\P(E)(1-\P(F))=\P(E)\P(F^C)
        \end{align*}
    \end{proof}

    However, if \begin{align*}
            &E_1,F\text{ are independent, and} \\
            &E_2,F\text{ are independent,} 
    \end{align*}

    that doesn't mean \begin{align*}
        &E_1\cup E_2,F\text{ are independent, or} \\
        &E_1\cap E_2,F\text{ are independent.} 
    \end{align*}

    \begin{definition}
        We say $E_1,E_2,E_3$ are (mutually) independent if:
        \begin{itemize}
            \item $\P(E_1\cap E_2\cap E_3)=\P(E_1)\P(E_2)\P(E_3)$
            \item $\P(E_1\cap E_2)=\P(E_1)\P(E_2)$
            \item $\P(E_1\cap E_3)=\P(E_1)\P(E_3)$
            \item $\P(E_2\cap E_3)=\P(E_2)\P(E_3)$
        \end{itemize}
        all hold.
    \end{definition}

    There is a more general version:
    \begin{definition}
        Given a sequence of events $E_1,E_2,E_3,\ldots$, we say they are (mutually) independent if 
        for any finite set $I$ of indices, $$\P\left(\bigcap_{i\in I}E_i\right)=\prod_{i\in I}\P(E_i)$$
    \end{definition}

    \begin{example}
        Inclusion-Exclusion for independent events.

        Let $E_1,E_2,E_3,\ldots,E_n$ be independent.
        \begin{align*}
            \P\left(\bigcup_{i=1}^n E_i\right)&=\sum_{\substack{I\inc[n] \\ I\neq\0}}(-1)^{|I|+1}\P\left(\bigcap_{i\in I}E_i\right) \\
            &=\sum_{\substack{I\inc[n] \\ I\neq\0}}(-1)^{|I|+1}\prod_{i\in I}\P(E_i) \\
            &=1-\prod_{i=1}^n(1-\P(E_i))
        \end{align*}

        Alternatively, use De Morgan to turn the union into an intersection:
        \begin{align*}
            \P\left(\bigcup_{i=1}^n E_i\right)&=1-\P\left(\left(\bigcup_{i=1}^n E_i\right)^C\right) \\
            &=1-\P\left(\bigcap_{i=1}^n E_i^C\right) \\
            &=1-\prod_{i=1}^n\P(E_i^C)=1-\prod_{i=1}^n(1-\P(E_i))
        \end{align*}
    \end{example}

    \noindent \textbf{Application.} Suppose we have a test with a false negative rate of 1\% and a false positive rate rate of 50\%.

    Suppose we can repeat the test independently.

    If actually positive, $\P(\text{pos, pos})=0.99\times0.99\geq0.98$.

    If actually negative, $\P(\text{pos, pos})=0.5\times0.5=0.25$.\\

    Let $S=(0,1]$, $z\in S$ be uniformly randomly chosen.
    That is, $\P(z\in(x,y])=y-x$.

    Let $E_1,E_2,\ldots$ be events in the probability space. Let $p_i=\P(E_i)$.

    The 1st Borel-Cantelli Lemma states that if $\sum_{n=1}^{\infty}p_n<\infty$, then $\P\left(\limsup_{n\to\infty}E_n\right)=0$.

    Homework: if $\sum_{n=1}^{\infty}$, then it is possible that $\P\left(\limsup_{n\to\infty}E_n\right)=1$.

    Also possible that $\P\left(\limsup_{n\to\infty}E_n\right)=0$. For example, $E_n=(0,\tfrac1n]$.
    
    \begin{theorem}[2nd Borel-Cantelli Lemma]{}
        If $E_1,E_2,\ldots$ are mutually independent events and $\sum_{n=1}^\infty\P(E_n)=\infty$,
        then $\P\left(\limsup_{n\to\infty}E_n\right)=1$.
    \end{theorem}
    \begin{proof}
        Recall that $\limsup_{n\to\infty}E_n=\bigcap_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_n\right)$.
        $$\P\left(\limsup_{n\to\infty}E_n\right)=1\Rightarrow\P\left(\left(\limsup_{n\to\infty}E_n\right)^C\right)=0$$
        $$\left(\limsup_{n\to\infty}E_n\right)^C=\left(\bigcap_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_n\right)\right)^C
        =\bigcup_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_n\right)^C=\bigcup_{n=1}^\infty\bigcap_{i=n}^\infty E_n^C$$
        \begin{align*}
            \P\left(\left(\bigcap_{n=1}^\infty\left(\bigcup_{i=n}^\infty E_n\right)\right)^C\right)
            &=\P\left(\bigcup_{n=1}^\infty\bigcap_{i=n}^\infty E_n^C\right) \\
            &=\lim_{n\to\infty}\P\left(\bigcap_{i=n}^\infty E_n^C\right) \tag{continuity}\\
            &=\lim_{n\to\infty}\prod_{i=n}^\infty\P(E_i^C) \tag{independence, *}\\
            &=\lim_{n\to\infty}\prod_{i=n}^\infty(1-\P(E_i)) \\
            &=\lim_{n\to\infty}0=0 \tag{**}
        \end{align*}
        (**) by convergence test for infinite product ($\sum_{n=1}^\infty\P(E_n)=\infty$) and (*) by
        \begin{align*}
            \P\left(\bigcap_{i=1}^\infty E_i^C\right)&=\P\left(\lim_{N\to\infty}\bigcap_{i=n}^NE_i^C\right) \\
            &=\lim_{N\to\infty}\P\left(\bigcap_{i=n}^NE_i^C\right) \tag{continuity}\\
            &=\lim_{N\to\infty}\prod_{i=n}^N\P(E_i^C) \tag{independence}\\
            &=\prod_{i=n}^\infty\P(E_i^C)
        \end{align*}
    \end{proof}
    
\chapter{Discrete Random Variables}

    \section{Discrete Random Variable}

    \begin{definition}
        Given a probability probability space $(S,\P)$, a \emph{random variable} is a function $X:S\to\R$.
        It is \emph{discrete} if it only takes countably many value.
    \end{definition}
    \noindent Observation. A discrete random variable defines a (simpler) probability space.
    
    Let $x_1,x_2,x_3,\ldots$ be the values $X$ can take. i.e. $X(S)=\set{x_1,x_2,x_3,\ldots}$. $\leftarrow$ new sample space
    $p(x_i)=\P(X(s)=x_i)=\P(\set[X(s)=x_i]{s\in S})$.\\
    \noindent Observation. \begin{align*}
        \sum_ip(x_i)&=\sum_i\P(X(s)=x) \\
        &=\sum_i\P(X^{-1}(x_i)) \tag{pairwise disjoint} \\
        &=\P(\cup_iX^{-1}(x_i)) \\
        &=\P(S)=1
    \end{align*}
    
    \begin{example} \label{multiple choice}
        Multiple choice exam \begin{itemize}
            \item 5 questions, each question has 4 options, one is correct
            \item pick uniformly random answer on each question, independently
        \end{itemize}
        \underline{Q.} What is the probability of getting non of them correct?\\
        \textbf{Solution.} Let $X=$ the number of correct answers.

        Calculate $\P(X=0)$:

        $\P(X=0)=\P(F_1\cap F_2\cap\cdots\cap F_5)$, $F_i=\set{\text{get $i$th question wrong}}$. $\P(F_i)=\frac34$.
        
        By independence, $\P\left(\bigcap_{i=1}^5F_i\right)=\prod_{i=1}^5\P(F_i)=\left(\frac34\right)^5$.

        We can calculate \begin{align*}
            \P(X=0)&=\left(\frac34\right)^5 \\
            \P(X=1)&=\binom51\left(\frac14\right)\left(\frac34\right)^4 \\
            \P(X=2)&=\binom52\left(\frac14\right)^2\left(\frac34\right)^3 \\
            \P(X=3)&=\binom53\left(\frac14\right)^3\left(\frac34\right)^2 \\
            \P(X=4)&=\binom54\left(\frac14\right)^4\left(\frac34\right) \\
            \P(X=5)&=\left(\frac14\right)^5
        \end{align*}
    \end{example}

    \begin{example}
        Promotion: $n$ different types of prizes

        In each attempt, we get a uniformly random prize, independent of previous attempt.\\
        \underline{Q.} How many attempts do we need to get all types of prizes?\\
        \textbf{Solution.}
        Let $S=\set[1\leq s_i\leq n]{(s_1,s_2,s_3,\ldots)}$, and $X((s_1,s_2,s_3,\ldots))=$\\$\min\set[(s_1,s_2,s_3,\ldots)\text{ has all numbers from 1 to }n]{t}$.

        If $t<n$, $\P(X=t)=0$.
        $$\P(X=n)=\frac{n!}{n^n}\approx\frac{1}{(e+o(1))^n}$$

        If $t>n$, $\P(X=t)=$?
        $$\P(X>t)=\P\left(\bigcup_{i=1}^nE_i\right)\text{ where }E_i=\set{i\text{th prize is ruisrily after $t$ attempts}}$$
        $$\P(E_i)=\left(\frac{n-1}{n}\right)^t\leftarrow\frac{n-1}{n}\text{ probability for each independent try}$$
        $$\P\left(\bigcup_{i=1}^nE_i\right)\overtext{inc-exc}{=}\sum_{\0\neq I\inc[n]}(-1)^{|I|+1}\P\left(\bigcap_{i\in I}E_i\right)$$
        $$\P\left(\bigcap_{i\in I}E_i\right)=\left(\frac{n-|I|}{n}\right)^t\leftarrow n-|I|\text{ bid options for each attempt}$$
        $$\P\left(\bigcup_{i=1}^nE_i\right)=\sum_{I\neq\0}(-1)^{|I|+1}\P\left(\bigcap_{i\in I}E_i\right)=\sum_{r=1}^n(-1)^{r+1}\binom nr\left(\frac{n-r}{n}\right)^t$$
        
        Therefore
        $$\P(X=t)=\P(X>t-1)-\P(X>t)=\sum_{r=1}^n(-1)^{r+1}\binom nr\left(\frac{n-r}{n}\right)^{t-1}\left(1-\frac{n-r}{n}\right)$$
    \end{example}

    \section{Expectation}
    \begin{definition}
        Given a probability space $(S,\P)$ and a discrete random variable $X:S\to\R$ which takes values $x_1,x_2,\ldots$, 
        the \emph{expectation} of $X$ is $$\E[X]=\sum_ix_ip(x_i)=\sum_ix_i\P(X=x_i).$$
    \end{definition}

    \begin{example} (See Example \ref{multiple choice}.)
        Multiple choice exam
        \begin{itemize}
            \item 2 questions, each question has 4 options
            \item pick uniformly random answer on each question, independently
        \end{itemize}
        \underline{Q.} What is the expected number of correct answers?\\
        \textbf{Solution.} $X$ takes values 0, 1, or 2.
        $$p(0)=\left(\frac34\right)^2=\frac{9}{16},\,p(1)=\binom21\left(\frac14\right)\left(\frac34\right)=\frac{6}{16},\,p(2)=\left(\frac14\right)^2=\frac{1}{16}$$
        $$\E[X]=0\cdot\frac{9}{16}+1\cdot\frac{6}{16}+2\cdot\frac{1}{16}=\frac{8}{16}=\frac12$$

        Multiple choice, +1 point if answer correct and $-1$ point if answer is incorrect.

        Let $Y=$ score. What is the xpectation of $Y$?
        \begin{center}
            \begin{tabular}{c|c|c}
                $X$ & $Y$ & $p(Y)$ \\[5pt] \hline & & \\[-10pt]
                0 & -2 & $\frac{9}{16}$ \\[10pt]
                1 & 0 & $\frac{6}{16}$ \\[10pt]
                2 & 2 & $\frac{1}{16}$
            \end{tabular}
        \end{center}
        $$Y=X-(2-X)=2X-2$$
        $$\E[Y]-\frac{9}{16}\cdot(-2)+\frac{6}{16}\cdot0+\frac{1}{16}\cdot2=-1=2\cdot\frac{1}{2}-2$$
    \end{example}

    \begin{lemma}[Linearity of Expectation]
        Let $X_1,X_2,\ldots,X_n$ be random variables in a probability space $(S,\P)$.

        Let $Y=\sum_{i=1}^{n}\alpha_iX_i$ for some $\alpha_i\in\R$.
        Then $\E[Y]=\sum_{i=1}^{n}\alpha_i\E[X_i]$.
    \end{lemma}
    \begin{proof}
        First, we prove the claim:
        \begin{claim}
            $\E[X]=\sum_{s\in S}X(s)\P(s)$.
        \end{claim}
        \begin{proof} (claim)
            By definition, if $X(S)=\set{x_2,x_2,\ldots}$, \begin{align*}
                \E[X]&=\sum_ix_ip(x_i) \\
                &=\sum_ix_i\P(\set[X(s)=x_i]{s\in S}) \\
                &=\sum_ix_i\P\left(\bigcup_{s\in X^{-1}(x_i)}\set{s}\right) \\
                &=\sum_ix_i\sum_{s\in X^{-1}(x_i)}\P(s) \\
                &=\sum_{s\in S}X(s)\P(s)
            \end{align*}
        \end{proof}
        \begin{align*}
            \Rightarrow\E[Y]&=\sum_{x\in S}Y(s)\P(s) \\
            &=\sum_{x\in S}\left(\sum_{i=1}^{n}\alpha_iX_i(s)\right)\P(s) \\
            &=\sum_{x\in S}\sum_{i=1}^{n}\alpha_iX_i(s)\P(s) \\
            &=\sum_{i=1}^{n}\alpha_i\sum_{x\in S}X_i(s)\P(s) \\
            &=\sum_{i=1}^n\alpha_i\E[x_i]
        \end{align*}
    \end{proof}

    \begin{example} (See Example \ref{multiple choice}.)
        Multiple choice exam
        \begin{itemize}
            \item $n$ questions, each question has $k$ options
            \item pick uniformly random answer on each question, independently
        \end{itemize}
        \underline{Q.} What is the expectation number of correct answers?\\
        \textbf{Solution.} Let $X=$ number of correct answers.
        Let $$X_i=\begin{cases}
            1 & \text{if the $i$th question is right }\left(\frac1k\right) \\
            0 & \text{otherwise }\left(\frac{k-1}k\right).
        \end{cases}$$

        Then $X=\sum_{i=1}^nX_i$. 
        $$\overtext{LoE}{\Rightarrow}\E[X]=\sum_{i=1}^n\E[X_i]=\sum_{i=1}^n\frac1k=\fbox{$\dfrac nk$}$$
    \end{example}

    \begin{example} (See Example \ref{multiple choice}.)
        Multiple choice exam
        \begin{itemize}
            \item first 10 questions have 3 options
            \item last 5 questions have 5 options
            \item pick uniformly random answer on each question, independently
        \end{itemize}
        \underline{Q.} What is \begin{enumerate}
            \item the probability of getting exactly $k$ correct?
            \item the expected number of correct answers?
        \end{enumerate}
        \textbf{Solution.} \begin{enumerate}
            \item Suppose we get $l$ correct from the first 10, $0\leq l\leq10$.\\
            $\Rightarrow k-l$ correct from last 5. Then the answer would be
            $$\sum_{l=0}^{10}\binom{10}l\binom5{k-l}\left( \frac{1}{3} \right)^l\left( \frac{2}{3} \right)^{10-l}\left( \frac{1}{5} \right)^{k-l}\left( \frac{4}{5} \right)^{5-k+1}.$$
            (Define $\binom nr=0$ for $r>n$.)
            \item Let $X_i$ be the indicator random variable for the event that we got the $i$-th question right.
            $$X_i=\begin{cases}
                1 & \text{if $i$-th question correct} \\
                0 & \text{if not}
            \end{cases}$$
            Then if $X=$ the number of correct answers, $X=\sum_{i=1}^{15}X_i$.\\
            By linearity of expectation, \begin{align*}
                \E[X]&=\sum_{i=1}^{15}\E[X_i]=\sum_{i=1}^{15}\P(X_i=1) \\
                &=\sum_{i=1}^{10}\P(i\text{-th question correct})+\sum_{i=11}^{10}\P(i\text{-th question correct}) \\
                &=\sum_{i=1}^{10}\frac13+\sum_{i=11}^{10}\frac15=\fbox{$\frac{13}{3}$}
            \end{align*}
        \end{enumerate}
    \end{example}

    \begin{theorem}[Markov's Inequality]{}
        If $X$ is a discrete random variable taking nonnegative values, then for any $t\in\R_{>0}$,
        $$\P(X\geq t)\leq\frac{\E[X]}{t}.$$
    \end{theorem}
    \begin{remark} \begin{enumerate}
        \item Nonnegativity is necessary. Consider $$X=\begin{cases}
            1 & \text{with probability }\frac12 \\[10pt]
            -1 & \text{with probability }\frac12
        \end{cases}$$
        Then $\E[X]=0$, but for $t\leq1$, $\P(X\geq t)\geq\frac12>0$.
        \item Inequality is useless for $t\leq\E[X]$, 
        but useful for saying a random variable is unlikely to be much bigger than its expectation.
    \end{enumerate} \end{remark}
    \begin{proof}
        \begin{align*}
            \E[X]&=\sum_{x}xp(x) \\
            &=\sum_{x:x<t}xp(x)+\sum_{x:x\geq t}xp(x) \\
            &\geq\sum_{x:x<t}0+\sum_{x:x\geq t}tp(x) \tag{$X$ is nonnegative} \\
            &=t\sum_{x:x\geq t}p(x) \\
            &=t\sum_{x:x\geq t}\P(\set{X=x}) \\
            &=t\P\left(\bigcup_{x:x\geq t}\set{X=x}\right) \tag{disjoint events} \\
            &=t\P(X\geq t)
        \end{align*}
    \end{proof}

    From Markov's inequality, we can know that if $\E[X]$ is low, $X$ is likely to be low.

    Is the converse true? if $\E[X]$ is high, is $X$ likely to be high?

    This is in general not true. For example, let
    $$X=\begin{cases}
        1000000 & \text{with probability }\frac{1}{1000} \\[10pt]
        0 & \text{with probability }\frac{999}{1000}.
    \end{cases}$$

    Then $\E[X]=1000000\cdot\frac{1}{1000}+0\cdot\frac{999}{1000}=1000$. But $\P(X>0)=\frac{1}{1000}$.
    
    Fun question. There are 3 investment option. Which one would you take?
    \begin{align*}
        &X_1=1 \text{ with probability }1 && \E[X_1]=1 \\
        &X_2=\begin{cases}
            1000 & \text{with probability }\frac{1}{1000} \\[10pt]
            0 & \text{with probability }\frac{999}{1000} \\
        \end{cases} && \E[X_2]=1 \\[5pt]
        &X_3=\begin{cases}
            \frac{2000}{999} & \text{with probability }\frac{999}{1000} \\[10pt]
            -1000 & \text{with probability }\frac{1}{1000} \\
        \end{cases} && \E[X_3]=1 
    \end{align*}

    \section{Variance}

    We want to know that how far from the expectation are we on average.
    \begin{definition}
        The \emph{variance} of a random variable $X$ with expectation $\mu$ is $$\Var(X)=\E[(X-\mu)^2].$$
    \end{definition}

    \begin{proposition}
        $\Var(X)=\E[X^2]-\E[X]^2$.
    \end{proposition}
    \begin{proof}
        \begin{align*}
            \Var(X)&=\E[(X-\mu)^2] \\
            &=\sum_x(x-\mu)^2p(x) \\
            &=\sum_x(x^2-2\mu x+\mu^2)p(x) \\
            &=\sum_xx^2p(x)-2\mu\sum_x xp(x)+\mu^2\sum_xp(x) \\
            &=\E[X^2]-2\mu^2+\mu^2 \\
            &=\E[X^2]-\mu^2
        \end{align*}
    \end{proof}

    \begin{example}
        Let $X_1$, $X_2$, $X_3$ be  the investment strategies from before.
        \begin{align*}
            \Var(X_1)=\E[(X_1-1)^2]&=0 \\
            \Var(X_2)=\E[(X_2-1)^2]&=999^2\cdot\frac1{1000}+(-1)^2\cdot\frac{999}{1000} \\
            &=\frac{999}{1000}(999+1)=999 \\
            &=\E[X_2^2]-\E[X_2]=\left(1000^2\cdot\frac{1}{1000}+0^2\cdot\frac{999}{1000}\right)-1^2 \\
            &=1000-1=999 \\
            \Var(X_3)=\E[(X_3-1)^2]&=\E[X_3^2]-\E[X_3] \\
            &=\left(\left(\frac{2000}{999}\right)^2\cdot\frac{999}{1000}+(-1000)^2\frac{1}{1000}\right)-1 \\
            &=\left(\frac{4000}{999}+1000\right)-1=1003\frac{4}{999}
        \end{align*}
    \end{example}

    \begin{definition}
        The \emph{standard deviation} of a random variable is the square root of its variance, often denoted by $\sigma(X)$.
    \end{definition}

    \begin{theorem}[Chebychev's Inequality]{}
        Let $X$ be a random variable with expectation $E[X]=\mu$. Then for any $t>0$, $$\P(|X-\mu|\geq t)\leq\frac{\Var(X)}{t^2}.$$
    \end{theorem}
    \begin{proof}
        Apply Markov's inequality to the nonnegative random variable $(X-\mu)^2$. Observe that
        $$\set{|X-\mu|\geq t}=\set{(X-\mu)^2\geq t^2}.$$

        By Markov, $$\P((X-\mu)^2\geq t^2)\leq\frac{\E[(X-\mu)^2]}{t^2}=\frac{\Var(X)}{t^2}.$$
    \end{proof}

    \begin{corollary}
        The probability that $X$ is at least $k$ standard deviations away from its expectation is $\leq\frac{1}{k^2}$.
    \end{corollary}

    \begin{remark} 
        Let $X$ be a random variable, $a,b\in\R$. Define $Y=aX+b$.

        By linearity, $\E[Y]=\E[aX+b]=a\E[X]+b$.

        What about the variance?
        \begin{align*}
            \Var(Y)&=\E[(Y-\E[Y])^2] \\
            &=\E[(aX+b-(a\E[X]+b))^2] \\
            &=\E[(a(X-E[X])^2)] \\
            &=a^2\E[(X-\E[x])^2]=a^2\Var(X)
        \end{align*}
    \end{remark}

\chapter{Discrete Distributions}

    \section{Binomial Distribution}
    Setting: \begin{itemize}
        \item run $n$ independent trial of a random experiment
        \item each trial is a success with probability $p$
        \item count the number of successes
    \end{itemize}

    Denoted by $\Bin(n,p)$.

    Distribution: The possible values are $0,1,2,\ldots,n$. The probability that we get $k$ successes is
    $$p(k)=\binom nkp^k(1-p)^{n-k}.$$
    
    \noindent Observation. $$\sum_kp(k)=\sum_{k=0}^n\binom nkp^k(1-p)^{n-k}=(p+(1-p))^n=1$$
    \begin{remark} When $n=1$, we get a Bernoulli distribution, defined by 
        $$X=\begin{cases}
            1 & \text{with probability }p \\
            0 & \text{with probability }1-p.
        \end{cases}$$

        Denoted by $\Ber(p)$.
        \end{remark}
    Therefore $$\Bin(n,p)=\text{ sum of $n$ independent Bernoulli random variables}.$$

    \textbf{Statistics.} Let $Y\sim\Ber(p)$ ($Y$ be a $\Ber(p)$ random variable). Then
    $$\E[Y]=1\cdot p+0\cdot(1-p)=p.$$

    Let $X\sim\Bin(n,p)$. Then $X=\sum_{i=1}^nX_i$ where each $X_i\sim\Ber(p)$ independently.\\
    $$\E[X]=\E\left[\sum_{i=1}^nX_i\right]=\sum_{i=1}^n\E[X_i]=\sum_{i=1}^np=\fbox{$np$}$$

    To calculate the expectation of the binomial distribution manually, we use the binomial theorem.
    \begin{equation*}
        \sum_{k=0}^n\binom nkx^ky^{n-k}=(x+y)^n \tag{binomial theorem}
    \end{equation*}
    \begin{equation*}
        \overtext{$\frac{\mathrm d}{\mathrm dx}$}{\Rightarrow}\sum_{k=0}^nk\binom nkx^{k-1}y^{n-k}=n(x+y)^{n-1}
    \end{equation*}

    Multiply both side by $x$,
    $$\sum_{k=0}^nk\binom nkx^ky^{n-k}=nx(x+y)^{n-1}.$$

    Substitute $x=p$, $y=1-p$, and we can get
    $$\E[X]=\sum_{k=0}^nk\binom nkp^k(1-p)^{n-k}=np(p+(1-p))^{n-1}=\fbox{$np$}.$$

    Now, to calculate the variance of the binomial distribution, we need to compute $\E[X^2]$. Observe
    $$\sum_{k=0}^nk\binom nkkx^ky^{n-k}=nx(x+y)^{n-1}$$
    $$\overtext{$\frac{\mathrm d}{\mathrm dx}$}{\Rightarrow}\sum_{k=0}^nk^2\binom nkkx^{k-1}y^{n-k}=n(x+y)^{n-1}+n(n-1)x(x+y)^{n-2}$$
    
    Multiply both side by $x$,
    $$\sum_{k=0}^nk^2\binom nkkx^ky^{n-k}=nx(x+y)^{n-1}+n(n-1)x^2(x+y)^{n-2}$$

    Substitute $x=p$, $y=1-p$, and we can get
    \begin{align*}
        \E[X^2]&=\sum_{k=0}^nk^2\binom nkp^k(1-p)^{n-k} \\
        &=np(p+(1-p))^{n-1}+n(n-1)p^2(p+(1-p))^{n-2}=\fbox{$np+n(n-1)p^2$}.
    \end{align*}

    Therefore
    \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=np+n(n-1)p^2-n^2p^2 \\
        &=np-np^2=\fbox{$np(1-p)$}
    \end{align*}

    Also, We can calculate the variance of Bernoulli distribution:
    $$X=\begin{cases}
        1 & \text{with probability }p \\
        0 & \text{with probability }1-p.
    \end{cases}$$
    $$X^2=\begin{cases}
        1 & \text{with probability }p \\
        0 & \text{with probability }1-p.
    \end{cases}$$
    $$\Rightarrow\E[X^2]=\E[X]=p$$
    \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=p-p^2=\fbox{$p(1-p)$}
    \end{align*}
    
    \begin{remark} 
        We have the following observation: \begin{enumerate}
            \item Let $X\sim\Bin(n,p)$. Then $\E[X]=np$ and $\Var(X)=np(1-p)$.\\
            By Chebychev we can know that $\P(|X-np|\geq t)\leq\frac{np(1-p)}{t^2}$.\\
            That is, even though there are $n+1$ values the distribution can take, 
            the probability it is outside an interval of with $\Theta(\sqrt n)$ around the expectation is very small.
            \item $\E[X^2]=\underbrace{\E[X(X-1)]}_{\sum_kk(k-1)p(k)}+\E[X]$.
        \end{enumerate}
    \end{remark}

    \section{Poisson Distribution}
    Setting: \begin{itemize}
        \item the number of earthquakes in Taiwan in a month
        \item on average, there are $\lambda$ earthquakes in a month
        \item divide into $n$ equal time intervals $\to$ expect $\frac{\lambda}{n}$ earthquakes in each interval
    \end{itemize}

    Assumption: \begin{itemize}
        \item At most one earthquakes per interval.
        \item Each interval is independent of the others.
    \end{itemize}

    The number of earthquakes $\sim\Bin(n,\tfrac\lambda n)$.

    Distribution: $$\P(k\text{ earthquakes in a month})\approx\binom nk\left(\frac{\lambda}{n}\right)^k\left(1-\frac{\lambda}{n}\right)^{n-k}$$
    
    Take $n\to\infty$,
    $$\binom nk\left(\frac{\lambda}{n}\right)^k=\frac{n(n-1)\cdots(n-k+1)}{k!}\left(\frac{\lambda}{n}\right)^k\to\frac{\lambda^k}{k!}$$
    $$\left(1-\frac{\lambda}{n}\right)^{n-k}=\frac{\left(1-\frac{\lambda}{n}\right)^n}{\left(1-\frac{\lambda}{n}\right)^k}\to\frac{e^{-\lambda}}{1}$$
    
    Therefore the Possion distribution with parameter $\lambda>0$, $\Poi(\lambda)$ has distribution
    $$p(k)=\frac{\lambda^ke^{-\lambda}}{k!}\quad\text{ for }k=0,1,2,\ldots.$$

    Fun fact. This is a distribution $p(k)\geq0$ for all $k\geq0$.
    \begin{align*}
        \sum_{k=0}^\infty p(k)&=\sum_{k=0}^\infty\frac{\lambda^ke^{-\lambda}}{k!} \\
        &=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{k!} \\
        &=e^{-\lambda}e^\lambda=1
    \end{align*}
    \begin{remark} 
        $\Poi(\lambda)$ is a good approximation for $\Bin(n,\tfrac{\lambda}{n})$ when $n$ is large.

        That is to say, Poisson distribution is appropriate when we have many independent events, each with small probability.
        
        For example, \begin{itemize}
            \item number of customers in a shop in an hour.
            \item number of people who will die in a day.
            \item radioactive decay.
        \end{itemize}
    \end{remark}

    \textbf{Statistics.} Let $X\sim\Poi(\lambda)$. The expectation is
    \begin{align*}
        \E[X]&=\sum_{k=0}^\infty kp(k) \\
        &=\sum_{k=0}^\infty k\cdot\frac{\lambda^ke^{-\lambda}}{k!} \\
        &=\sum_{k=1}^\infty\frac{\lambda^ke^{-\lambda}}{(k-1)!} \\
        &=\sum_{k=0}^\infty\frac{\lambda^{k+1}e^{-\lambda}}{k!} \\
        &=\lambda\sum_{k=0}^\infty\frac{\lambda^ke^{-\lambda}}{k!} \\
        &=\lambda\sum_{k=0}^\infty p(k)=\fbox{$\lambda$}
    \end{align*}

    The variance is \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=\E[X(X-1)]+\E[X]-\E[X]^2 \\
        &=\E[X(X-1)]+\lambda-\lambda^2
    \end{align*}

    where \begin{align*}
        \E[X(X-1)]&=\sum_{k=0}^\infty k(k-1)p(k) \\
        &=\sum_{k=0}^\infty k(k-1)\frac{\lambda^ke^{-\lambda}}{k!} \\
        &=\sum_{k=2}^\infty k(k-1)\frac{\lambda^ke^{-\lambda}}{k!} \\
        &=\sum_{k=2}^\infty\frac{\lambda^ke^{-\lambda}}{(k-2)!} \\
        &=\sum_{k=0}^\infty\frac{\lambda^{k+2}e^{-\lambda}}{k!}=\lambda^2
    \end{align*}

    Therefore \begin{align*}
        \Var(X)&=\E[X(X-1)]+\lambda-\lambda^2 \\
        &=\lambda^2+\lambda-\lambda^2=\fbox{$\lambda$}
    \end{align*}

    Like what we mentioned above, $\Poi\approx\Bin(n,\tfrac\lambda n)$,
    which has expectation $np=\lambda$ and variance $np(1-p)=n\cdot\frac{\lambda}{n}\left(1-\frac{\lambda}{n}\right)\approx\lambda$.\\

    \textbf{The Poisson Paradigm.}
    The Possion distribution is more widely applicable: 
    if we have $n$ events $E_1,E_2,E_3,\ldots,E_n$ such that \begin{itemize}
        \item $p_i=\P(E_i)$ is small for every $i$, and
        \item the events are ``weakly independent'': for $j\neq i$, $\P(E_i|E_j)\approx p_i$,
    \end{itemize}
    then if $\lambda=p_1+p_2+\cdots+p_n$,
    $\Poi(\lambda)$ is a good approximation to the number of events that occur.\\
    
    \begin{example} (See Example \ref{party}.)
        There are a party with $n$ people and $n$ hats.
        What is the probability that nobody gets their own hat?\\
        \textbf{Solution.} Let $E_i=\set{\text{$i$-th person gets own hat}}$. Then
        $\P(E_i)=\frac1n$, $\P(E_i|E_j)=\frac1{n-1}$.

        Therefore the Poisson paradigm applies.
        The number of correct hats $\approx\Poi(1)$.
        $$\P(\text{nobody gets own hat})\approx\frac{1^0e^{-1}}{0!}=\frac{1}{e}.$$
        $$\P(\text{exactly $k$ gets own hat})\approx\frac{1^ke^{-1}}{k!}=\frac{1}{k!e}.$$
    \end{example}

    \begin{example}
        Toss a fair coin $n$ times. Let $L_n$ denote the length of longest sequence of consecutive heads.
        \begin{align*}
            E&=\set{\text{there is a sequence of $k$ heads in a row}} \\
            &=\set{L_n\geq k} \\
            &=\bigcup_{i=1}^{n-k+1}E_i,\text{ where }E_i=\set{\text{tosses }i,i+1,\ldots,i+k-1\text{ are all heads}}
        \end{align*}

        We have $\P(E_i)=\frac1{2^k}$. However, these events are far from independence:
        $$\P(E_i|E_j)=\frac1{2^k}\text{ if }i-j\geq k,$$

        but $\P(E_i|E_{i-1})=\frac12$. So the Poisson paradigm does not apply in this setting.

        Fortunately, we can fix the problem by letting $E=\bigcup_{i=1}^{n-k+1}E_i'$, where
        $$E_i'=\begin{cases}
            \text{tosses }i,i+1,\ldots,i+k-1\text{ are all heads AND $i+k$ is tail } & \text{ if }1\leq i\leq n-k \\
            \text{tosses }n-k+1,n-k+2,\ldots,n\text{ are all heads} & \text{ if }i=n-k+1.
        \end{cases}$$

        Then $$\P(E_i')\begin{cases}
            \frac1{2^{k+1}} & \text{ if }1\leq i\leq n-k \text{ (fix outcome of $k+1$ tosses) } \\[10pt]
            \frac1{2^k} & \text{ if }i=n-k+1 \text{ (same as before) }
        \end{cases}$$

        Hence we have $$\P(E_i'|E_j')=\begin{cases}
            \P(E_i) & \text{ if }i,j\text{ are far apart } \\
            0 & \text{ if sequence overlap }\to\text{ close to $\P(E_i')$}. %\leftarrow\text{ differ is lon toss of the first sequence }
        \end{cases}$$

        Then Poisson paradigm applies. :)

        The number of $k$ heads followed by a tail at the end of tosses is 
        $$X_k\sim\Poi\left(\frac{n-k}{2^{k+1}}+\frac1{2^k}\right)=\Poi\left(\frac{n-k+2}{2^{k+1}}\right).$$
        $$\set{L_n\leq k}=\set{X_{k+1}=0}$$

        By the Poisson paradigm, \begin{align*}
            \P(X_{k+1}=0)&\approx\frac{\lambda_{k+1}^0e^{-\lambda_{k+1}}}{0!} \\
            &=e^{-\lambda_{k+1}},\text{ where }\lambda_{k+1}=\frac{n-k+1}{2^{k+2}}
        \end{align*}
        \begin{align*}
            \P(L_n\leq k)&\approx e^{-\frac{n-k+1}{2^{k+2}}} \\
            &\approx e^{-\frac{n}{2^{k+2}}}
        \end{align*}
        
        Finally,
        \begin{align*}
            \P(L_n=k)&=\P(L_n\leq k)-\P(L_n\leq k-1) \\
            &=e^{-\frac{n}{2^{k+2}}}-e^{-\frac{n}{2^{k+1}}} \\
            &=e^{-\frac{n}{2^{k+2}}}\left(1-e^{-\frac{n}{2^{k+2}}}\right)
        \end{align*}

        In order to have $\P(L_n=k)\not\to0$, we need $e^{-\frac{n}{2^{k+2}}}\not\to0$ and $e^{-\frac{n}{2^{k+2}}}\not\to1$.\\
        Therefore we need $k\approx\log_2n-2$.
    \end{example}

    \section{Geometric Distribution}
    Setting: \begin{itemize}
        \item Independent trials, successful with probability $p$.
        \item How many trials until our first success?
    \end{itemize}
    
    Denoted by $\Geom(p)$.

    Distribution: $\P(X=k)
    =\P(\overbrace{FFF\ldots F}^{\substack{\text{first $k-1$} \\ \text{trials failed}}}\underbrace{S}_{\substack{k\text{-th trial} \\ \text{success}}})
    =(1-p)^{k-1}p$

    Verify this is a valid distribution: \begin{align*}
        \sum_{k=1}^\infty\P(X=k)&=\sum_{k=1}^\infty(1-p)^{k-1}p \\
        &=p\sum_{k=1}^\infty(1-p)^{k-1} \\
        &=p\cdot\frac{1}{1-(1-p)}=\frac{p}{p}=1
    \end{align*}

    \textbf{Statistics.} To calculate the expectation of the geometry distribution, we observe
    \begin{equation*}
        \sum_{k=1}^\infty x^k=\frac{x}{1-x} \tag{geometric series}
    \end{equation*}
    \begin{equation*}
        \overtext{$\frac{\mathrm d}{\mathrm dx}$}{\Rightarrow}\sum_{k=1}^nkx^{k-1}=(1-x)^{-1}+x(1-x)^{-2}=\frac{1}{(1-x)^2}
    \end{equation*}

    Substitute $x=1-p$, and we can get
    $$\E[X]=p\sum_{k=1}^\infty k(1-p)^{k-1}=\fbox{$\frac1p$}.$$
    %\newcolumntype{M}{>{\begin{varwidth}{4cm}}l<{\end{varwidth}}}

    \begin{example}
        A casino has a game where you gave a 50\% chance of winning.

        If you bet \$$x$, then if you win, you get \$$2x$.

        If you lose, you get \$0.\\
        \underline{Q1.} What is your expected profit/loss?\\
        \textbf{Solution.} Let $X=$ profit. Then $$X=\begin{cases}
            \$x & \text{ if we win, }\P=\frac12 \\[10pt]
            -\$x & \text{ if we lose, }\P=\frac12.
        \end{cases}$$

        We have $\E[X]=\frac12\$x+\frac12(-\$x)=\$0$.\\
        \underline{Q2.} You aren't happy with losing, so your strategy is to keep betting \$1 until you win.
        What is your expected profit/loss?\\
        \textbf{Solution.} \begin{align*}
            X&=\$1-(\text{number of losses})\cdot\$1 \\
            &=\$2-\underbrace{(\text{number of trials})}_{\Geom\left(\frac12\right)}\cdot\$1
        \end{align*}
        
        Let $Y=$ number of trials until first win. Then $Y\sim\Geom\left(\tfrac12\right)$.
        Compute $$\E[X]=\E[2-Y]=2-\E[Y]=2-\frac{1}{\frac12}=\fbox{0}.$$
        \underline{Q3.} You have a new strategy: every time we lose, we double our bet and go again. Repeat until we win.\\
        \begin{center}
            \,\\\,
            \begin{tabular}[c]{m{1.2cm}|l|c}%{c|ccccc}
                number of games & profit & how much money we need \\ \hline
                1 & $+\$1$ & $\$1$ \\
                2 & $-\$1+\$2=+\$1$ & $\$1+\$2=\$3$ \\
                3 & $-\$1-\$2+\$4=+\$1$ & $\$1+\$2+\$4=\$7$ \\
                \vdots & \vdots & \vdots \\
                $k$ & $-\$1-\$2-\cdots-\$2^{k-2}+\$2^{k-1}=+\$1$ & $\$1+\$2+\$4+\cdots+\$2^{k-1}=\$2^k-1$
            \end{tabular}
        \end{center}

        Note that no matter how many times you lose before you win, you win \$1 back.

        Therefore $\E[X]=\$1$ since $\P(X=1)=1$.

        However, \begin{align*}
            \E[\text{amount of money needed}]&=\sum_{k=1}^\infty(2^k-1)\left(\frac12\right)^k \\
            &=\sum_{k=1}^\infty1^k-\sum_{k=1}^\infty\left(\frac12\right)^k \\
            &=\infty-1
        \end{align*}
    \end{example}

    \begin{example}
        Coupon collecter (See Homework 3.2.).

        There are $n$ types of coupons. Every coupon we get is uniformly random, independent of previous coupons.\\
        \underline{Q.} How many coupon do we need to collect them all?\\
        \textbf{Solution.} Let $X_i$ be the number of coupons we need to get the $i$-th new coupon after we got the $(i-1)$-th.
        The answer we want is $X_1+X_2+\cdots+X_n$.
        \begin{align*}
            X_1&=1 \tag{first coupon is always new} \\
            X_2&\sim\Geom\left(\frac{n-1}n\right) \\
            &\to\text{ each coupon is independent} \\
            &\to\text{ probability of being new }=\frac{n-1}{n} \\
            &\to\text{ repeat until we get a new one} \\
            X_i&\sim\Geom\left(\frac{n-i+1}{n}\right)
        \end{align*}

        Therefore \begin{align*}
            \E[X]&=\E\left[\sum_{i=1}^nX_i\right]=\sum_{i=1}^n\E[X_i] \tag{LoE} \\
            &=\sum_{i=1}^n\frac{1}{\frac{n-i+1}{n}}
            =\sum_{i=1}^n\frac{n}{n-i+1} \\
            &=\sum_{i=1}^n\frac{n}{i}
            =n\sum_{i=1}^n\frac{1}{i} \\
            &=nH_n\approx n\log n
        \end{align*}
    \end{example}

    Calculate the variance of $\Geom(p)$:
    \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=\E[X(X-1)]+\underbrace{\E[X]}_{\frac1p}-\underbrace{\E[X^2]}_{\frac1{p^2}}
    \end{align*}

    To calculate $\E[X(X-1)]$, observe
    $$\sum_{k=1}^\infty kx^{k-1}=\frac{1}{(1-x)^2}$$
    $$\overtext{$\frac{\mathrm d}{\mathrm dx}$}{\Rightarrow}\sum_{k=1}^\infty k(k-1)x^{k-2}=\frac{2}{(1-x)^3}$$
    
    Multiply both side by $x$,
    $$\sum_{k=1}^\infty k(k-1)x^{k-1}=\frac{2x}{(1-x)^3}.$$

    Substitute $x=1-p$, and we can get
    \begin{align*}
        \E[X(X-1)]&=\sum_{k=1}^\infty k(k-1)(1-p)^{k-1}p \\
        &=p\frac{2(1-p)}{(1-(1-p))^3}=\frac{2(1-p)}{p^2}
    \end{align*}

    Therefore 
    \begin{align*}
        \Var(X)&=\E[X(X-1)]+\E[X]-\E[X]^2 \\
        &=\frac{2(1-p)}{p^2}+\frac1p-\frac1{p^2}=\fbox{$\frac{1-p}{p^2}$}.
    \end{align*}

    \begin{example}
        Estimate $X=$ the number of dice rolls until the first 6.

        Then $X\sim\Geom(\tfrac16)$.
        $$\E[X]=\frac{1}{\frac16}=6$$
        $$\Var(X)=\frac{1-\frac16}{\frac{1}{36}}=30$$
    \end{example}

    \section{Other Distributions}
    \noindent \underline{Negative Binomial Distribution.} \begin{itemize}
        \item Repeat independent trials, each with success probability $p$, until $r$-th success.
        \item How many trials do we need?
    \end{itemize}
    \noindent Observation. When $r=1$, this is just $\Geom(p)$.

    In general, this is sum of $r$ independent $\Geom(p)$ variables.
    
    Distribution: $\P(X=n)=\binom{n-1}{r-1}p^r(1-p)^{n-r}$.\\

    \noindent \underline{Hypergeometric Distribution.} \begin{itemize}
        \item Bucket with $N$ balls, $m$ of which are good.
        \item We draw $n$ balls from the bucket.
        \item How many are good?
    \end{itemize}
    Distribution: $\P(X=k)=\frac{(\text{choice of $k$ good balls})(\text{choice of $N-k$ bad balls})}{(\text{choice of $N$ balls})}=\frac{\binom mk\binom{N-m}{m-k}}{\binom Nn}$.\\
    \textbf{Statistics.} We try to find the expectation of $X$.

    Imagine we draw the balls one at a time. Let $X_i$ be the indicator of the $i$-th ball being good.
    Then $X=\sum_{i=1}^nX_i$. \begin{align*}
        \E[X]&=\sum_{i=1}^n\E[X_i] \tag{LoE} \\
        &=\sum_{i=1}^n\P(X_i=1) \\
        &=\sum_{i=1}^n\P(i\text{-the ball is good})
    \end{align*}

    By careful obervation, we can find that any of the $N$ balls is equally likely to be the $i$-th ball.
    
    Therefore we can view the $i$-th ball as uniformly distributed.

    Then $\P(i\text{-th ball is good})=\frac mN$.
    Hence $\E[X]=\fbox{$\frac{nm}{N}$}$.\\

    In conclusion,
    \definecolor{lightgray}{rgb}{0.85,0.85,0.85}
    \begin{center}
        \begin{tabular}{r|m{4.7cm}cc}
            Distribution & Definition & Expectation & Variance \\ \hline
            \href{https://en.wikipedia.org/wiki/Binomial_distribution}{$\Bin(n,p)$} & number of successes in $n$ trials, each is independent with success probability $p$ & $np$ & $np(1-p)$ \\[10pt]
            ${\displaystyle\color{gray}\lim_{n\to\infty}\Bin(n,\tfrac{\lambda}{n})=\;}$\href{https://en.wikipedia.org/wiki/Poisson_distribution}{$\Poi(\lambda)$} & \rule{0pt}{3ex}number of rare independent events occurring in a fixed time frame & $\lambda$ & $\lambda$ \\[10pt]
            ${\color{gray}\text{NB}(1,p)=\;}$\href{https://en.wikipedia.org/wiki/Geometric_distribution}{$\Geom(p)$} & \rule{0pt}{3ex}number of trials needed, each is independent with success probability $p$, until first success & $\displaystyle\frac1p$ & $\displaystyle\frac{1-p}{p^2}$ \\
            \href{https://en.wikipedia.org/wiki/Negative_binomial_distribution}{$\text{NB}(r,p)$} & \rule{0pt}{3ex}number of trials needed, each is independent with success probability $p$, until $r$-th success & $\displaystyle\frac{r}{p}$ & {\color{lightgray}lack content} \\
            \href{https://en.wikipedia.org/wiki/Hypergeometric_distribution}{$\text{Hypergeometric}(N,m,n)$} & \rule{0pt}{3ex}$N$ outcomes, $m$ of which are good, select $n$ without replacement, number of good outcomes & $\displaystyle\frac{nm}{N}$ & $\displaystyle\frac{nm(N-m)(N-n)}{N^2(N-1)}$
        \end{tabular}
    \end{center}

\chapter{Continuous Random Variables}

    \section{Cumulative Distribution Function}
    
    \begin{definition}
        Let $X$ be a random variable. We define the \emph{cumulative distribution function} $F_X:\R\to[0,1]$ as
        $$F_X(x)=\P(X\leq x).$$
    \end{definition}

    \noindent Observation. Given $F_X$, we have $\P(a<X\leq b)=F_X(b)-F_X(a)$.

    This can be obtained from the identity $\set{X\leq b}=\set{X\leq a}\disj\set{a<X\leq b}$
    and thus $\P(X\leq b)=\P(X\leq a)+\P(a<X\leq b)$.

    Some other properties: \begin{itemize}
        \item $F_X(x)$ is increasing in $x$.
        \item $\lim_{x\to\infty}F_X(x)=1$. This is obtained from $$\lim_{x\to\infty}\P(\set{X\leq x})\overtext{continuity}
        =\P\left(\bigcup_{x\to\infty}\set{X\leq x}\right)=\P(X\in\R)=1.$$
        \item $\lim_{x\to-\infty}F_X(x)=0$.
        \item If $x_n\searrow x$, then $\lim_{n\to\infty}F_X(x_n)=F_X(x)$. (right continuity)\\
        This is obtained from $\bigcap_n\set{X\leq x_n}=\set{X\leq x}$.
    \end{itemize}
    \begin{remark} 
        If $x_n\nearrow x$, then $\bigcup_n\set{X\leq x_n}=\set{X<x}$, so
        $$\lim_{n\to\infty}F_X(x_n)=F_X(x)-\P(X=x).$$
    \end{remark}

    \begin{example}
        Let $X$ be the outcome of a roll of a die.
        Then the plot of its cdf $F_X$ is shown below:
        \begin{center}
            \begin{tikzpicture}
                \draw[help lines, color=gray!30, dashed] (-0.9,0) grid (7.9,6.9);
                \draw[->,thick] (-1,0)--(8,0) node[right]{$x$};
                \draw[->,thick] (0,-1)--(0,7) node[above]{$F_X(x)$};
                % \tkzInit[xmin = -5, xmax = 5, ymin = -5, ymax = 5]
                % \tkzAxeXY
                \foreach \x in {1,...,7}{
                    \node [anchor=north] at (\x,0) {\x};
                }
                \node [anchor=east] at (0,6) {1};

                \draw[blue, thick] (-1,0) -- (1,0);
                \node [circle, draw, fill=none, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (1,0) {};
                \foreach \a in {1,...,5}{
                    \draw[blue, thick] (\a,\a) -- (\a+1,\a);
                    \node [circle, draw, fill, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (\a,\a) {};
                    \node [circle, draw, fill=none, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (\a+1,\a) {};
                }
                \draw[blue, thick] (6,6) -- (7.9,6);
                \node [circle, draw, fill, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (6,6) {};

                \node [anchor=north east] (O) at (0,0) {$O$};
            \end{tikzpicture}
        \end{center}
    \end{example}

\chapter{Continuous Random Variable}

    Many random situation have uncountably many possible outcomes. For example,
    \begin{itemize}
        \item How long over time will this lecture run?
        \item How many seconds will it take for the first student to fall asleep?
    \end{itemize}

    \begin{definition}
        A random variable $X$ is said to be \emph{(absolutely) continuous} 
        if there is a function $f:\R\to\R_{\geq0}$ such that the cummulative distribution function is given by
        $$\P(X\leq x)=F_X(x)=\int_{-\infty}^{x}f(t)\dt.$$

        $f$ is called the \emph{probability density function} (pdf). 
    \end{definition}

    \noindent \underline{Q.} What does the pdf represent?

    Observe that $$\diff{x}F_X(x)=f_X(x).$$

    If $f_X$ is continuous, then 
    \begin{align*}
        \P(x-\varepsilon\leq X\leq x+\varepsilon)&=\P(x+\varepsilon)-\P(x-\varepsilon) \\
        &=F_X(x+\varepsilon)-F_X(x-\varepsilon) \\
        &=\int_{x-\varepsilon}^{x+\varepsilon}f_X(t)\dt
    \end{align*}

    More generally, for any event $E\inc\R$, $\P(E)=\int_{E}f_X(t)\dt$.

    Since $$\int_{x-\varepsilon}^{x+\varepsilon}f_X(t)\dt\overtext{$\substack{\text{$f_X$} \\[2pt] \text{continuous}}$}\approx f_X(x)\dt=2\varepsilon f_X(x),$$

    therefore $f_X(x)$ approximately represents the likelihood of $X$ being near $x$.

    \begin{example}
        Let $$f(x)=\begin{cases}
            \frac{C}{x^3} & \text{ if }x\geq1 \\[5pt]
            0 & \text{ if }x<1
        \end{cases}$$

        for some constant $C$.\\
        \underline{Q.} What is $C$? What is $F(X)$?\\
        \textbf{Solution.} $$F(x)=\int_{-\infty}^xf(t)\dt=\begin{cases}
            0 & \text{ if }x<1 \\[5pt]
            \int_1^x\frac{C}{t^3}\dt=\left.\frac{-C}{2t^2}\right|_1^x=\frac C2-\frac{C}{2x^2} & \text{ if }x\geq1
        \end{cases}$$

        Since the total probability is 1, we have
        $$1=\lim_{x\to\infty}F(x)=\lim_{x\to\infty}\frac C2-\frac{C}{2x^2}=\frac C2$$
        $$\Rightarrow C=2.$$

        Therefore we have
        $$F(x)=\begin{cases}
            1-\frac{1}{x^2} & \text{ if }x\geq1 \\
            0 & \text{ if }x\leq1.
        \end{cases}$$
        %
    \end{example}
    
    \section{Expectation}

    In the discrete setting, $\E[X]=\sum_ix_i\cdot\P(X=x_i)$.
    
    For a continuous random variable, observe $$\P(x-\varepsilon\leq X<x+\varepsilon)\approx2\varepsilon f_X(x).$$

    Therefore we define 
    $$\E[X]=\int_{-\infty}^\infty tf_X(t)\dt.$$
    
    \begin{example}
        Let $X$ have pdf $$f(x)=\begin{cases}
            0 & \text{ if }x\leq1 \\
            \frac{2}{x^3} & \text{ if }x\geq1.
        \end{cases}$$
        \underline{Q.} What is $\E[X]$?\\
        \textbf{Solution.} \begin{align*}
            \E[X]&=\int_{-\infty}^{\infty} tf(t)\dt \\
            &=\int_1^\infty t\frac{2}{t^3}\dt \\
            &=\int_1^\infty \frac{2}{t^2}\dt \\
            &=\left.\frac{-2}{t}\right|_1^\infty=\fbox{2}
        \end{align*}
    \end{example}

    \begin{example}
        The lecturer walks from their office to the lecture hall.
        The time of the walk is a random variable $W$ with pdf $f_W$.
        \begin{itemize}
            \item If the lecturer arrives early, they incur a cost of $c$ per minute.
            \item If the lecturer arrives late, then they incur a cost of $k$ per minute.
        \end{itemize}
        \underline{Q1.} If the lecturer leaves the office $t$ before the lecture starts, what is the expected cost?\\
        \underline{Q2.} When should they leave to minimize the cost?\\
        \textbf{Solution.} % may insert some graph of f_w

        The cost if the walk takes $w$ minute is $$g_t(w):=\begin{cases}
            c(t-w) & \text{ if }w\leq t \\
            k(w-t) & \text{ if }w\geq t
        \end{cases}$$

        The expectation cost is \begin{align*}
            \E[g_t(w)]&=\int_{-\infty}^\infty g_t(w)f_W(w)\dt[w] \\
            &=\int_0^\infty g_t(w)f_W(w)\dt[w] \\
            &=\int_0^t g_t(w)f_W(w)\dt[w]+\int_t^\infty g_t(w)f_W(w)\dt[w] \\
            &=\int_0^t c(t-w)f_W(w)\dt[w]+\int_t^\infty k(w-t)f_W(w)\dt[w] \\
            &=:C(t)
        \end{align*}

        To minimize the expected cost, differentiate with respect to $t$.
        \begin{align*}
            \diff[C]{t}&=\diff{t}\left(\int_0^t c(t-w)f_W(w)\dt[w]+\int_t^\infty k(w-t)f_W(w)\dt[w]\right) \\
            &=\cancel{\left.c(t-w)f(w)\right|_{w=t}}+\int_0^t cf_W(w)\dt[w]-\int_t^\infty kf_W(w)\dt[w]-\cancel{\left.k(w-t)f(w)\right|_{w=t}} \\
            &=\int_0^t (c+k)f_W(w)\dt[w]-\int_0^\infty kf_W(w)\dt[w] \\
            &=(c+k)F_W(t)-k
        \end{align*}

        Setting the derivative equal to 0, \begin{align*}
            \diff[C]{t}&=0\iff(c+k)F_W(t)-k \\
            &\iff F_W(t)=\frac{k}{c+k}
        \end{align*}

        Therefore the optimal $t$ is $F_W^{-1}\left(\frac{k}{c+k}\right)$.
    \end{example}

    Observe that the linearity of expectation still works for continuous random variables (by the linearity of integral).
    
    \section{Variance}

    We define the variance as before
    $$\Var(X)=\E[(X-\E[X])^2]=\int_{-\infty}^{\infty}(t-\E[X])^2f(t)\dt.$$

    Alternatively, $\Var(X)=\E[X^2]-\E[X]^2$.
    
    \begin{example}
        Let
        $$f_X(x)=\begin{cases}
            \frac{2}{x^3} & \text{ if }x\geq1 \\[5pt]
            0 & \text{ if }x\leq1
        \end{cases}$$

        We saw that $\E[X]=2$. What is $\Var(X)$?

        Compute \begin{align*}
            \E[X^2]&=\int_{-\infty}^\infty t^2f(t)\dt \\
            &=\int_1^\infty t^2\cdot\frac{2}{x^3}\dt \\
            &=\int_1^\infty \frac{2}{t}\dt \\
            &=2\ln t\big|_1^\infty \\
            &=\infty \tag{!!}
        \end{align*}
    \end{example}

    \begin{example}
        Game show

        Two envelopes are with \$$x$ and one with \$$y$, and $1\leq x<y$.

        First choose an envelope and open it.
        Then decide whether to take it or take the other.\\
        \underline{Q.} What strategy can we use to maximize the chance of getting the more valuable envelope?\\
        \textbf{Solution.} Attempts to give a lower bound:

        50\%. Choose a random envelope and keep it no matter what.

        Can we do better from 50\% chance?

        Strategy: Choose a thresgold value \$$z$. Choose a random envelope.

        If the amount is less than \$$z$, switch. Otherwise we keep it.
        \begin{center}
            \begin{tabular}{c|c|c|c}
                \diagbox{Envelope we choose}{Threshold} & $z\leq x<y$ & $x<z\leq y$ & $x<y\leq z$ \\ \hline
                \$$x$ & \$$x$ & \$$y$ & \$$y$ \\ \hline
                \$$y$ & \$$y$ & \$$y$ & \$$x$
            \end{tabular}
        \end{center}
        \begin{align*}
            \P(\text{get \$}y)&=\P(\text{get \$}y|z\leq x<y)\P(z\leq x<y) \\
            &\quad+\P(\text{get \$}y|x<z\leq y)\P(x<z\leq y) \\
            &\quad+\P(\text{get \$}y|x<y\leq z)\P(x<y\leq z) \\
            &=\frac12\P(z\leq x<y)+\P(x<z\leq y)+\frac12\P(x<y\leq z) \\
            &=\frac12+\frac12\P(x<z\leq y)
        \end{align*}

        Choose $z$ according to a continuous random variable with $\P(z\in(x,y])>0$ for all $1\leq x<y$.
        For example, $f_X(x)=\begin{cases}
            \frac{2}{x^3} & \text{ if }x\geq1 \\[5pt]
            0 & \text{ if }x\leq1.
        \end{cases}$
    \end{example}

\chapter{Continuous Distributions}

    \section{Uniform Distribution}
    Setting: Want to choose a uniformly random number in $\alpha,\beta$. \begin{itemize}
        \item run $n$ independent trial of a random experiment
        \item each trial is a success with probability $p$
        \item count the number of successes
    \end{itemize}

    Denoted by $\Unif(\alpha,\beta)$. The pdf is $f(x)=\frac{1}{\beta-\alpha}\chi_{(\alpha,\beta)}$.
    \begin{center}
        \begin{tikzpicture}
            \draw[->,thick] (-1,0)--(4,0) node[right]{$x$};
            \draw[->,thick] (0,-1)--(0,2) node[above]{$f(x)$};
            
            \draw[blue, thick] (-1,0) -- (1,0);
            \node [circle, draw, fill, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (1,0) {};
            \draw[blue, thick, dashed] (1,1) -- (1,0);
            \node [anchor=north] at (1,0) {$\alpha$};
            
            \draw[blue, thick] (1,1) -- (3,1);
            \node [circle, draw, fill=none, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (1,1) {};
            \node [circle, draw, fill=none, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (3,1) {};
            \draw[blue, thick, dashed] (1,1) -- (0,1);
            \node [anchor=east] at (0,1) {$\frac1{\beta-\alpha}$};
            
            \draw[blue, thick] (3,0) -- (4,0);
            \node [circle, draw, fill, line width=.5pt, color=blue, inner sep=0pt, minimum size=3pt] (ca) at (3,0) {};
            \draw[blue, thick, dashed] (3,1) -- (3,0);
            \node [anchor=north] at (3,0) {$\beta$};

            \node [anchor=north east] (O) at (0,0) {$O$};
        \end{tikzpicture}
    \end{center}

    \textbf{Statistics.} The cummulative distribution function is $$F_X(x)=\P(X\leq x)=\int_{-\infty}^xf(t)\dt=\begin{cases}
        0 & \text{ if }x\leq\alpha \\[5pt]
        \int_\alpha^x\frac1{\beta-\alpha}\dt=\frac{x-\alpha}{\beta-\alpha} & \text{ if }\alpha<x<\beta\\[5pt]
        1 & \text{ if }x\geq\beta
    \end{cases}$$
    \begin{align*}
        \E[X]&=\int_{-\infty}^\infty tf(t)\dt \\
        &=\int_\alpha^\beta\frac{t}{\beta-\alpha}\dt \\
        &=\frac{1}{\beta-\alpha}\left.\frac12t^2\right|_\alpha^\beta \\
        &=\frac{\beta^2-\alpha^2}{2(\beta-\alpha)}=\fbox{$\frac{\beta+\alpha}{2}$}
    \end{align*}
    \begin{align*}
        \E[X^2]&=\int_{-\infty}^\infty t^2f(t)\dt \\
        &=\int_\alpha^\beta\frac{t^2}{\beta-\alpha}\dt \\
        &=\frac{1}{\beta-\alpha}\left.\frac13t^3\right|_\alpha^\beta \\
        &=\frac{\beta^3-\alpha^3}{2(\beta-\alpha)}=\frac{\beta^2+\alpha\beta+\alpha^2}{3}
    \end{align*}
    \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=\frac{\beta^2+\alpha\beta+\alpha^2}{3}-\left(\frac{\beta+\alpha}{2}\right)^2 \\
        &=\fbox{$\frac{(\beta-\alpha)^2}{12}$}
    \end{align*}

    \begin{example}
        Bus arrives every 15 minutes in the hour (e.g. 7:00, 7:15, 7:30, 7:45,...).
        A passenger arrives at a uniformly random time between 7:00 and 7:30.
        \begin{enumerate}
            \item What is $\P(\text{wait > 5 minutes})$:
            \item What is $\E[\text{waiting time}]$?
        \end{enumerate}
        \textbf{Solution.} 
        \begin{enumerate}
            \item Let $X=$ arrival time and $X\sim\Unif(0,30)$.
            \begin{align*}
                \P(\text{wait > 5 minutes})&=\P(X\in(0,10)\cup(15,25)) \\
                &=\P(X\in(0,10))+\P(X\in(15,25)) \\
                &=\int_{0}^{10}\frac{1}{30}\dt+\int_{15}^{25}\frac{1}{30}\dt \\
                &=\frac{20}{30}=\fbox{$\frac{2}{3}$}
            \end{align*}
            \item Let $W=$ waiting time.
            $$W=\begin{cases}
                15-x & \text{ if }0\leq x\leq 15 \\
                30-x & \text{ if }15<x\leq30
            \end{cases}$$
            \begin{align*}
                \E[W]&=\int_{0}^{30}W(t)f(t)\dt \\
                &=\frac1{30}\int_{0}^{15}15-t\dt+\frac1{30}\int_{15}^{30}15-t\dt \\
                &=\frac2{30}\int_{0}^{15}15-t\dt \\
                &=\frac1{15}\left(15^2-\frac{15^2}2\right)=\fbox{$\frac{15}2$}
            \end{align*}
        \end{enumerate}
    \end{example}

    \textbf{Generating Random Variables.} We have a continuous random variable $X$.
    Let $F_X(x)$ be its cummulative distribution function.

    Let $U\sim\Unif(0,1)$ be uniform in $(0,1)$.
    Let $Y=F_X^{-1}(U)$.

    \begin{claim}
        $Y$ has the same distribution as $X$.
    \end{claim}
    \begin{proof}
        $F_Y(x)=\P(Y\leq x)=\P(F_X^{-1}(U)\leq x)=\P(U\leq F_X(x))=F_U(F_X(x))=F_X(x)$.
    \end{proof}

    \section{Exponential Distribution}
    Let $X\sim\Exp(\lambda)$ with $\lambda>0$. The pdf is given by $$f(x)=\begin{cases}
        \lambda e^{-\lambda x} & \text{ if }x\geq0 \\
        0 & \text{ if }x<0
    \end{cases}$$

    Verify this is a valid distribution: 
    $$\int_{-\infty}^{\infty}f(t)\dt=\int_0^\infty\lambda e^{-\lambda t}\dt=\left.-e^{-\lambda t}\right|_0^\infty=1.$$
    
    The cdf is $F(x)=\int_{-\infty}^xf(t)\dt=\begin{cases}
        0 & \text{ if }x<0 \\
        \int_0^x\lambda e^{-\lambda x}\dt=\left.-e^{-\lambda t}\right|_0^x=1-e^{-\lambda x} & \text{ if }x\geq 0
    \end{cases}$
    \begin{center}
        \begin{tikzpicture}
            \draw[->] (-0.2,0) -- (4.2,0) node[right] {$x$};
            \draw[->] (0,-0.2) -- (0,2.2) node[above] {$y$};

            \draw[domain=0:4,smooth,variable=\x,blue] plot ({\x}, 
            {1.4 * exp(-0.7 * \x)});
            \node [anchor=south east,blue] (f) at (4,0.09) {$f(x)$};

            \draw[domain=0:4,smooth,variable=\x,red] plot ({\x}, 
            {2 - 2 * exp(-0.7 * \x)});
            \node [anchor=south east,red] (F) at (4,1.86) {$F(x)$};
        \end{tikzpicture}
    \end{center}

    Let $X\sim\Exp(\lambda)$.
    $$\P(X>s)=1-\P(X\leq s)=1-F_X(s)=1-(1-e^{-s\lambda})=e^{-s\lambda}$$
    $$\P(X>s+t|X>t)=\frac{\P(\set{X>s+t}\cap\set{X>t})}{\P(X>t)}$$

    Since $\set{X>s+t}\inc\set{X>t}$,
    $$\P(X>s+t|X>t)=\frac{\P(\set{X>s+t})}{\P(X>t)}=\frac{e^{-(s+t)\lambda}}{e^{-t\lambda}}=e^{-s\lambda}=\P(X>s).$$
    
    This is an important property of the exponential distribution:
    \begin{definition}
        We say that a distribution is $memoryless$ if $\P(X>s+t|X>t)=\P(X>s)$.
    \end{definition}
    For this reaon we often use exponentials to model the lifetime of appliances, radioactive decay, etc.\\
    
    \textbf{Statistics.} \begin{align*}
        \E[X]&=\int_{-\infty}^{\infty}tf(t)\dt \\
        &=\int_{0}^{\infty}t\lambda e^{-\lambda t}\dt \\
        &=\left.-te^{-\lambda t}\right|_0^\infty-\int_{0}^{\infty}-e^{-\lambda t}\dt \\
        &=\left.-\frac{1}{\lambda}e^{-\lambda t}\right|_0^\infty=\fbox{$\frac{1}{\lambda}$}
    \end{align*}
    \begin{align*}
        \E[X^2]&=\int_{-\infty}^{\infty}t^2f(t)\dt \\
        &=\int_{0}^{\infty}t\lambda e^{-\lambda t}\dt \\
        &=\left.-t^2e^{-\lambda t}\right|_0^\infty-\int_{0}^{\infty}-2te^{-\lambda t}\dt \\
        &=\frac2\lambda\int_{0}^{\infty}t\lambda e^{-\lambda t}\dt \\
        &=\frac2\lambda\E[X]=\frac{2}{\lambda^2}
    \end{align*}
    \begin{align*}
        \Var(X)&=\E[X^2]-\E[X]^2 \\
        &=\frac{2}{\lambda^2}-\left(\frac{1}{\lambda}\right)^2=\fbox{$\frac{1}{\lambda^2}$}
    \end{align*}

    \begin{example}
        The waiting time for a bus is exponentially distributed with expectation of $\frac{15}{2}$ minutes.
        What is the probability that we have to wait more than 5 minutes?\\
        \textbf{Solution.} From $\E[X]=\frac{15}{2}$ we can get $\lambda=\frac{2}{15}$.
        $$\P(X>5)=e^{-5\lambda}=\fbox{$e^{-\frac25}$}.$$
    \end{example}

    \begin{example}
        Passengers can take one of the two buses.
        \begin{itemize}
            \item First bus's waiting time is exponential, with expectation $\frac{15}{2}$.
            \item Second bus's is exponential with expectation 15.
        \end{itemize}
        \underline{Q.} What is the expected waiting time for a bus,
        if the two buses arrive independently?\\
        \textbf{Solution.} Let $(X,Y)=$ the waiting time for the (first, second) type of bus.
        Then $X\sim\Exp\left(\frac{2}{15}\right)$ and $Y\sim\Exp\left(\frac{1}{15}\right)$.
        
        To calculate $\E[\min(X,Y)]$, we need the distribution of $\min(X,Y)$.
        \begin{align*}
            \P(\min(X,Y)\leq t)&=1-\P(\min(X,Y)>t) \\
            &=1-\P(\set{X>t}\cap\set{Y>t}) \\
            &=1-\P(X>t)\P(Y>t) \\
            &=1-e^{-\frac{2}{15}t}e^{-\frac{1}{15}t}=1-e^{-\frac{3}{15}t}
        \end{align*}

        Magically, $\min(X,Y)\sim\Exp\left(\frac{3}{15}\right)$. (!!!)

        Therefore $\E[\min(X,Y)]=\frac{15}{3}=\fbox{5}$.
        
        Generally, If $X_i\sim\Exp(\lambda_i)$, then $\min(X_i)_{1\leq i\leq n}\sim\Exp\left(\sum_{i=1}^n\lambda_i\right)$.
    \end{example} % Continue debugging here

    \begin{theorem}{}
        The exponential distribution is the only memoryless distribution.
    \end{theorem}

    \begin{proof}
        Let $X$ be memoryless, and $g(X)=\P(X>x)$. Then \begin{align*}
            g(x+y)&=\P(X>x+y) \\
            &=\P(X>x+y|X>y)\P(X>y) \\
            &\overtext{memoryless}{=}\P(X>x)\P(X>y)=g(x)g(y)
        \end{align*}

        Let $g(1)=e^{-\lambda}$ for some $\lambda\geq0$. Then we have $g(x)=e^{-\lambda x}$ for all $x\in\Q$.

        Since $g$ is right continuous, $g(x)=e^{-\lambda x}$.

        Then we can obtain the pdf of $X$ and see that $X\sim\Exp(\lambda)$.
    \end{proof}

    \section{Normal Distribution}
    Also named Gaussian distribution.

    The normal distribution with mean $\mu$ and standard deviation $\sigma$, denoted $N(\mu,\sigma^2)$,
    is the continuous random variable with probability density function
    $$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\,x\in\R.$$ 
    \begin{center}
        \begin{tikzpicture}
            \draw[->] (-2.2,0) -- (2.2,0) node[right] {$x$};
            \draw[->] (0,-0.2) -- (0,2.2) node[above] {$y$};

            \draw[domain=-2:2,smooth,variable=\x,blue] plot ({\x}, 
            {8 / (sqrt(2) * pi) * exp(-\x * \x / 2)});
            \node [anchor=south east,blue] (f) at (2,1.3) {$f(x)$};
        \end{tikzpicture}
    \end{center}

    Verify this is a valid distribution: We need to show that $1=\int_{-\infty}^{\infty}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\dt$.

    Let $y=\frac{t-\mu}{\sigma}$ and $\diff[y]{t}=\frac{1}{\sigma}$. Then the integral becomes 
    \begin{align*}
        I=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-y^2}\dy
    \end{align*}

    Square the integral to get $I^2=\frac{1}{2\pi}\iint_{y,z\in\R^2}e^{-(y^2+z^2)}\dy\dt[z]$.

    Then switch to polar coordinates to get $I^2=\frac{1}{2\pi}\int_0^{2\pi}\int_0^\infty e^{-r^2}\cdot r\dt[r]\dt[\theta]=\int_0^\infty re^{-r^2}\dt[r]=1$.

    We call $N(0,1)$ the \emph{standard normal distribution}.
    
    Observation. If $X\sim N(\mu,\sigma^2)$, then $Z=\frac{X-\mu}{\sigma}\sim N(0,1)$.
    
    We can obtain this by calculating the cdf: \begin{align*}
        \P(Z\leq z)&=\P\left(\frac{X-\mu}{\sigma}\leq z\right) \\
        &=\P(X\leq\mu+z\sigma) \\
        &=\int_{-\infty}^{\mu+z\sigma}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{-(t-\mu)^2}{2\sigma^2}}\dt \\
        &=\int_{-\infty}^{z}\frac{1}{\sqrt{2\pi}}e^{-\frac{-y^2}{2}}\dy=\P(N(0,1)\leq z)
    \end{align*}

    So $Z\sim N(0,1)$.

    We denote the cdf of the standard normal distribution as 
    $$\Phi(z):=\P(Z\leq z)=\frac1{\sqrt{2\pi}}\int_{-\infty}^ze^{-\frac{t^2}{2}}\dt.$$

    Observe that $\Phi(z)=1-\Phi(-z)$ for $z>0$.

    Also if $X\sim N(\mu,\sigma^2)$, the cdf of $X$ is $F_X(x)=\Phi\bigg(\underbrace{\frac{x-\mu}{\sigma}}_{\text{``$z$-score''}}\bigg).$

    \begin{example}
        On a midterm exam, the grades are normally distributed with $\mu=60$, $\sigma=10$.

        Students get a B if they score between 75 and 85. What proportion of students get a B?\\
        \textbf{Solution.} Let $X\sim N(60,10)$, so $\frac{X-60}{10}\sim N(0,1)$.
        \begin{align*}
            \P(75\leq X\leq 85)&=\P\left(1.5\leq\frac{X-60}{10}\leq2.5\right) \\
            &=\P\left(\frac{X-60}{10}\leq2.5\right)-\P\left(\frac{X-60}{10}\leq1.5\right) \\
            &=\Phi(2.5)-\Phi(1.5) \\
            &\approx0.9938-0.9332=\fbox{0.0606}
        \end{align*}
    \end{example}

    \textbf{Statistics.} Let $Z\sim N(0,1)$. Compute the expectation
    $$\E[Z]=\frac1{\sqrt{2\pi}}\int_{-\infty}^{\infty}te^{-\frac{t^2}2}\dt=-\frac1{\sqrt{2\pi}}\left.e^{-\frac{t^2}2}\right|_{-\infty}^\infty=0.$$

    If $X\sim N(\mu,\sigma^2)$, then $X=\sigma Z+\mu$, so $\E[X]=\sigma\E[Z]+\mu=\fbox{$\mu$}$.

    Now compute the variance. \begin{align*}
        \E[Z^2]&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}t^2e^{-\frac{t^2}2}\dt \\
        &=\frac{1}{\sqrt{2\pi}}\left(t(-e^{-\frac{t^2}2})\big|_{-\infty}^{\infty}-\int_{-\infty}^{\infty}1\cdot-e^{-\frac{t^2}2}\dt\right) \\
        &=\int_{-\infty}^{\infty}1\cdot-e^{-\frac{t^2}2}\dt=1
    \end{align*}

    Then $\Var(Z)=\E[Z^2]-\E[Z]^2=1-0=1$.

    If $X\sim N(\mu,\sigma^2)$, then $\Var(X)=\Var(\sigma Z+\mu)=\sigma^2\Var(Z)=\fbox{$\sigma^2$}$.

    \begin{example}
        Transmitting a binary string.

        Send $n$ bits, each bit is 0 or 1, across a cable.

        Assume the cable introduces some noise $\sim N(0,0.3^2)$.

        That is, If we send 1, receiver gets $1+X$ where $X\sim N(0,0.3^2)$.

        If we send 0, receiver gets $0+X$ where $X\sim N(0,0.3^2)$.
        \begin{center}
            \begin{tikzpicture}
                \draw[->] (-1.7,0) -- (3.2,0) node[right] {$x$};
                \draw[->] (0,-0.2) -- (0,2.2) node[above] {$y$};

                \draw[dashed] (0.75,2.2) -- (0.75,-0.2) node[below] {0.5};
            
                \fill[blue!30] plot[domain=1.5:3, smooth] ({\x / 2}, 
                {8 / (sqrt(2) * pi) * exp(-\x * \x / 2)}) -- (1.5,0) -- (0.75,0) -- cycle;
                \draw[domain=-3:3,smooth,variable=\x,blue] plot ({\x / 2}, 
                {8 / (sqrt(2) * pi) * exp(-\x * \x / 2)});
                \node [anchor=east,blue] (f) at (-0.5,1.3) {send 0};

                \fill[red!30] plot[domain=-3:-1.5, smooth] ({(\x + 3) / 2}, 
                {8 / (sqrt(2) * pi) * exp(-\x * \x / 2)}) -- (0.75,0) -- (0,0) -- cycle;
                \draw[domain=-3:3,smooth,variable=\x,red] plot ({(\x + 3) / 2}, 
                {8 / (sqrt(2) * pi) * exp(-\x * \x / 2)});
                \node [anchor=west,red] (f) at (2,1.3) {send 1};
            \end{tikzpicture}
        \end{center}

        Decoding algorithm: If $x<0.5$, decode as 0. If $x>1.5$, decode as 1.\\
        \underline{Q.} What is the probability of misreading a bit (shaded area)?\\
        \textbf{Solution.} Let $X$ be the noise.
        \begin{align*}
            \P(\text{error})&=\P(X<-0.5) \\
            &=\P\left(\frac{X-0}{0.3}\leq\frac{-0.5-0}{0.3}\right) \\
            &=\Phi\left(-\frac{0.5}{0.3}\right)\approx\Phi(-1.67) \\
            &=\P(Z\leq-1.67)=\P(Z\geq1.67) \\
            &=1-\P(Z\leq1.67)=1-\Phi(1.67) \\
            &=1-0.9525=\fbox{0.0475}
        \end{align*}
    \end{example}

    \textbf{Normal Approximation to the Binomial.} 
    Recall that $\Bin(n,p)$ has mean $np$ and variance $np(1-p)$.

    If $p$ is constant in $(0,1)$ then as $n\to\infty$, $np,np(1-p)\to\infty$.
    Therefore we cannot use Poisson approximation.

    But we do get an approximately normal distribution!

    \begin{theorem}[de Moivre-Laplace]
        Fix $p\in(0,1)$. Then for any $a<b$, we have $$\P\left(a\leq\frac{X-np}{\sqrt{np(1-p)}}\leq b\right)
        \to\P(a\leq Z\leq b)=\Phi(b)-\Phi(a)$$

        where $X\sim\Bin(n,p)$, as $n\to\infty$.
    \end{theorem}
    \begin{proof}
        Later at \ref{nomral_approx}.
    \end{proof}
    
    \begin{remark} 
        \begin{enumerate}
            \item In practice, $np(1-p)\geq 10$ is a enough for a good approximation.
            \item Continuity correction: $\P(X=k)=\P\left(k-\frac12\leq X\leq k+\frac12\right)
            \approx\Phi\left(\frac{k+\frac12-np}{\sqrt{np(1-p)}}\right)-\Phi\left(\frac{k-\frac12-np}{\sqrt{np(1-p)}}\right)$.
        \end{enumerate}
    \end{remark}

    \begin{example}
        Toss a fair coin 40 times.\\
        \underline{Q.} What is the probability of getting 20 heads?\\
        \textbf{Solution.} Let $X=$ number of heads $\sim\Bin(40,0.5)$.\\
        $$\P(X=20)=\binom{40}{20}\left(\frac12\right)^{40}=\fbox{0.1254}$$

        On the other hand, the normal approximation gives
        \begin{align*}
            \P(X=20)&=\P(19.5\leq X\leq 20.5) \\
            &=\P\left(\frac{19.5-20}{10}\leq\frac{X-20}{10}\leq\frac{20.5-20}{10}\right) \\
            &\approx\Phi\left(\frac{0.5}{\sqrt{10}}\right)-\Phi\left(\frac{-0.5}{\sqrt{10}}\right) \\
            &=2\Phi\left(\frac{0.5}{\sqrt{10}}\right)-1 \\
            &\approx2\cdot\Phi(0.16)-1\approx0.1272
        \end{align*}
    \end{example}

    \begin{example}
        A country has a population of $N$.
        The government wants to estimate the level of support for a new initiative.

        Suppose $p$ proportion of the population is for, $1-p$ is against.
        
        The government do a poll: Choose $n$ people at random, ask if they are in favor.\\
        \underline{Q.} How large should $n$ be to estimate $p$ to within 1\% with probability 95\%?\\
        \textbf{Solution.} Let $X=$ number of people in favor $\sim\Bin(n,p)$. %$\sim\text{Hypergeometric}(N,Np,n)$.
        \begin{align*}
            &\P(\text{good approximation}) \\
            &=\P(pn-0.01n\leq X\leq pn+0.01n) \\
            &=\P\left(\frac{-0.01n}{\sqrt{np(1-p)}}\leq\frac{X-np}{\sqrt{np(1-p)}}\leq\frac{0.01n}{\sqrt{np(1-p)}}\right) \\
            &=\P\left(-0.02\sqrt n\leq\frac{X-np}{\sqrt{np(1-p)}}\leq0.02\sqrt n\right) \tag{since $p(1-p)\leq\frac14$} \\ 
            &\approx\Phi(0.02\sqrt n)-\Phi(-0.02\sqrt n) \\ 
            &=2\Phi(0.02\sqrt n)-1
        \end{align*}

        Therefore $\P(\text{good approximaiton})\geq 0.95$ if $\Phi(0.02\sqrt n)\geq0.975$.

        This can be satisfied if $0.02\sqrt n\geq1.96$ and thus \fbox{$n\geq9604$}.
    \end{example}

\chapter{Function of Random Variables}
    Let $X$ be a continuous random variable with pdf $f_X(x)$.

    Let $Y=g(X)$ be a function $g:\R\to\R$.\\
    \underline{Q.} What can we say about the distribution of $Y$?

    For example, let $Y=g(X)=X^2$. Then
    \begin{align*}
        \P(Y\leq y)&=\P(X^2\leq y) \\
        &=\P(-\sqrt y\leq X\leq\sqrt y) \\
        &=\int_{-\sqrt y}^{\sqrt y}f_X(t)\dt
    \end{align*}

    \begin{theorem}
        Let $Y=g(X)$ where $g$ is monotone increasing. 
        Then if $X$ has pdf $f_X$, the pdf of $Y$ is $$f_Y(y)=\begin{cases}
            0 & \text{ if }y\notin g(\R) \\[5pt]
            f_X(g^{-1}(y))\cdot\diff{y}(g^{-1}(y)) & \text{ otherwise }.
        \end{cases}$$

        where the sum is over all $x$ such that $g(x)=y$.
    \end{theorem}
    \begin{proof}
        \begin{align*}
            F_Y(y)&=\P(Y\leq y) \\
            &=\P(g(X)\leq y) \\
            &=\begin{cases}
                0 \text{ or } 1 & \text{ if }y\notin g(\R) \\[5pt]
                \P(X\leq g^{-1}(y)) & \text{ otherwise }
            \end{cases}
        \end{align*}

        Taking the derivative with respect to $y$;
        $$f_Y(y)=\diff{y}F_Y(y)=\begin{cases}
            0 & \text{ if }y\notin g(\R) \\[5pt]
            f_X(g^{-1}(y))\cdot\diff{y}(g^{-1}(y)) & \text{ otherwise }.
        \end{cases}$$

        Note that $\diff{y}(g^{-1}(y))=\frac{1}{g'(g^-1(y))}.$
    \end{proof}

    \section{Measurable Sets}
    \underline{Q.} Can we define a uniform probability distribution on $(0,1)$?
    i.e. $\P:2^{(0,1)}\to[0,1]$.

    Desirable properties: \begin{enumerate}[label=(\arabic*)]
        \item $\P((0,1))=1$.
        \item If $E_1,E_2,E_3,\ldots\in2^{(0,1)}$ are pairwise disjoint, 
        then $\P\left(\bigcup_n E_n\right)=\sum_n\P(E_n)$.
        \item Also, consider representing numbers in $(0,1)$ in binary.\\
        $$x=\frac{x_1}{2^1}+\frac{x_2}{2^2}+\frac{x_3}{2^3}+\ldots$$
        where $x_i\in\{0,1\}$.\\
        Let $A_i(x)=(x_1,x_2,\ldots,x_{i-1},1-x_i,x_{i+1},\ldots)$,
        where $x=(x_1,x_2,\ldots)$.\\
        For any $E\inc(0,1)$ and any $i\in\N$, We want $\P(A_i(E))=\P(E)$.
    \end{enumerate}

    \begin{theorem}[Vitali, 1905]{}
        Such a $\P$ does not exist.
    \end{theorem}
    \begin{proof}
        Define an equivalence relation on $\set{0,1}^\N$: $\vec x\sim \vec y$ if $|\set{i:x_i\neq y_i}|<\infty$.

        Let $\left[\vec x\right]$ denote the equivalence class of $\vec x$.

        \begin{claim} $\left[\vec x\right]$ is countable for all $\vec x$. \end{claim}
        This can be obtained by $$\left[\vec x\right]=\bigcup_{\substack{I\inc\N \\ |I|<\infty}}A_I(\vec x).$$
        where $A_I(\vec x)=A_{i_1}(A_{i_2}(\cdots A_{i_r}(\vec x)))$ and $I=\set{i_1,i_2,\ldots,i_r}$.
        
        Such $I$ is countable since the choices are $\binom{\N}{0}\cup\binom{\N}{1}\cup\binom{\N}{2}\cup\cdots$, countable terms.
        
        For each equivalence class, choose a representative, and let $E$ be the set of representatives.

        This naturally gives that $(0,1)=\bigdisj_{|I|<\infty}A_I(E)$.

        Therefore $1\overtext{(1)}=\P((0,1))=\P\left(\bigcup_{|I|<\infty}A_I(E)\right)\overtext{(2)}=\sum_{|I|<\infty}\P(A_I(E))\overtext{(3)}=\sum_{|I|<\infty}\P(E)=\infty$.\\
    \end{proof}

    {\noindent\color{red}[Insert the note on 4/17, 4/22 here.]}

\chapter{Independent Random Variables}

    \begin{definition}
        We say two random variables $X$ and $Y$ are \emph{independent} if $\P(X\in A,Y\in B)=\P(X\in A)\P(Y\in B)$ for all $A,B$.
    \end{definition}

    \begin{remark} 
        This is equivalent to $$F(a,b)=F_X(a)F_Y(b)\iff\begin{matrix}
            p(x,y)=p_X(x)p_Y(y) & \text{(discrete)} \\[5pt]
            f(x,y)=f_X(x)f_Y(y) & \text{(continuous).} \\
        \end{matrix}$$
    \end{remark}

    \begin{example}
        Let $(X,Y)$ be uniformly distributed in the unit square $[0,1]^2$.\\
        \underline{Q.} Are $X$ and $Y$ independent?\\
        \textbf{Solution.} We have $$f(x,y)=1_{[0,1]^2}(x,y)=1_{[0,1]}(x)\cdot1_{[0,1]}(y)$$

        Thus they are independent.
    \end{example}

    \begin{example}
        Let $(X,Y)$ be uniformly distributed in the triangle with vertices $(0,0)$, $(1,0)$, $(1,1)$.\\
        \underline{Q.} Are $X$ and $Y$ independent?\\
        \textbf{Solution.} We have $$\P(X\in(0,\tfrac12),Y\in(\tfrac12,1))=0$$

        but $$\P(X\in(0,\tfrac12))\P(Y\in(\tfrac12,1))=\frac{1}{4}\cdot\frac{1}{4}\neq0$$
        \begin{center}
            \begin{tikzpicture}
                \draw[->] (-0.2,0) -- (2.2,0) node[right] {$x$};
                \draw[->] (0,-0.2) -- (0,2.2) node[above] {$y$};
            
                \fill[blue!30] (0,0) -- (2,0) -- (2,2) -- cycle;
                \fill[red!50] (0,1) -- (1,1) -- (1,2) -- (0,2) -- cycle;

                \draw[dashed] (1,2) -- (1,0) node[below] {0.5};
                \draw[dashed] (2,1) -- (0,1) node[left] {0.5};
            \end{tikzpicture}
        \end{center}

        Thus they are not independent.
    \end{example}

    \section{Sums of Independent Random Variables}

    If $X$ and $Y$ are independent, with densities $f_X$ and $f_Y$ respectively, what is the distribution of their sum $X+Y$?\\
    \begin{align*}
        F_{X+Y}(a)&=\P(X+Y\leq a) \\
        &=\iint_{x+y\leq a}f(x,y)\dx\dy \\
        &=\iint_{x+y\leq a}f_X(x)f_Y(y)\dx\dy \tag{independence} \\
        &=\int_{-\infty}^\infty f_Y(y)\int_{-\infty}^{a-y}f_X(x)\dx\dy \\
        &=\int_{-\infty}^\infty f_Y(y)F_X(a-y)\dy \\
    \end{align*}

    Therefore \begin{align*}
        f_{X+Y}(a)&=\diff{a}F_{X+Y}(a) \\
        &=\diff{a}\int_{-\infty}^\infty f_Y(y)F_X(a-y)\dy \\
        &=\int_{-\infty}^\infty f_Y(y)\left(\diff{a}F_X(a-y)\right)\dy \\
        &=\int_{-\infty}^\infty f_X(x)f_Y(a-x)\dx
    \end{align*}

    which is the convolution of $f_X$ and $f_Y$. This gives us the following result:

    \begin{proposition}
        If $X$ and $Y$ are independent, then $$f_{X+Y}(a)=\int_{-\infty}^\infty f_Y(y)f_X(a-y)\dy=\int_{-\infty}^\infty f_X(x)f_Y(a-x)\dx.$$
    \end{proposition}

    \section{Sums of Uniform Random Variables}
    Let $X,Y$ be i.i.d (independent identically distributed) $\Unif(0,1)$ random variables.\\
    \underline{Q.} What is the distribution of $X+Y$?\\
    \textbf{Solution.} We have $$f_X(x)=f_Y(x)=\begin{cases}
        1 & \text{ if } x\in(0,1) \\
        0 & \text{ otherwise.}
    \end{cases}$$

    Therefore
    $$f_{X+Y}(a)=\int_{-\infty}^\infty f_Y(y)f_X(a-y)\dy=\int_0^1 f_X(a-y)\dx.$$

    Since $0\leq X,Y\leq1$, we have $f_{X+Y}(a)=0$ if $a<0$ or $a>2$.

    If $0\leq a\leq1$, then $$f_{X+Y}(a)=\int_0^1f_X(a-y)\dy=\int_0^a1\dy=a.$$

    If $1\leq a\leq2$, then $$f_{X+Y}(a)=\int_0^1f_X(a-y)\dy=\int_{a-1}^11\dy=1-(a-1)=2-a.$$
    \begin{center}
        \begin{tikzpicture}
            \draw[->] (-0.2,0) -- (4.2,0) node[right] {$a$};
            \draw[->] (0,-0.2) -- (0,2.2) node[above] {$f_{X+Y}(a)$};

            \draw[blue] (-0.1,0) -- (0,0) -- (2,2) -- (4,0) -- (4.1,0);
            \node[anchor=north east] (0,0) {0};
            \node[below] at (2,0) {1};
            \node[below] at (4,0) {2};
        \end{tikzpicture}
    \end{center}

    This is called ``the triangular distribution''.

    \noindent \underline{Q.} Suppose $X_1,X_2,\ldots,X_n$ are i.i.d $\Unif(0,1)$. What can we say about $\sum_{i=1}^nX_i$?
    \begin{proposition}
        For $n=1,2,\ldots$, $$f_{\sum_{i=1}^nX_i}(x)=\frac{x^{n-1}}{(n-1)!}\quad \text{ if }0\leq x\leq1.$$
    \end{proposition}
    \begin{proof}
        We have \begin{align*}
            f_{\sum_{i=1}^{n+1}X_i}(x)&=f_{X_{n+1}+\sum_{i=1}^nX_i}(x) \\
            &=\int_{-\infty}^\infty f_{X_{n+1}}(y)f_{\sum_{i=1}^nX_i}(x-y)\dy \\
            &=\int_0^1f_{\sum_{i=1}^nX_i}(x-y)\dy \\
            &=\int_0^x\frac{(x-y)^{n-1}}{(n-1)!}\dy \tag{induction hypothesis} \\
            &=\left.\frac{-(x-y)^n}{n!}\right|_0^x=\frac{x^n}{n!}
        \end{align*}

        By induction, we can get the result.
    \end{proof}

    \begin{example}
        A person has 10 cakes in their fridge.
        Each day their hunger is independent,  urging them to eat $x$ cakes, where $x\sim\Unif(0,1)$.\\
        \underline{Q.} What is the expected number of days until the first cake is finished?\\
        \textbf{Solution.} Let $N=$ number of days to finish the first cake.

        Let $S_n=$ the amount of cake eaten after $n$ days.

        Then $S_n=\sum_{i=1}^nX_i$ where $X_i\sim\Unif(0,1)$ i.i.d.

        By definition, $N=\min\set{n:S_n\geq1}$.
        \begin{align*}
            \P(N>n)&=\P(S_n<1) \\
            &=\P(S_n\leq1) \\
            &=\int_0^1f_{S_n}(x)\dx \\
            &=\int_0^1\frac{x^{n-1}}{(n-1)!}\dx \\
            &=\left.\frac{x^n}{n!}\right|_0^1=\frac{1}{n!}
        \end{align*}

        Therefore $$\P(N=n)=\P(N>n-1)-\P(N>n)=\frac{1}{(n-1)!}-\frac{1}{n!}=\frac{n-1}{n!}.$$

        Thus $$\E[N]=\sum_{k=1}^\infty k\cdot\P(N=k)=\sum_{k=1}^\infty\frac{k(k-1)}{k!}=\sum_{k=2}^\infty\frac{1}{(k-2)!}=\sum_{n=0}^\infty\frac{1}{n!}=\fbox{$e$}.$$
    \end{example}

    \subsection{Gamma Distribution}
    Recall that $$\Gamma(\alpha,\lambda):f_Y(y)=\begin{cases}
        \frac{\lambda e^{-\lambda y}(\lambda y)^{\alpha-1}}{\Gamma(\alpha)} & \text{ if }y\geq0 \\
        0 & \text{ otherwise.}
    \end{cases}$$

    \begin{proposition}
        If $X,Y$ are independent, $X\sim\Gamma(\alpha,\lambda)$, $Y\sim\Gamma(\beta,\lambda)$, 
        then $X+Y\sim\Gamma(\alpha+\beta,\lambda)$.
    \end{proposition}
    \begin{corollary}
        The sum of $n$ i.i.d $\Exp(\lambda)$ random variables is $\Gamma(n,\lambda)$.
    \end{corollary}
    \begin{proof}
        \begin{align*}
            f_{X+Y}(a)&=\int_{-\infty}^\infty f_Y(y)f_X(a-y)\dy \\
            &=\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_0^a\left( \lambda e^{-\lambda y}(\lambda y)^{\beta-1} \right)\left( \lambda e^{-\lambda(a-y)}(\lambda(a-y))^{\alpha-1} \right)\dy \\
            &=\frac{\lambda^{\alpha+\beta}e^{-\lambda a}}{\Gamma(\alpha)\Gamma(\beta)}\int_0^ay^{\beta-1}(a-y)^{\alpha-1}\dy \tag{*}
        \end{align*}

        Let $x=\frac ya$. Then 
        \begin{align*}
            (\text{*})&=\frac{\lambda^{\alpha+\beta}e^{-\lambda a}}{\Gamma(\alpha)\Gamma(\beta)}\int_0^1(xa)^{\beta-1}a^{\alpha-1}(1-x)^{\alpha-1}\cdot a\dx \\
            &=\frac{\lambda^{\alpha+\beta}e^{-\lambda a}}{\Gamma(\alpha)\Gamma(\beta)}\underbrace{\left(\int_0^1x^{\beta-1}(1-x)^{\alpha-1}\dx\right)}_{C_{\alpha,\beta}'}a^{\alpha+\beta-1}
        \end{align*}
        \begin{align*}
            \Rightarrow f_{X+Y}(a)&=C_{\alpha,\beta}'\cdot e^{-\lambda a}a^{\alpha+\beta-1} \\
            &=C_{\alpha,\beta}\cdot e^{-\lambda a}(\lambda a)^{\alpha+\beta-1} % WARNING
        \end{align*}

        where $C_{\alpha,\beta}=\frac{\lambda}{\Gamma(\alpha+\beta)}$
        since this pdf has the form of a Gamma distribution with rate $\lambda$ and order $\alpha+\beta$.
        
        Therefore $X+Y\sim\Gamma(\alpha+\beta,\lambda)$.
    \end{proof}

    \noindent \textbf{Application.} Let $Z_i\sim N(0,1)$ i.i.d.\\
    \underline{Q.} What is the distribution of $\sum_{i=1}^nZ_i^2$?\\
    \textbf{Solution.} The distribution of $Z_i^2$ is given by \begin{align*}
        F_{Z_i^2}(a)&=\P(Z_i^2\leq a) \\
        &=\P(-\sqrt a\leq Z_i\leq\sqrt a) \\
        &=\int_{-\sqrt a}^{\sqrt a}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\dx
    \end{align*}
    \begin{align*}
        \Rightarrow f_{Z_i^2}(a)&=\diff{a}F_{Z_i^2}(a) \\
        &=\diff{a}\left(\int_{-\sqrt a}^{\sqrt a}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\dx\right) \\
        &=\left.\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\right|_{x=\sqrt a}\cdot\diff{a}(\sqrt a)-\left.\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\right|_{x=-\sqrt a}\cdot\diff{a}(-\sqrt a) \\
        &=\frac{1}{\sqrt{2\pi}}e^{-\frac{a}{2}}\cdot\frac{1}{2\sqrt a}-\frac{1}{\sqrt{2\pi}}e^{-\frac{a}{2}}\cdot\frac{-1}{2\sqrt a} \\
        &=\frac{1}{\sqrt{2\pi a}}e^{-\frac{a}{2}} \\
        &=\frac{1}{\sqrt{2\pi}}e^{-\frac{a}{2}}\left(\frac a2\right)^{\frac12-1}
    \end{align*}

    Therefore $Z_i^2\sim\Gamma\left(\frac12,\frac12\right)$ (and $\Gamma\left(\frac12\right)=\sqrt\pi$).

    Hence $\sum_{i=1}^nZ_i^2\sim\Gamma\left(\frac n2,\frac12\right)$ has density function
    $\frac{\frac12\cdot e^{\frac12a}\cdot\left(\frac12a\right)^{\frac n2-1}}{\Gamma\left(\frac n2\right)}$
    where $$\Gamma\left(\frac n2\right)=\left(\frac n2-1\right)\Gamma\left(\frac n2-1\right)=\begin{cases}
        \left(\frac n2-1\right)! & \text{ if }n\text{ is even} \\
        \sqrt\pi\left(\frac n2-\frac12\right)! & \text{ if }n\text{ is odd}
    \end{cases}$$

    \begin{remark} 
        The distribution of $\sum_{i=1}^nZ_i^2$ is called the \emph{chi-squared distribution} with $n$ degrees of freedom, denoted by $\chi^2_n$.
    \end{remark}

    \subsection{Normal Distribution}

    \begin{proposition}
        Let $X_1,X_2,\ldots,X_n$ be independent $N(\mu_i,\sigma_i^2)$ random variables.

        Then $$\sum_{i=1}^nX_i\sim N\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i\right).$$
    \end{proposition}
    \begin{proof}
        We complete the proof in the following steps: \begin{enumerate}[label=(\arabic*)]
            \item Show that $n=2$ case implies the general case.
            \item $n=2$: Show that $\mu_1=\mu_2=0$ and $\sigma_1=1$, $\sigma_2=\sigma$ implies the general case.
            \item $n=2$: $\mu_1=\mu_2=0$, $\sigma_1=1$, $\sigma_2=\sigma$.
        \end{enumerate}
        (1) Proof by induction on $n$.

        \textbf{Base case.} The case $n=2$ is $X_1+X_2\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.

        \textbf{Induction step.} We have $$\sum_{i=1}^n X_i=\sum_{i=1}^{n-1}X_i+X_n$$ where
        $\sum_{i=1}^{n-1}X_i\sim N\left(\sum_{i=1}^{n-1}\mu_i,\sum_{i=1}^{n-1}\sigma_i^2\right)$ and $X_n\sim N(\mu_n,\sigma_n^2)$.
        
        By induction hypothesis we have the desired result.\\
        (2) Let $X_1\sim N(\mu_1,\sigma_1^2)$, $X_2\sim N(\mu_2,\sigma_2^2)$.

        Then \begin{align*}
            X_1+X_2&=(X_1-\mu_1)+(X_2-\mu_2)+(\mu_1+\mu_2) \\
            &=\sigma_1\Bigg(\underbrace{\frac{X_1-\mu_1}{\sigma_1}}_{\sim N(0,1)}
            +\underbrace{\frac{X_2-\mu_2}{\sigma_1}}_{\sim N\left(0,\frac{\sigma_2^2}{\sigma_1^2}\right)}\Bigg)+(\mu_1+\mu_2)
        \end{align*}

        By (3), we have $$\frac{X_1-\mu_1}{\sigma_1}+\frac{X_2-\mu_2}{\sigma_1}\sim N\left(0,1+\frac{\sigma_2^2}{\sigma_1^2}\right)$$
        $$\Rightarrow\sigma_1\left(\frac{X_1-\mu_1}{\sigma_1}+\frac{X_2-\mu_2}{\sigma_1}\right)\sim N(0,\sigma_1^2+\sigma_2^2)$$
        $$\Rightarrow X_1+X_2\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$$
        (3) Let $X_1\sim N(0,1)$, $X_2\sim N(0,\sigma^2)$ independently. Then 
        \begin{align*}
            f_{X+Y}(a)&=\int_{-\infty}^{\infty}f_X(a-y)f_Y(y)\dy \\
            &=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{\frac{-(a-y)^2}{2}}\cdot\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-y^2}{2\sigma^2}}\dy \\
            &=\int_{-\infty}^{\infty}\frac{1}{2\pi\sigma}e^{\frac{-a^2}{2}}\cdot e^{-\frac12\left(y^2+\frac{y^2}{\sigma^2}-2ay\right)}\dy
        \end{align*}

        Compute \begin{align*}
            y^2+\frac{y^2}{\sigma^2}-2ay&=y^2\cdot\frac{\sigma^2+1}{\sigma^2}-2ay \\
            &=\frac{\sigma^2+1}{\sigma^2}\left(\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2-\left(\frac{\sigma^2}{\sigma^2+1}\right)^2a^2\right) \\
            &=\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2-\frac{\sigma^2}{\sigma^2+1}a^2
        \end{align*}

        Therefore \begin{align*}
            f_{X+Y}(a)&=\int_{-\infty}^{\infty}\frac{1}{2\pi\sigma}e^{\frac{-a^2}{2}}\cdot 
            e^{-\frac12\left(\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2-\frac{\sigma^2}{\sigma^2+1}a^2\right)}\dy \\
            &=\int_{-\infty}^{\infty}\frac{1}{2\pi\sigma}e^{\frac{-a^2}{2}+\frac{\sigma^2}{\sigma^2+1}\cdot\frac{a^2}{2}}\cdot 
            e^{-\frac12\cdot\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2}\dy \\
            &=\int_{-\infty}^{\infty}\frac{1}{2\pi\sigma}e^{-\frac12\cdot\frac{1}{\sigma^2+1}\cdot a^2}\cdot 
            e^{-\frac12\cdot\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2}\dy \\
            &=\frac{1}{2\pi\sigma}e^{-\frac{a^2}{2(\sigma^2+1)}}\int_{-\infty}^{\infty}
            e^{-\frac12\cdot\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2}\dy
        \end{align*}

        Let $z=y-\frac{\sigma^2a}{\sigma^2+1}$ and $\dy=\dt[z]$. So
        $$\int_{-\infty}^{\infty}e^{-\frac12\cdot\frac{\sigma^2+1}{\sigma^2}\left(y-\frac{\sigma^2a}{\sigma^2+1}\right)^2}\dy
        =\int_{-\infty}^{\infty}e^{-\frac12\cdot\frac{\sigma^2+1}{\sigma^2}z^2}\dt[z]:=C$$

        Then $f_{X+Y}(a)=C'e^{-\frac{a^2}{2(\sigma^2+1)}}$ for some constant $C'$.

        Hence $X+Y$ is a normal random variable with mean 0 and variance $1+\sigma^2$.
    \end{proof}

    \noindent \textbf{Application.} There is a stock with price $S(n)$ after $n$ weeks.

    We introduce the definition:
    \begin{definition}
        $X$ is \emph{log-normally distributed} with parameters $(\mu,\sigma)$ if $\log(X)\sim N(\mu,\sigma^2)$.
    \end{definition}
    The random variables $\frac{S(n)}{S(n-1)}$ are independent, log-normed with $(\mu,\sigma)=(0.0165,0.0730)$.
    \begin{enumerate}
        \item What is the probability that $S(n)$ increases every week for a year?
        \item What is the probability that $S(n)$ increases for at least 30 weeks in a year?
        \item What is the probability that the value of the stock increase by the end of the year?
    \end{enumerate}
    \textbf{Solution.}
    \begin{enumerate}
        \item We have $$\P(S(n)\geq S(n-1))=\P\left(\frac{S(n)}{S(n-1)}\geq1\right)=\P\left(\log\left(\frac{S(n)}{S(n-1)}\right)>0\right)$$
        where $\log\left(\frac{S(n)}{S(n-1)}\right)\sim N(0.0165,0.0730^2)$.\\
        Therefore the probability is equal to $\P\left(Z\geq\frac{0-0.0165}{0.0730}\right)=\Phi\left(\frac{0.0165}{0.0730}\right)\approx0.5894$.\\
        Then we can compute
        \begin{align*}
            &\P(S(n)\geq S(n-1)\text{ for }n=1,2,\ldots,52) \\
            &=\prod_{n=1}^{52}\P(S(n)\geq S(n-1)) \\
            &\approx0.5894^{52}\approx\fbox{$1.15\times10^{-12}$}
        \end{align*}
        \item The goal is $\P(\text{stock price goes up for $\geq30$ weeks in the year})$.\\
        Let $X=$ the number of weeks the stock price goes up in the year. Then $X\sim\Bin(52,0.5894)$.
        We have 
        $$\P(X\geq30)=\sum_{k=30}^{52}\binom{52}{k}0.5894^k\times0.4106^{52-k}.$$
        We can use a normal approximation $$X\simeq Y\sim N(52\times0.5894,52\times0.5894\times0.4106).$$
        Then we can compute
        $$\P(X\geq30)\approx\P(Y\geq29.5)=\P\left(Z\geq\frac{29.5-\mu}{\sigma}\right)\approx\fbox{0.627}.$$
        \item \begin{align*}
            \P(S(52)\geq S(0))&=\P\left(\frac{S(52)}{S(0)}\geq1\right) \\
            &=\P\left(\frac{S(52)}{S(51)}\times\frac{S(51)}{S(50)}\times\cdots\times\frac{S(1)}{S(0)}\geq1\right) \\
            &=\P\left(\log\left(\frac{S(52)}{S(51)}\times\frac{S(51)}{S(50)}\times\cdots\times\frac{S(1)}{S(0)}\right)\geq0\right) \\
            &=\P\left(\sum_{n=1}^{52}\log\left(\frac{S(n)}{S(n-1)}\right)\geq0\right)\approx\fbox{0.9484}
        \end{align*}
        since $\log\left(\frac{S(n)}{S(n-1)}\right)\sim N(0.0165,0.0730^2)$ and by independence, 
        $\sum_{n=1}^{52}\log\left(\frac{S(n)}{S(n-1)}\right)\sim N(52\times0.0165,52^2\times0.0730^2)$.
    \end{enumerate}

    \section{Sums of Discrete Random Variables}

    Let $X$ and $Y$ be independent discrete random variables.

    To calculate $\P(X+Y=a)$, we partition the sample space based on value of $X$, and use total probability.
    \begin{align*}
        &\P(X+Y=a) \\
        &=\sum_{x\in\text{Range}(X)}\P(X+Y=a|X=x)\P(X=x) \\
        &=\sum_x\P(Y=a-x|X=x)\P(X=x) \\
        &=\sum_x\P(Y=a-x)\P(X=x) \tag{independence} \\
        &=\sum_x p_Y(a-x)p_X(x)
    \end{align*}

    \begin{example}
        Let $X\sim\Poi(\lambda_1)$ and $Y\sim\Poi(\lambda_2)$ independently.
        What is $\P(X+Y=k)$?\\
        \textbf{Solution.} Compute \begin{align*}
            \P(X+Y=k)&=\sum_x\P(Y=k-x)\P(X=x) \\
            &=\sum_{x=0}^k\P(Y=k-x)\P(X=x) \\
            &=\sum_{x=0}^k\frac{e^{-\lambda_2}\lambda_2^{k-x}}{(k-x)!}\cdot\frac{e^{-\lambda_1}\lambda_1^x}{x!} \\
            &=e^{-(\lambda_1+\lambda_2)}\sum_{x=0}^k\frac{\lambda_1^x\lambda_2^{k-x}}{x!(k-x)!} \\
            &=\frac{e^{-(\lambda_1+\lambda_2)}}{k!}\sum_{x=0}^k\frac{k!}{x!(k-x)!}\lambda_1^x\lambda_2^{k-x} \\
            &=\frac{e^{-(\lambda_1+\lambda_2)}}{k!}\sum_{x=0}^k\binom kx\lambda_1^x\lambda_2^{k-x} \\
            &=\frac{e^{-(\lambda_1+\lambda_2)}}{k!}(\lambda_1+\lambda_2)^k \tag{binomial thm}
        \end{align*}

        Therefore $X_1+X_2\sim\Poi(\lambda_1+\lambda_2)$.
    \end{example}

\chapter{Conditional Distributions}

    Setting: We have two (not necessarily independent) random variables. 
    If we know the value of one, what can we say about the other?
    \section{Discrete Conditional Distribution} First we introduce the notation:
    \begin{definition}
        Given discrete random variables $X$ and $Y$,
        the \emph{conditional probability mass function of $X$ given $Y=y$} is
        $$p_{X|Y}(x|y):=\P(X=x|Y=y)=\frac{\P(X=x,Y=y)}{\P(Y=y)}=\frac{p(x,y)}{p_Y(y)}.$$
    \end{definition}
    
    \noindent Observation. If $X$ and $Y$ are independent, then $p_{X|Y}(x|y)=p_X(x)$.
    
    This is followed by $p_{X|Y}(x|y)=\frac{p(x,y)}{p_Y(y)}\overtext{indep.}{=}\frac{p_X(x)p_Y(y)}{p_Y(y)}$.

    \begin{example}
        Let $X\sim\Poi(\lambda_1)$ and $Y\sim\Poi(\lambda_2)$ independently.

        What is $\P(X=k|X+Y=n)$?\\
        \textbf{Solution.} We have $$\P(X=k|X+Y=n)=\frac{\P(X=k,X+Y=n)}{\P(X+Y)=n}=\frac{\P(X=k,Y=n-k)}{\P(X+Y)=n}.$$
        
        Since $X$ and $Y$ are independent, we have the following facts:
        \begin{enumerate}[label=(\arabic*)]
            \item $\P(X=k,Y=n-k)=\P(X=k)\P(Y=n-k)=\frac{e^{-\lambda_1}\lambda_1^k}{k!}\cdot\frac{e^{-\lambda_2}\lambda_2^{n-k}}{(n-k)!}$.
            \item $X+Y\sim\Poi(\lambda_1+\lambda_2)$, so $\P(X+Y=n)=\frac{e^{-(\lambda_1+\lambda_2)}(\lambda_1+\lambda_2)^n}{n!}$
        \end{enumerate}

        Therefore \begin{align*}
            &\P(X=k|X+Y=n) \\
            &=\frac{\frac{\cancel{e^{-\lambda_1}}\lambda_1^k}{k!}\cdot\frac{\cancel{e^{-\lambda_2}}\lambda_2^{n-k}}{(n-k)!}}{\frac{\cancel{e^{-(\lambda_1+\lambda_2)}}(\lambda_1+\lambda_2)^n}{n!}} \\
            &=\binom nk\left(\frac{\lambda_1}{\lambda_1+\lambda_2}\right)^k\left(\frac{\lambda_2}{\lambda_1+\lambda_2}\right)^{n-k}
        \end{align*}

        Hence $X|(X+Y=n)\sim\Bin\left(n,\tfrac{\lambda_1}{\lambda_1+\lambda_2}\right)$.
    \end{example}

    \begin{example}
        Let $X\sim\Bin(n,p)$ and $\vec{o}=$ order of outcomes $\in\set{S,F}^n$ consists of sucesses and failures.
        \begin{claim}
            For any vector $\vec{v}$ with exactly $k$ $S$'s, we have $p_{\vec o|X}(\vec v|k)=\frac{1}{\binom nk}$.\\
            i.e. the order of outcomes is uniformly random once we condition on the number of successes.
        \end{claim}
        \begin{proof} By definition,
            \begin{align*}
                p_{\vec o|X}(\vec v|k)&=\frac{\P(\vec o=\vec v,X=k)}{\P(X=k)} \\
                &=\frac{\P(\vec o=\vec v)}{\P(X=k)} \tag{$\set{\vec o=\vec v}\inc\set{X=k}$} \\
                &=\frac{p^k(1-p)^{n-k}}{\binom nkp^k(1-p)^{n-k}}=\frac{1}{\binom nk}
            \end{align*}
        \end{proof}
        \begin{remark} 
            If $X$, $Y$ are independent, then $\underbrace{p_{X|Y}(x|y)}_{\text{conditonal}}=\underbrace{p_X(x)}_{\text{marginal}}$.

            This result is from the generalization of the above proof $$p_{X|Y}(x|y)=\frac{p(x,y)}{p_Y(y)}\overtext{indep.}{=}\frac{p_X(x)p_Y(y)}{p_Y(y)}=p_X(x).$$
        \end{remark}
    \end{example}

    \section{Continuous Conditional Distribution}
    \begin{definition}
        Given continuous random variables $X$ and $Y$, 
        the \emph{conditional probability density function of $X$ given $Y=y$} is
        $$f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}.$$
    \end{definition}
    Then, given a set $A$, $\P(X\in A|Y=y)=\int_A f_{X|Y}(x|y)\dx$.
    
    Note that Even though $\P(Y=y)=0$, this allows us to condition in the value of $Y$.\\

    Justification: Consider \begin{align*}
        &\P(x\leq X\leq x+\dx[n]|y\leq Y\leq y+\dy[n]) \text{ as }\dx[n],\dy\to0 \\
        &=\frac{\P(x\leq X\leq x+\dx[n]|y\leq Y\leq y+\dy[n])}{\P(y\leq Y\leq y+\dy[n])}
    \end{align*}

    where
    \begin{align*}
        \P(y\leq Y\leq y+\dy[n])&=\int_y^{y+\dy[n]}f_Y(t)\dt \\
        &\approx f_Y(y)\dy
    \end{align*}
    \begin{align*}
        \P(x\leq X\leq x+\dx[n]|y\leq Y\leq y+\dy[n])&=\int_x^{x+\dx[n]}\int_y^{y+\dy[n]}f(u,v)\dt[u]\dt[v] \\
        &\approx f(x,y)\dx\dy
    \end{align*}

    Therefore
    $$f_{X|Y}(x|y)\dx\approx\P(x\leq X\leq x+\dx[n]|y\leq Y\leq y+\dy[n])\approx\frac{f(x,y)\dx\dy}{f_Y(y)\dy}$$
    $$\Rightarrow f_{X|Y}(x|y)\approx\frac{f(x,y)}{f_Y(y)}$$

    \begin{example}
        Let $$f_{X,Y}(x,y)=\begin{cases}
            \frac{e^{-\frac xy}e^{-y}}{y} & \text{ if }x,y>0 \\
            0 & \text{ otherwise.}
        \end{cases}$$
        \underline{Q.} What is $\P(X>1|Y=y)$?\\
        \textbf{Solution.} $$\P(X>1|Y=y)=\int_1^\infty f_{X|Y}(x|y)\dx$$

        where $f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}$.
        \begin{align*}
            f_Y(y)&=\int_{-\infty}^\infty f(x,y)\dx \\
            &=\int_0^\infty\frac{e^{-\frac xy}e^{-y}}{y}\dx \\
            &=\frac{e^{-y}}{y}\int_0^\infty e^{-\frac xy}\dx \\
            &=\frac{e^{-y}}{y}\cdot -ye^{-\frac xy}\big|_{x=0}^\infty \\
            &=e^{-y}
        \end{align*}

        Therefore
        $$f_{X|Y}(x,y)=\frac{\frac{e^{-\frac xy}e^{-y}}{y}}{e^{-y}}=\frac1ye^{-\frac xy}.$$

        Then $$\P(X>1|Y=y)=\int_1^\infty\frac1ye^{-\frac xy}\dx=-e^{-\frac xy}\big|_{x=1}^\infty=\fbox{$e^{-\frac 1y}$}.$$
    \end{example}

    \begin{example}
        We have a possibly weighted coin.
        It comes up heads with probability, where $p\sim\Unif(0,1)$.

        We toss the coin $n$ times, and find that we get $k$ heads.

        What is the distribution of $p$ given this outcome?\\
        \textbf{Solution.} Let $X=$ the number of heads. By Bayes' theorem,
        \begin{align*}
            f_{p|X}(p'|k)&=\frac{\P(\set{X=k}\cap\set{p=p'})}{\P(X=k)} \\
            &=\frac{\P(X=k)f_p(p')}{\P(X=k)} \\
            &=\frac{\binom nk p'^k(1-p')^{n-k}}{\P(X=k)}
        \end{align*}

        where $\P(X=k)$ is constant, independent of $p'$.
        Therefore $$f_{p|X}(p'|k)=cp'^k(1-p')^{n-k}$$

        where $c$ is such that $\int_0^1cp'^k(1-p')^{n-k}\dt[p']=1$.
    \end{example}

\chapter{Joint Distributions of Functions of Random Variables}
    \begin{proposition} \label{joint-distribution}
        Let $X_1$ and $X_2$ be continuous random variables with joint pdf $f_{X_1,X_2}$.

        We define new random variables $Y_1=g_1(X_1,X_2)$ and $Y_2=g_2(X_1,X_2)$ where 
        \begin{enumerate}
            \item $(X_1,X_2)\mapsto(Y_1,Y_2)$ is a bijection, and
            \item continuous partial derivatives of $g_1$, $g_2$ satisfies $$J(x_1,x_2)=\abs{\begin{pmatrix}
                \pdiff[g_1]{x_1} & \pdiff[g_1]{x_2} \\[5pt]
                \pdiff[g_2]{x_1} & \pdiff[g_2]{x_2}
            \end{pmatrix}}\neq0.$$
        \end{enumerate}

        Then $Y_1$, $Y_2$ are jointly continuous random variables with pdf
        $$f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\cdot\abs{J(x_1,x_2)}^{-1}$$

        where $y_1=g_1(x_1,x_2)$ and $y_2=g_2(x_1,x_2)$.
    \end{proposition}
    \begin{proof}
        (Sketch) We have \begin{align*}
            F_{Y_1,Y_2}(y_1,y_2)&=\P(Y_1\leq y_1,Y_2\leq y_2) \\
            &=\iint_{\substack{g_1(x_1,x_2)\leq y_1 \\ g_2(x_1,x_2)\leq y_2}}f_{X_1,X_2}(x_1,x_2)\dt[x_1]\dt[x_2] 
        \end{align*}

        Then differentiate with respect to $y_1$ and $y_2$.
    \end{proof}

    \begin{example}
        Given $X_1,X_2$ random variables, let $Y_1=X_1+X_2$ and $Y_2=X_1-X_2$.

        Determine $f_{Y_1,Y_2}$ in terms of $f_{X_1,X_2}$.\\
        \textbf{Solution.} Observe that \begin{enumerate}
            \item $X_1=\frac12(Y_1+Y_2)$ and $X_2=\frac12(Y_1-Y_2)$, so we do have a bijection, and
            \item $$J(x_1,x_2)=\abs{\begin{pmatrix}
                1 & 1 \\
                1 & -1
            \end{pmatrix}}=2\neq0.$$
        \end{enumerate}

        By Proposition \ref{joint-distribution}, \begin{align*}
            f_{Y_1,Y_2}(y_1,y_2)&=f_{X_1,X_2}(x_1,x_2)\cdot\abs{J(x_1,x_2)}^{-1} \\
            &=\frac12f_{X_1,X_2}\left(\frac{y_1+y_2}{2},\frac{y_1-y_2}{2}\right)
        \end{align*}
    \end{example}

    \begin{example}
        Continue from the previous example.\\
        Let $X_1,X_2\sim N(0,1)$ independently. Then $Y_1\sim N(0,2)$ and $Y_2\sim N(0,2)$.

        To calculate the joint density, $$f_{X_1,X_2}(x_1,x_2)=f_{X_1}(x_1)f_{X_2}(x_2)=\frac{1}{2\pi}e^{-\frac{x_1^2}{2}-\frac{x_2^2}{2}},$$

        and thus \begin{align*}
            f_{Y_1,Y_2}(y_1,y_2)&=\frac12f_{X_1,X_2}\left(\frac{y_1+y_2}{2},\frac{y_1-y_2}{2}\right) \\
            &=\frac{1}{4\pi}e^{-\frac{\left(\frac{y_1+y_2}{2}\right)^2}{2}-\frac{\left(\frac{y_1-y_2}{2}\right)^2}{2}} \\
            &=\frac{1}{4\pi}e^{-\frac{y_1^2+y_2^2}{4}} \\
            &=\left(\frac{1}{\sqrt{2\cdot2\pi}}e^{-\frac{y_1^2}{2\cdot2}}\right)\left(\frac{1}{\sqrt{2\cdot2\pi}}e^{-\frac{y_2^2}{2\cdot2}}\right)
        \end{align*}

        That means, in fact, $Y_1$ and $Y_2$ are independent.
        
        Actually, if $X_1,X_2$ are independent and $Y_1=X_1+X_2,Y_2=X_1-X_2$ are independent, then $X_1,X_2$ are normal.
    \end{example}

    More generally, if we have $X_1,X_2,\ldots,X_n$ and we define $Y_i=g_i(X_1,X_2,\ldots,X_n)$ for $1\leq i\leq n$
    such that $(x_1,x_2,\ldots,x_n)\to(y_1,y_2,\ldots,y_n)$ is a bijection and
    $$J(x_1,x_2,\ldots,x_n)=\abs{\begin{pmatrix}
        \pdiff[g_1]{x_1} & \pdiff[g_1]{x_2} & \cdots & \pdiff[g_1]{x_n} \\[15pt]
        \vdots & \vdots & \ddots & \vdots \\[15pt]
        \pdiff[g_n]{x_1} & \pdiff[g_n]{x_2} & \cdots & \pdiff[g_n]{x_n}
    \end{pmatrix}}\neq0,$$
    then $$f_{Y_1,Y_2,\ldots,Y_n}(y_1,y_2,\ldots,y_n)=f_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n)\cdot\abs{J(x_1,x_2,\ldots,x_n)}^{-1}$$
    where $(x_1,x_2,\ldots,x_n)\mapsto(y_1,y_2,\ldots,y_n)$.

    \section{Expectation}

    \begin{proposition} \label{expectation}
        Let $X$ be a non-negative continuous random variable. Then
        $$\E[X]=\int_0^\infty \P(X\geq x)\dx.$$
    \end{proposition}
    \begin{proof}
        \begin{align*}
            \int_0^\infty \P(X\geq t)\dt&=\int_0^\infty\int_t^\infty f(x)\dx\dt \\
            &=\int_0^\infty\int_0^x f(x)\dt\dx \\
            &=\int_0^\infty x f(x)\dx=\E[X]
        \end{align*}
    \end{proof}

    \begin{proposition}
        If $X$ and $Y$ are random variables\\ 
        (discrete) with joint probability mass function $p$, then
        for any $g:\R^2\to\R$, $$\E[g(x,y)]=\sum_{x,y}g(x,y)p(x,y).$$
        (continuous) with joint probability density function $f$, then
        for any $g:\R^2\to\R$, $$\E[g(x,y)]=\iint_{\R^2}g(x,y)f(x,y)\dx\dy.$$
    \end{proposition}
    \begin{proof}
        (continuous) If $g\geq0$, we have \begin{align*}
            \E[g(X,Y)]&=\int_0^\infty\P(g(X,Y)\geq t)\dt \tag{Proposition \ref{expectation}} \\
            &=\int_0^\infty\iint_{(x,y):g(x,y)\geq t}f(x,y)\dx\dy\dt \\
            &=\iint_{\R^2}\int_0^{g(x,y)} f(x,y)\dt\dx\dy \\
            &=\iint_{\R^2}g(x,y)f(x,y)\dx\dy
        \end{align*}
        
        General case in homework.
    \end{proof}

    \begin{example}
        On a road of length $L$, an accident occurs at position $X$,
        uniformly distributed along the road.

        An ambulance is at position $Y$, also independently uniformly distributed along the road.
        \begin{center}
            \begin{tikzpicture}
                \draw (0,0) -- (7,0);
    
                \draw[shift={(0,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {0};
                \draw[shift={(2,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$X$};
                \draw[shift={(5,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$Y$};
                \draw[shift={(7,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$L$};
            \end{tikzpicture}
        \end{center}
        \underline{Q.} What is the expected distance the ambulance has to travel to arrive at the accident?\\
        \textbf{Solution.} Let $X\sim\Unif(0,L)$ and $Y\sim\Unif(0,L)$ independently.
        The distance is $g(X,Y)=|X-Y|$. 
        Since $X$ and $Y$ are independent and $f_X=f_Y=\frac1L\textbf{1}_{[0,L]}$, 
        its expectation is $$\E[|X-Y|]=\int_0^L\int_0^L|x-y|f(x,y)\dx\dy=\frac{1}{L^2}\int_0^L\int_0^L|x-y|\dx\dy.$$
        
        Compute \begin{align*}
            \int_0^L|x-y|\dy&=\int_0^xx-y\dy+\int_x^Ly-x\dy \\
            &=\left.\left(xy-\frac12y^2\right)\right|_{y=0}^x+\left.\left(\frac12y^2-xy\right)\right|_{y=x}^L \\
            &=\frac12x^2+\left(\frac12L^2-xL-\left(-\frac12x^2\right)\right) \\
            &=\frac12L^2+x^2-xL
        \end{align*}
        \begin{align*}
            \int_0^L\frac12L^2+x^2-xL\dx&=\left(\frac12L^2x+\frac13x^3-\frac12x^2L\right)_{x=0}^L \\
            &=\frac12L^3+\frac13L^3-\frac12L^3=\frac13L^3
        \end{align*}

        Therefore $$\E[|X-Y|]=\frac{1}{L^2}\int_0^L\int_0^L|x-y|\dx\dy=\fbox{$\dfrac{L}{3}$}$$%\frac13L^3=\fbox{$\fracL3$}.$$
    \end{example}

    \begin{example} (linearity)
        Let $g(X,Y)=aX+bY$. Then \begin{align*}
            \E[g(X,Y)]&=\E[aX+bY] \\
            &=\int_{-\infty}^\infty\int_{-\infty}^\infty(ax+by)f(x,y)\dx\dy \\
            &=a\int_{-\infty}^\infty\int_{-\infty}^\infty xf(x,y)\dx\dy+b\int_{-\infty}^\infty\int_{-\infty}^\infty yf(x,y)\dx\dy \\
            &=a\int_{-\infty}^\infty x\int_{-\infty}^\infty f(x,y)\dy\dx+b\int_{-\infty}^\infty y\int_{-\infty}^\infty f(x,y)\dx\dy \\
            &=a\int_{-\infty}^\infty x f_X(x)\dx+b\int_{-\infty}^\infty y f_Y(y)\dy \tag{by definition} \\
            &=a\E[X]+b\E[Y]
        \end{align*}
    \end{example}

    \noindent \textbf{Application.} (Monotonicity) If $X\geq Y$, then $\E[X]\geq\E[Y]$.
    \begin{proof}
        If $X\geq Y$, then $X-Y\geq0$.
        Then $\E[X]-\E[Y]=\E[X-Y]\geq0$.
    \end{proof} 

    \noindent \textbf{Application.} (Union Bound, Boole's Inequality) Let $A_1,A_2,\ldots,A_n$ are events,
    then $$\P\left(\bigcup_{i=1}^n A_i\right)\leq\sum_{i=1}^n\P(A_i).$$
    \begin{proof}
        For each $i$, let $I_i$ be the indicators $$I_i=\begin{cases}
            1 & \text{ if }A_i\text{ occurs} \\
            0 & \text{ otherwise.}
        \end{cases}$$

        Then $\E[I_i]=\P(I_i=1)=\P(A_i)$. Let $X=\sum_{i=1}^n I_i$ be the number of events that occur.

        Let $Y=I_{\set{\bigcup_{i=1}^nA_i}}=\begin{cases}
            1 & \text{ if }X\geq1 \\
            0 & \text{ if }X=0
        \end{cases}$ be the indicator of the union of events. Then $Y\leq X$.

        By the above application, we have 
        $$\P\left(\bigcup_{i=1}^nA_i\right)=\E[Y]\leq\E[X]=\E\left[\sum_{i=1}^nI_i\right]
        =\sum_{i=1}^n\E\left[I_i\right]=\sum_{i=1}^n\P(A_i).$$
    \end{proof}

    \noindent \textbf{Application.} A coin is tossed $n$ times. Each toss is heads with probability $p$ 
    and tails with probability $1-p$ independently.

    How many different runs (= sequence of successive heads/tails) do we expect?
    $$\underbrace{HHHHHHHHHH}_{\text{1 run}}$$
    $$\underbrace{HTTHTHHHTT}_{\text{6 runs}}$$

    Let $X_i$ be the indicator of the event that a new run starts from position $i$.

    Then $X=$ number of runs $=\sum_{i=1}^nX_i$. Therefore
    $$\E[X]=\E\left[\sum_{i=1}^nX_i\right]=\sum_{i=1}^n\E[X_i]=\sum_{i=1}^n\P(\text{a new run starts from position }i)=:p_i.$$
    
    We have $p_1=1$ and \begin{align*}
        p_i&=\P(\text{outcome $i\neq$ outcome }i-1) \\
        &=\P(\text{outcome $i=T$, outcome }i-1=H)+\P(\text{outcome $i=H$, outcome }i-1=T) \\
        &=(1-p)p+p(1-p)=2p(1-p)
    \end{align*}

    Therefore $\E[X]=1+2p(1-p)(n-1).$\\

    \noindent \textbf{Application.} (Analysis of Quicksort)

    Sorting problem: Given a permutation of $1,2,\ldots,n$, we want an algorithm that outputs a sorted list 
    in comparison-based model.

    Quicksort algorithm: Pick a pivot $x$ uniformly at random from the list,
    and then compare everthing else to $x$.

    Then repeat the process on the left and right sublists.\\
    \underline{Q.} On average, how many comparisons does Quicksort need?\\
    \textbf{Solution.} For every $1\leq i\leq j\leq n$, let $X_{i,j}$ be the indicator of the event that $i$ and $j$ are compared.

    Then $X=\sum_{1\leq i<j\leq n}X_{i,j}$ is the total number of comparisons.
    Therefore $$\E[X]=\sum_{1\leq i<j\leq n}\E[X_{i,j}]=\sum_{1\leq i<j\leq n}\P(i,j\text{ are compared}).$$

    Consider the interval $i,i+1,\ldots,j$. \begin{itemize}
        \item If the pivot $x$ is outside this interval, then the interval stays together for the next round.
        \item If the pivot $x$ is inside the interval, \begin{itemize}
            \item if $x=i$ or $x=j$, then $i$ and $j$ are compared.
            \item if $x\neq i,j$, then $i$ and $j$ are not compared.
        \end{itemize}
    \end{itemize}

    Therefore \begin{align*}
        \P(i,j\text{ are compared})&=\P\left(\substack{\text{when pivot $x$ satisfies $i\leq x\leq j$,} \\ \text{we have $x\in\set{i,j}$}}\right) \\
        &=\P(x=i\text{ or }j|x\sim\Unif(\set{i,i+1,\ldots,n})) \\
        &=\frac{2}{j-i+1}.
    \end{align*}

    Hence \begin{align*}
        \E[X]&=\sum_{1\leq i<j\leq n}\E[X_{i,j}]=\sum_{i=1}^{n-1}\sum_{j=i+1}^n\frac{2}{j-i+1} \\
        &=\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}\frac{2}{j+1} \\
        &\approx2\sum_{i=1}^{n-1}\ln(n-i+1)=2\sum_{i=2}^n\ln i \\
        &\approx2\int_2^n\ln x\dx \\
        &\approx2n\ln n
    \end{align*}

\chapter{Moments of Numbers of Events}

    Setting:
    \begin{itemize}
        \item Let $A_1,A_2,\ldots,A_n$ be events in a probability space.
        \item Let $I_1,I_2,\ldots,I_n$ be the corresponding indicators random variables.
    \end{itemize}

    Then $X=\sum_{i=1}^nI_i$ is the number of events that occur, and $\E[X]=\sum_{i=1}^n\P(A_i)$.

    Now consider $Y=\sum_{1\leq i<j\leq n}I_iI_j$. We have $$I_iI_j=\begin{cases}
        1 & \text{ if }A_i\cap A_j\text{ occur} \\
        0 & \text{ otherwise.}
    \end{cases}$$

    Therefore $Y$ is the number of pairs of events that occur $=\binom X2$.

    By linearity of expectation, \begin{align*}
        \E\left[\binom X2\right]&=\E\left[\sum_{1\leq i<j\leq n}I_iI_j\right] \\
        &=\sum_{1\leq i<j\leq n}\E[I_iI_j] \\
        &=\sum_{1\leq i<j\leq n}\P(A_i\cap A_j)
    \end{align*}

    More generally, $\binom Xk=\sum_{i_1<i_2<\cdots<i_k}I_{i_1}I_{i_2}\cdots I_{i_k}$ and 
    $$\E\left[\binom Xk\right]=\sum_{i_1<i_2<\cdots<i_k}\P(A_{i_1}\cap A_{i_2}\cap\cdots\cap A_{i_k}).$$

    \begin{example}
        Let $X\sim\Bin(n,p)$. Let $A_i=\set{\text{the $i$-th trial is a success}}$.

        Then $X=\sum_{i=1}^nI_i$ where $I_i$ is the indicator random variable for $A_i$.
        \begin{align*}
            \E\left[\binom X2\right]&=\sum_{i<j}\P(A_i\cap A_j) \\
            &=\sum_{i<j}\P(A_i)\P(A_j) \tag{independence} \\
            &=\sum_{i<j}p^2=\binom n2p^2 \\
        \end{align*}

        This is equal to the result of the calculation
        \begin{align*}
            \E\left[\binom X2\right]&=\E\left[\frac{X(X-1)}2\right] \\
            &=\frac{1}{2}\E[X^2-X] \\
            &=\frac{1}{2}\left(\E[X^2]-\E[X]\right)
        \end{align*}

        Therefore $$n(n-1)p^2=\E[X^2]-\E[X]=\E[X^2]-np$$
        $$\E[X^2]=n(n-1)_np$$
        \begin{align*}
            \Var(X)&=\E[X^2]-\E[X]^2 \\
            &=n(n-1)p^2+np-n^2p^2 \\
            &=np-np^2=np(1-p)
        \end{align*}
    \end{example}

    \begin{example}
        Hat matching problem. 
        
        Let $X=$ number of people getting own hat and 
        $A_i=\set{i\text{-th person gets own hat}}$, $I_i=I_{A_i}$.

        Then $X=\sum_{i=1}^nI_i$ and $\P(A_i)=\frac{1}{n}$.

        By linearity, $\E[X]=\sum_{i=1}^n\P(A_i)=\sum_{i=1}^{n}\frac{1}{n}=1$.
        \begin{align*}
            \E\left[\binom X2\right]&=\sum_{i<j}\P(A_i\cap A_j) \\
            &=\sum_{i<j}\P(A_i)\P(A_j|A_i) \\
            &=\sum_{i<j}\frac{1}{n}\cdot\frac{1}{n-1}=\binom n2\frac{1}{n(n-1)}
        \end{align*}

        Therefore $\E[X(X-1)]=1$ and thus $\Var(X)=1$.

        More generally,
        \begin{align*}
            &\E[X(X-1)\cdots(X-k+1)] \\
            &=k!\E\left[\binom Xk\right] \\
            &=k!\sum_{i_1<i_2<\cdots<i_k}\P(A_{i_1}\cap A_{i_2}\cap\cdots\cap A_{i_k}) \\
            &=k!\sum_{i_1<i_2<\cdots<i_k}\P(A_{i_1})\P(A_{i_2}|A_{i_1})\cdots\P(A_{i_k}|A_{i_1}\cap\cdots\cap A_{i_{k-1}}) \\
            &=k!\binom nk\frac{1}{n(n-1)\cdots(n-k+1)}=1
        \end{align*}
    \end{example}

    \begin{example}
        Coupon collecter.

        Every time we get a uniformly random type of coupon from $1$ to $n$.

        Let $X=$ number of coupons we collect until we have a full set. Then
        $$\E[X]=n\left(1+\frac12+\cdots+\frac1n\right)\approx n\log n.$$

        This means on average, we expect to have $\log n$ copies of each coupon.

        Let $Y=$ number of types of coupon that we only have a SINGLE copy of when we finish our first set.

        We already know that $Y\geq1$. What is $\E[Y]$ and $\Var(Y)$?\\
        \textbf{Solution.} Let $A_i=\set{\text{the $i$-th type of coupon we get appears uniquely}}$.

        Let $I_i=I_{A_i}$ be the indicator random variable for $A_i$ and $Y=\sum_{i=1}^nI_i$.

        Note that \begin{align*}
            A_i&=\set{\substack{\text{in the coupons after getting coupon $i$ for the first time,} \\
            \text{we get coupons $i,i+1,\ldots,n$ before getting $i$}}} \\
            &=\set{\substack{\text{in the set of coupons $\set{i,i+1,\ldots,n}$,} \\
            \text{coupon $i$ comes last}}}
        \end{align*}
        \begin{center}
            \begin{tikzpicture}
                \draw (0,0) -- (8,0);
    
                \draw[shift={(0,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {0};
                \draw[shift={(4,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$\substack{\text{coupon $i$} \\ \text{for first time}}$};
                \draw[shift={(8,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$X$};

                \node[above] at (6,0) {no coupon $i$};
            \end{tikzpicture}
        \end{center}

        Therefore $\P(A_i)=\frac{1}{n-i+1}$ by symmetry. Then
        $$\E[Y]=\sum_{i=1}^n\P(A_i)=\sum_{i=1}^n\frac{1}{n-i+1}=\sum_{i=1}^n\frac{1}{i}\approx\log n.$$

        Then in order to compute $\Var(Y)$, we look at the second moment.
        \begin{center}
            \begin{tikzpicture}
                \draw (0,0) -- (9,0);
    
                \draw[shift={(0,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {0};
                \draw[shift={(3,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$\substack{\text{coupon $i$} \\ \text{for first time}}$};
                \draw[shift={(6,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$\substack{\text{coupon $j$} \\ \text{for first time}}$};
                \draw[shift={(9,0)},color=black] (0pt,3pt) -- (0pt,-3pt) node[below] {$X$};

                \node[above] at (4.5,0) {no coupon $i$};
                \node[above] at (7.5,0) {no coupon $i,j$};
            \end{tikzpicture}
        \end{center}

        Let $S_{i,j}=\set{\substack{\text{among the coupons }\set{i,i+1,\ldots,n}\text{ after seeing coupon $i$ for the first time}, \\ 
        \text{$i$ is not in the first $j-i$}}}$, and \\
        $T_{i,j}=\set{\substack{\text{among the coupons }\set{i,j,j+1,\ldots,n}\text{ after seeing coupon $j$ for the first time}, \\ 
        \text{$i,j$ are the last two to appear}}}$.

        By uniformity, coupon $i$ is equally likely to be in any position.
        Therefore $\P(S_{i,j})=1-\frac{j-i}{n-i+1}$. 
        Then $$\P(S_{i,j}\cap T_{i,j})=\P(S_{i,j})\P(T_{i,j}|S_{i,j})=\P(S_{i,j})\P(T_{i,j})$$

        since the ``tail'' coupons are independent of previous ones.
        
        By uniformity, we have $\P(T_{i,j})=\frac{2}{(n-j+2)(n-j+1)}$.
        Therefore \begin{align*}
            \E\left[\binom Y2\right]&=\sum_{i<j}\P(A_i\cap A_j) \\
            &=\sum_{i<j}\P(S_{i,j})\P(T_{i,j}) \\
            &=\sum_{i<j}\left(1-\frac{j-i}{n-i+1}\right)\cdot\frac{2}{(n-j+2)(n-j+1)} \\
            &=\sum_{i<j}\frac{2}{(n-i+1)(n-j+2)}
        \end{align*}

        and \begin{align*}
            \Var(X+Y)&=\E[(X+Y)^2]-\E[X+Y]^2 \\
            &=\E[X^2+Y^2+2XY]-(\E[X]+\E[Y])^2 \\
            &=\E[X^2]+\E[Y^2]+2\E[XY]-\E[X]^2-\E[Y]^2-2\E[X]\E[Y]
        \end{align*}
    \end{example}

\chapter{Covariance, Variance of Sums, and Correlation} \label{part::} %Chapter title and label for references - these could follow the textbook chapter/section schemes

\section{Covariance} \label{sec::covariance} %Section title and label for references

    \begin{proposition}
        If $X$ and $Y$ are independent, then for any function $g$, $h$, $$\E[g(X)h(Y)]=\E[g(X)]\E[h(Y)].$$
    \end{proposition}
    \begin{proof}
        (Continuous) \begin{align*}
            \E[g(X)h(Y)]&=\iint_{\R^2}g(X)h(Y)f(x,y)\dx\dy \\
            &=\iint_{\R^2}g(X)h(Y)f_X(x)f_Y(y)\dx\dy \\
            &=\int_{\R}g(X)f_X(x)\dx\int_{\R}h(Y)f_Y(y)\dy \\
            &=\E[g(X)]\E[h(Y)]
        \end{align*}
    \end{proof}

    \begin{definition}
        Let $X$ and $Y$ be random variables.
        We define the \emph{covariance} of $X$ and $Y$ to be $$\Cov(X,Y)=\E[(X-\E[X])(Y-\E[Y])].$$
    \end{definition}
    Observe \begin{align*}
        &\E[(X-\E[X])(Y-\E[Y])] \\
        &=\E[XY-X\E[Y]-Y\E[X]+\E[X]\E[Y]] \\
        &=\E[XY]-\E[X\E[Y]]-\E[Y\E[X]]+\E[\E[X]\E[Y]] \\
        &=\E[XY]-\E[X]\E[Y]-\E[Y]\E[X]+\E[X]\E[Y] \\
        &=\E[XY]-\E[X]\E[Y]
    \end{align*}

    This implies that if $X$, $Y$ are independent, then $\Cov(X,Y)=0$.

    WARNING: The converse is NOT true. Counterexample: Let $$X=\begin{cases}
        -1 & \text{ with probability }\frac{1}{3} \\[10pt]
        0 & \text{ with probability }\frac{1}{3}, \\[10pt]
        1 & \text{ with probability }\frac{1}{3}
    \end{cases}\quad Y=\begin{cases}
        0 & \text{ if }X\neq0 \\
        1 & \text{ if }X=0.
    \end{cases}$$

    Then $\E[XY]-\E[X]\E[Y]=0$ but $p_{X,Y}(1,1)=0\neq\frac13\cdot\frac13=p_X(1)p_Y(1)$.

\begin{proposition} 
    The covariance has the following properties: \begin{enumerate}[label=(\roman*)]
        \item $\Cov(X,Y)=\Cov(Y,X)$
        \item $\Var(X)=\Cov(X,X)$
        \item $\Cov(aX,Y)=a\Cov(X,Y)$
        \item $\Cov\left(\sum_{i=1}^nX_i,\sum_{j=1}^{m}Y_j\right)=\sum_{i=1}^n\sum_{j=1}^{m}\Cov(X_i,Y_j)$
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item[(i)-(iii)] Followed by definition.
        \item[(iv)] Let $\mu_i=\E[X_i]$, $\nu_j=\E[Y_j]$. Then by linearity of expectation,
        \begin{align*}
            \E\left[\sum_{i=1}^nX_i\right]&=\sum_{i=1}^n\E[X_i]=\sum_{i=1}^n\mu_i \\
            \E\left[\sum_{j=1}^mY_j\right]&=\sum_{j=1}^m\nu_j
        \end{align*}
        \begin{align*}
            \Rightarrow \Cov\left(\sum_{i=1}^nX_i,\sum_{j=1}^mY_j\right)&=\E\left[\left(\sum_{i=1}^nX_i-\E\left[\sum_{i=1}^nX_i\right]\right)\left(\sum_{j=1}^mY_j-\E\left[\sum_{j=1}^mY_j\right]\right)\right] \\
            &=\E\left[\left(\sum_{i=1}^nX_i-\sum_{i=1}^n\mu_i\right)\left(\sum_{j=1}^mY_j-\sum_{j=1}^m\nu_j\right)\right] \\
            &=\E\left[\left(\sum_{i=1}^n(X_i-\mu_i)\right)\left(\sum_{j=1}^m(Y_j-\nu_j)\right)\right] \\
            &=\E\left[\sum_{i=1}^n\sum_{j=1}^m(X_i-\mu_i)(Y_j-\nu_j)\right] \\
            &=\sum_{i=1}^n\sum_{j=1}^m\E\left[(X_i-\mu_i)(Y_j-\nu_j)\right] \tag{LoE} \\
            &=\sum_{i=1}^n\sum_{j=1}^m\Cov(X_i,Y_j)
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{corollary}
    Let $X=\sum_{i=1}^nX_i$ be a sum of random variables. Then
    $$\Var(X)=\Cov(X,X)\overtext{(b)}{=}\Cov\left(\sum_{i=1}^nX_i,\sum_{j=1}^nX_j\right)\overtext{(d)}{=}\sum_{i=1}^n\sum_{j=1}^n\Cov(X_i,X_j).$$

    Therefore, by (a) and (b), $$\Var\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^n\Var(X_i)+2\sum_{i<j}\Cov(X_i,X_j).$$
\end{corollary}
\begin{corollary}
    If $X_1,X_2,\ldots,X_n$ are pairwise independent, then $$\Var\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^n\Var(X_i).$$ 
\end{corollary}
We show that the independence is necessary: Let $X_2=-X_1$, where $X_1$ is a non-constant random variable.

Then $\Var(X_2)=(-1)^2\Var(X_1)=\Var(X_1)\neq0$, but $$0=\Var(X_1+X_2)\neq\Var(X_1)+\Var(X_2).$$

\begin{example}
    Let $X\sim\Bin(n,p)$. Then $X=\sum_{i=1}^nX_i$, where $X_i\sim\Ber(p)$ are i.i.d Bernoulli random variables.

    Since $X_i$ are independent, $$\Var(X)=\sum_{i=1}^n\Var(X_i)=n\Var(X_1).$$
    \begin{align*}
        \E[X_1]&=1\P(X_1=1)+0\P(X_1=0)=p \\
        \E[X_1^2]&=\E[X_1]=p \\
        \Rightarrow\Var(X_1)&=\E[X_1^2]-\E[X_1]^2=p(1-p) \\
        \Rightarrow\Var(X)&=np(1-p)
    \end{align*}
\end{example}

\begin{example}
    Let $X_1,X_2,\ldots,X_n$ be i.i.d random variables with mean $\mu$ and variance $\sigma^2$.

    Define the \emph{sample mean} $\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$.
    Then $$\E[\overline{X}]=\E\left[\sum_{i=1}^nX_i\right]\overtext{LoE}{=}\frac{1}{n}\sum_{i=1}^n\E[X_i]=\frac{1}{n}\sum_{i=1}^n\mu=\fbox{$\mu$}.$$
    $$\Var(\overline{X})=\Var\left(\frac{1}{n}\sum_{i=1}^nX_i\right)=\frac{1}{n^2}\Var\left(\sum_{i=1}^nX_i\right)\overtext{indep.}{=}\frac{1}{n^2}\sum_{i=1}^n\Var(X_i)=\frac{1}{n^2}\sum_{i=1}^n\sigma^2=\fbox{$\dfrac{\sigma^2}{n}$}.$$

    Compared to using just $X_1$ to extimate
    $$\E[X_1]=\mu,\quad\text{ but }\Var(X_1)=\sigma^2.$$

    Therefore using the average $\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$ reduces the variance,
    hence a more accurate estimate.
\end{example}

\begin{definition}
    We define the \emph{deviation} of $X_i$ as $X_i-\overline{X}.$
    Note that $\sum_i(X_i-\overline{X})\equiv0$.
\end{definition}

Let $S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$ (square to remove cancellation).
What is $\E[S^2]$?
$$\E[(n-1)S^2]=\E\left[\sum_{i=1}^n(X_i-\overline{X})^2\right]$$

where \begin{align*}
    \sum_{i=1}^n(X_i-\overline{X})^2&=\sum_{i=1}^n(X_i-\mu+\mu-\overline{X})^2 \\
    &=\sum_{i=1}^n\left((X_i-\mu)^2+(\overline{X}-\mu)^2-2(X_i-\mu)(\overline{X}-\mu)\right) \\
    &=\sum_{i=1}^n(X_i-\mu)^2+n(\overline{X}-\mu)^2-2(\overline{X}-\mu)\sum_{i=1}^n(X_i-\mu) \\
    &=\sum_{i=1}^n(X_i-\mu)^2-n(\overline{X}-\mu)^2 \\
\end{align*}

Therefore \begin{align*}
    \E[(n-1)S^2]&=\E\left[\sum_{i=1}^n(X_i-\mu)^2-n(\overline{X}-\mu)^2\right] \\
    &=\sum_{i=1}^n\E[(X_i-\mu)^2]-n\E[(\overline{X}-\mu)^2] \tag{LoE} \\
    &=\sum_{i=1}^n\Var(X_i)-n\Var(\overline{X}) \\
    &=n\sigma^2-n\frac{\sigma^2}{n}=(n-1)\sigma^2
\end{align*}
$$\Rightarrow\E[S^2]=\sigma^2$$

In summary, given samples $X_1,X_2,\ldots,X_n$,
$\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$ is an unbiased estimator of the mean $\mu$, 
and $S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$ is an unbiased estimator of the variance $\sigma^2$.

\begin{definition}
    Let $X,Y$ be random variable with $\Var(X),\Var(Y)>0$.
    Then we define the \emph{correlation} of $X$ and $Y$ to be
    $$\rho(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.$$
\end{definition}

\begin{claim}
    For any $X,Y$, $-1\leq\rho(X,Y)\leq1$.
\end{claim}
\begin{proof}
    Let $\sigma_X=\sqrt{\Var(X)}$, $\sigma_Y=\sqrt{\Var(Y)}$.
    Consider $\Var\left(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}\right)$:
    \begin{align*}
        0\leq\Var\left(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}\right)&=\Var\left(\frac{X}{\sigma_X}\right)+\Var\left(\frac{Y}{\sigma_Y}\right)+2\Cov\left(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y}\right) \\
        &=\frac{\Var(X)}{\sigma_X^2}+\frac{\Var(Y)}{\sigma_Y^2}+2\frac{\Cov(X,Y)}{\sigma_X\sigma_Y} \\
        &=2(1+\rho(X,Y))
    \end{align*}

    Therefore $\rho(X,Y)\geq-1$.

    Similarly, \begin{align*}
        0\leq\Var\left(\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y}\right)&=\Var\left(\frac{X}{\sigma_X}\right)+\Var\left(\frac{Y}{\sigma_Y}\right)-2\Cov\left(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y}\right) \\
       &=2(1-\rho(X,Y))
    \end{align*}

    Therefore $\rho(X,Y)\leq1$.
\end{proof}

\begin{itemize}
    \item Note that \begin{align*}
        \rho(X,Y)=1&\Rightarrow\Var\left(\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y}\right)=0 \\
        &\Rightarrow\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y}\text{ is constant} \\
        &\Rightarrow Y=aX+b,a>0
    \end{align*}
    \item $\rho(X,Y)=0\Leftarrow X,Y$ are independent. (The reverse is not true.) \\
    If $\rho(X,Y)=0$, we say $X,Y$ are \emph{unrelated}.
    \item $\rho(X,Y)=-1\Rightarrow Y=aX+b,a<0$ \\
\end{itemize}

\begin{example}
    Let $A,B$ be events in a probability space.

    Let $I_A$, $I_B$ be their indicator random variables.
    Then $$\Cov(I_A,I_B)=\E[I_AI_B]-\E[I_A]\E[I_B]=\E[I_{A\cap B}]-\E[I_A]\E[I_B]=\P(A\cap B)-\P(A)\P(B).$$

    We can understand this covariance as \begin{align*}
        \Cov(I_A,I_B)&=\P(A\cap B)-\P(A)\P(B) \\
        &=\P(B)\left(\frac{\P(A\cap B)}{\P(B)}-\P(A)\right) \\
        &=\P(B)\left(\P(A|B)-\P(A)\right)
    \end{align*}
\end{example}

\section{Conditional Expectation} \label{sec::cond_exp}

\noindent (Discrete case) Given $X,Y$, we can look at the distribution of $X$ given the value of $Y$ 
$$p_{X|Y}(x|y)=\frac{p(x,y)}{p_Y(y)}.$$

This defines a (conditional) distribution.

Therefore we can discuss what its expectation is.
$$\E[X|Y=y]=\sum_{x}xp_{X|Y}(x|y).$$

\begin{example}
    Let $X,Y\sim\Bin(n,p)$ be independent. What is $\E[X|X+Y=m]$?\\
    \textbf{Solution.} Compute
    \begin{align*}
        p_{X|X+Y}(k|m)&=\frac{\P(X=k,X+Y=m)}{\P(X+Y=m)} \\
        &=\frac{\P(X=k,Y=m-k)}{\P(X+Y=m)} \\
        &=\frac{\P(X=k)\P(Y=m-k)}{\P(X+Y=m)}
    \end{align*}

    where \begin{align*}
        \P(X=k)&=\binom{n}{k}p^k(1-p)^{n-k} \\
        \P(Y=m-k)&=\binom{n}{m-k}p^{m-k}(1-p)^{n-(m-k)} \\
        \P(X+Y=m)&=\binom{2n}{m}p^m(1-p)^{2n-m}
    \end{align*}

    since $X+Y\sim\Bin(2n,p)$. Continue the computation: \begin{align*}
        p_{X|X+Y}(k|m)&=\frac{\P(X=k)\P(Y=m-k)}{\P(X+Y=m)} \\
        &=\frac{\binom{n}{k}p^k(1-p)^{n-k}\binom{n}{m-k}p^{m-k}(1-p)^{n-(m-k)}}{\binom{2n}{m}p^m(1-p)^{2n-m}} \\
        &=\frac{\binom{n}{k}\binom{n}{m-k}}{\binom{2n}{m}},\quad0\leq k\leq\min\left\{n,m\right\}
    \end{align*}

    Therefore $X|X+Y=m$ has the hypergeometric distribution with parameters $2n,n,m$.
    Using what we know about the distribution, $\E[X|X+Y=m]=\frac{m}{2}$.
\end{example}

\noindent (Continuous case) Given the value of $Y$, we can define the conditional distribution of $X$
through the conditional density function $$f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}.$$

The conditional expectation is then $$\E[X|Y=y]=\int_{-\infty}^{\infty}xf_{X|Y}(x|y)\dx.$$

\begin{example}
    Let $$f_{X,Y}(x,y)=\begin{cases}
        \frac{e^{-\frac{x}{y}}e^{-y}}{y} & \text{ if }x,y>0 \\
        0 & \text{ otherwise }.
    \end{cases}$$

    What is $\E[X|Y=y]$?\\
    \textbf{Solution.} First calculate the conditional density:
    $$f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}$$
    \begin{align*}
        f_Y(y)&=\int_{-\infty}^\infty f_{X,Y}(x,y)\dx \\
        &=\int_0^\infty\frac{e^{-\frac{x}{y}}e^{-y}}{y}\dx \\
        &=\frac{e^{-y}}{y}\int_0^\infty e^{-\frac{x}{y}}\dx \\
        &=\frac{e^{-y}}{y}\cdot-ye^{-\frac{x}{y}}\big|_{x=0}^\infty=e^{-y}
    \end{align*}

    Therefore $$f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}=\frac{\frac{1}{y}e^{-\frac{x}{y}}e^{-y}}{e^{-y}}=\frac{1}{y}e^{-\frac{x}{y}}\quad\text{ when }x,y>0.$$

    We find that $X|Y=y\sim\Exp(\frac{1}{y})$, and thus $\E[X|Y=y]=\fbox{$y$}$.
\end{example}

Now we have $$\E[X|Y=y]=\begin{cases} 
    \displaystyle \sum_xxp_{X|Y}(x|y) & \text{ if }X\text{ is discrete} \\
    \displaystyle \int_{-\infty}^\infty xf_{X|Y}(x|y)\dx & \text{ if }X\text{ is continuous}.
\end{cases}$$

Note that $\E[X|Y=y]$ is a function of $y$ and $\E[X|Y]$ is a random variable.

\begin{proposition}
    For any two random variables $X,Y$, $\E[\E[X|Y]]=\E[X]$.
\end{proposition}
\begin{proof}
    (both $X$ and $Y$ are discrete)
    Observe \begin{align*}
        \E[X|Y=y]&=\sum_xxp_{X|Y}(x|y)=\sum_xx\cdot\frac{p(x,y)}{p_Y(y)} \\
        \E[g(Y)]&=\sum_yg(y)p_Y(y) \quad\text{ (apply $g(y)=\E[X|Y=y]$) } \\
        \Rightarrow\E[\E[X|Y]]&=\sum_y\E[X|Y=y]p_Y(y) \\
        &=\sum_y\left(\sum_xx\cdot\frac{p_{X,Y}(x,y)}{p_Y(y)}\right)p_Y(y) \\
        &=\sum_y\sum_xxp(x,y) \\
        &=\sum_x\sum_yp(x,y) \\
        &=\sum_xxp(x)=\E[X]
    \end{align*}
\end{proof}

\begin{example}
    A stop has a random number of customers each day, whose distribution is $\Poi(50)$.
    Each customer spends a random amount of money, uniform on \$$[0,1000]$,
    independent of all other customers. What is the expected daily income?

    \noindent \textbf{Solution.} Let $N=$ number of customers $\sim\Poi(50)$.

    Let $X_i=$ amount of money spent by customer $i$, so $X_i\sim\Unif[0,1000]$.

    Let $X$ be the total income $X=\sum_{i=1}^NX_i$. Then 
    \begin{align*}
        \E[X]&=\E[\E[X|N]] \\
        \E[X|N=n]&=\E\left[\sum_{i=1}^nX_i\right] \\
        &=\sum_{i=1}^n\E[X_i]=500n \tag{LoE}
    \end{align*}
    \begin{align*}
        \Rightarrow\E[X]&=\E[500N]=500\E[N] \\
        &=500\cdot50=\fbox{25000}
    \end{align*}
\end{example}

\begin{example}
    $n$ students take an exam. Their grades are independent and random.
    Each student gets \begin{align*}
        \text{A} &\text{ with probability }p_1, \\
        \text{B} &\text{ with probability }p_2, \\
        \text{C} &\text{ with probability }p_3, \\
        \text{\color{red}Fail} &\text{ with probability }p_4.
    \end{align*}

    Let $N_i$ be the number of students who get the $i$-th grade respectively.
    
    \noindent \underline{Q.} What is $\E[N_i|N_j>0]$?

    \noindent \textbf{Solution.} Define a new random variable $$I=\begin{cases}
        1 & \text{ if }N_j>0 \\
        0 & \text{ otherwise.}
    \end{cases}$$

    Therefore $\E[N_i|N_j>0]=\E[N_i|I=1]$.

    By our proposition, \begin{align*}
        \E[N_i]&=\E[\E[N_i|I]] \\
        &=\E[N_i|I=1]\P(I=1)+\E[N_i|I=0]\P(I=0)
    \end{align*}

    Therefore $$\E[N_i|I=1]=\frac{\E[N_i]-\E[N_i|I=0]\P(I=0)}{\P(I=1)}.$$

    Note that $N_i\sim\Bin(n,p_i)$. Therefore $\E[N_i]=np_i$.

    Then note that $N_i|I=0\sim\Bin\left(n,\frac{p_i}{1-p_j}\right)$.
    Therefore $\E[N_i|I=0]=\frac{np_i}{1-p_j}$.

    Also, $\P(I=0)=(1-p_j)^n$ and thus $\P(I=1)=1-(1-p_j)^n$. 
    
    Therefore we can compute $\E[N_i|I=1]$ by plugging in the values:
    $$\E[N_i|I=1]=\frac{np_i-\frac{np_i}{1-p_j}\cdot(1-p_j)^n}{1-(1-p_j)^n}.$$
\end{example}

\begin{example}
    (Variance of a geometric random variable) There is a sequence of independent trials, 
    each is successful with probability $p$.

    Let $N$ be the number of trials until the first success. Then $N\sim\Geom(p)$.

    Let $Y$ be the indicator random variable for the event that the first trial is successful.

    Our goal is to compute $\E[N^2]$.
    \begin{align*}
        \E[N^2]&=\E[\E[N^2|Y]] \\
        &=\E[N^2|Y=0]\P(Y=0)+\E[N^2|Y=1]\P(Y=1) \\
        &=\E[N^2|Y=1]p+\E[N^2|Y=0](1-p)
    \end{align*}
    $$\E[N^2|Y=1]=1\quad\text{ since }Y=1\Rightarrow N=1$$
    $$\E[N^2|Y=0]=\E[(N+1)^2]\quad\substack{\text{ since the number of addititonal trials } \\ \text{ has the same geometric distribution }}$$

    Therefore $$\E[N^2]=\underbrace{\E[(N+1)^2]}_{\E[N^2]+2\E[N]+1}(1-p)+1\cdot p$$
    $$p\E[N^2]=\frac{2(1-p)}{p}+1$$
    $$\E[N^2]=\frac{2(1-p)}{p^2}+\frac{1}{p}$$
    $$\Var(N)=\E[N^2]-\E[N]^2=\frac{2(1-p)}{p^2}+\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}$$
\end{example}

\begin{example}
    There are $r$ tech companies. The $i$-th company has $n_i$ billion dollars, $n_i\in\N$.

    Two companies will go to court for copyright infringement.
    There is a probability of $\frac{1}{2}$ for each company to win, and the loser pays the winner \$1 billion.

    Repeat until all but one company is bankrupt, and the remaining company has $n$ billion dollars, 
    where $n=\sum_{i=1}^rn_i$.

    \noindent \underline{Q.} What is the expected value of court cases?

    \noindent \textbf{Solution.} We start with the case $r=2$.
    
    There are $n$ billion dollars in total. Let $X_i=$ the number of rounds 
    if the first company starts with $i$ billion, $1\leq i\leq n-1$.

    Let $Y$ be the outcome of the first case $$Y=\begin{cases}
        1 & \text{ if the first company wins} \\
        2 & \text{ if the second company wins}.
    \end{cases}$$

    Let $A_i$ be the number of additional rounds after the first if the first company starts with $i$ billion.
    Therefore $X_i=1+A_i$.

    Define $m_i=\E[X_i]$. Then $m_i=\E[X_i]=\E[1+A_i]=1+\E[A_i]$.
    \begin{align*}
        \E[A_i|Y=1]&\sim X_{i+1} \\
        \E[A_i|Y=2]&\sim X_{i-1}
    \end{align*}

    Therefore $\E[A_i]=\E[\E[A_i|Y]]=\frac12\E[X_{i+1}]+\frac12\E[X_{i-1}]$.
    That is, $m_i=1+\frac12m_{i+1}+\frac12m_{i-1}.$
    Solve the recurrence relation and we can get that $m=i(n-i)$.

    General case. Let $X^{(i)}$ be the number of court cases involving the $i$-th company.
    Then $X=\frac12\sum_{i=1}^rX^{(i)}$, and thus $\E[X]=\frac12\sum_{i=1}^r\E[X^{(i)}]$.
    
    For $X^{(i)}$, its distribution is the same as the two-company process with initial amounts $n_i$ and $n-n_i$. (Why?)

    Therefore $\E[X^{(i)}]=n_i(n-n_i)$, and $$\E[X]=\frac12\sum_{i=1}^rn_i(n-n_i)=\fbox{$\displaystyle\frac{1}{2}\left(n^2-\sum_{i=1}^rn^2\right)$}.$$
\end{example}

\begin{example}
    A professor examines $n$ students. She wants to give the best student an A+.
    But she has to give the grades immediately.

    Suppose the students come in a random order. How can the professor maximize the probability of 
    giving the best student A+?

    \noindent \textbf{Solution.} We attempt to find some strategies:

    Strategy 1: Give the first student A+. This strategy has a probability of $\frac{1}{n}$ to success.

    Strategy $k$: Look at the first $k$ students and don't give them an A+.
    Then when she sees a student who is better than everyone before, give them an A+.

    Let $E=\set{\text{give the best student the A+}}$ and $X=$ position of the best student.
    Then $\P(E)=\sum_{i=1}^n\P(E|X=i)\P(X=i)$.

    If $i\leq k$, then $\P(E|X=i)=0$.

    If $i>k$, for $E|X=i$ to happen, the best of the first $i-1$ students has to be in the first $k$.
    Thus $\P(E|X=i)=\frac{k}{i-1}$. Therefore \begin{align*}
        \P(E)&=\sum_{i=1}^n\P(E|X=i)\P(X=i) \\
        &=\sum_{i=k+1}^n\P(E|X=i)\P(X=i) \\
        &=\sum_{i=k+1}^n\frac{k}{i-1}\cdot\frac{1}{n} \\
        &=\frac{k}{n}\sum_{i=k+1}^n\frac{1}{i-1} \\
        &=\frac{k}{n}\left(\sum_{i=1}^{n-1}\frac{1}{i}-\sum_{i=1}^{k}\frac{1}{i}\right) \\
        &\approx\frac{k}{n}\ln\frac{n}{k}
    \end{align*}

    Optimizing for the best choice of $k$, we get $k\approx\frac{n}{e}$
    and $\P(E)=\frac{1}{e}$.
\end{example}

\section{Conditional Variance} \label{sec::cond_var}

\begin{definition}
    The \emph{conditional variance} of $X$ given $Y$ is $$\Var(X|y)=\E[(X-\E[X|Y=y])^2|Y=y]
    =\E[X^2|Y=y]-\E[X|Y=y]^2.$$
\end{definition}

We have the amazing result:

\begin{proposition}
    $$\Var(X)=\E[\Var(X|Y)]+\Var(\E[X|Y]).$$
\end{proposition}
\begin{proof}
    Homework :)
\end{proof}

\chapter{Moment Generating Functions} \label{part::mgf} 

\begin{definition}
    Given a random variable $X$, its \emph{moment generating function} is defined by
    $$M_X(t)=\E[e^{tX}].$$
\end{definition}

\begin{proposition}
    For any $n\in\N$, $\E[X^n]=M_X^{(n)}(0)\left(=\diff[^n]{t^n}M(0)\right)$.
\end{proposition}
\begin{proof}
    \begin{align*}
        M_X&=\E[e^{tX}] \\
        M^{(n)}(t)&=\diff[^n]{t^n}\E[e^{tX}]\overtext{(*)}{=}\E\left[\diff[^n]{t^n}e^{tX}\right]=\E[X^ne^{tX}] \\
        \Rightarrow M^{(n)}(0)&=\E[X^ne^{0X}]=\E[X^n]
    \end{align*}
\end{proof}

\begin{remark}
    In (*) we exchanged the order of differentiation and expectation.
    This is generally valid for all distributions we care about.
\end{remark}

\begin{fact}
    The distribution of $X$ is determined by its moment generating function.
\end{fact}

\begin{example}
    Let $X\sim\Bin(n,p)$.
    Then \begin{align*}
        M_X(t)&=\E[e^{tX}] \\
        &=\sum_{k=0}^n e^{tk}\binom{n}{k}p^k(1-p)^{n-k} \\
        &=\sum_{k=0}^n\binom{n}{k}(e^tp)^k(1-p)^{n-k} \\
        &=(e^tp+(1-p))^n \tag{binomial thm}
    \end{align*}
    $$\E[X]=M'(0)=n(e^tp+(1-p))^{n-1}pe^t\big|_{t=0}=np$$
    \begin{align*}
        \E[X^2]&=M''(0) \\
        &=\left.\left(n(n-1)(e^tp+(1-p))^{n-2}(pe^t)^2+n(e^tp+(1-p))^{n-1}pe^t\right)\right|_{t=0} \\
        &=n(n-1)p^2+np
    \end{align*}
    $$\Rightarrow\Var(X)=\E[X^2]-\E[X]^2=n(n-1)p(1-p)$$
\end{example}

\begin{example}
    Let $X\sim\Poi(\lambda)$.
    Then \begin{align*}
        M_X(t)&=\E[e^{tX}] \\
        &=\sum_{n=0}^\infty e^{tn}\frac{e^{-\lambda}\lambda^n}{n!} \\
        &=e^{-\lambda}\sum_{n=0}^\infty\frac{(\lambda e^t)^n}{n!} \\
        &=e^{-\lambda+\lambda e^t}=e^{\lambda(e^t-1)}
    \end{align*}
    $$\E[X]=M'(0)=\lambda e^te^{\lambda(e^t-1)}\big|_{t=0}=\lambda$$
    \begin{align*}
        \E[X^2]&=M''(0) \\
        &=\left.\left(\lambda e^t\cdot e^{\lambda(e^t-1)}+(\lambda e^{t})^2e^{\lambda(e^t-1)}\right)\right|_{t=0} \\
        &=\lambda+\lambda^2
    \end{align*}
    $$\Rightarrow\Var(X)=\E[X^2]-\E[X]^2=\lambda$$
\end{example}

\begin{example}
    Let $X\sim\Exp(\lambda)$.
    Then \begin{align*}
        M_X(t)&=\E[e^{tX}] \\
        &=\int_0^\infty e^{tx}\lambda e^{-\lambda x}\dx \\
        &=\lambda\int_0^\infty e^{(t-\lambda)x}\dx \\
        &=\lambda\cdot\left.\frac{e^{(t-\lambda)x}}{t-\lambda}\right|_{t=0}^\infty \\
        &=\frac{\lambda}{\lambda-t} \tag{if $t<\lambda$}
    \end{align*}
    $$\E[X]=M'(0)=\left.\frac{\lambda}{(\lambda-t)^2}\right|_{t=0}=\frac{1}{\lambda}$$
    \begin{align*}
        \E[X^2]&=M''(0) \\
        &=\left.\frac{2\lambda}{(\lambda-t)^3}\right|_{t=0}=\frac{2}{\lambda^2}
    \end{align*}
    $$\Rightarrow\Var(X)=\E[X^2]-\E[X]^2=\frac{1}{\lambda^2}$$
\end{example}

\begin{example}
    Let $X\sim N(\mu,\sigma^2)$.

    Case: $\mu=0$, $\sigma^2=1$. $Z\sim N(0,1)$.
    Then \begin{align*}
        M_Z(t)&=\E[e^{tZ}] \\
        &=\int_{-\infty}^\infty e^{tz}\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}\dz \\
        &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac{1}{2}(z^2-2tz)}\dz \\
        &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac{1}{2}(z-t)^2}e^{\frac{t^2}{2}}\dz \\
        &=e^{\frac{t^2}{2}}\int_{-\infty}^\infty \underbrace{\frac{1}{\sqrt{2\pi}}e^{-\frac{(z-t)^2}{2}}}_{\text{pdf of }N(t,1)}\dz=e^{\frac{t^2}{2}}
    \end{align*}

    Case: $X\sim(\mu,\sigma^2)$.

    We already know that if $Z\sim N(0,1)$, then $\E[e^{tZ}]=e^{\frac{t^2}{2}}$.
    Then \begin{align*}
        M_X(t)&=\E[e^{tX}] \\
        &=\E[e^{t(\sigma Z+\mu)}] \\
        &=e^{t\mu}\E[e^{(t\sigma)Z}] \\
        &=e^{t\mu}M_Z(t\sigma) \\
        &=e^{t\mu}e^{\frac{(t\sigma)^2}{2}}=e^{t\mu+\frac{t^2\sigma^2}{2}}
    \end{align*}
    $$\E[X]=M'(0)=\left.\left(\mu+t\sigma^2\right)e^{t\mu+\frac{t^2\sigma^2}{2}}\right|_{t=0}=\mu$$
    \begin{align*}
        \E[X^2]&=M''(0) \\
        &=\left.\left(\sigma^2e^{t\mu+\frac{t^2\sigma^2}{2}}+(\mu+t\sigma^2)^2e^{t\mu+\frac{t^2\sigma^2}{2}}\right)\right|_{t=0} \\
        &=\mu^2+\sigma^2
    \end{align*}
    $$\Rightarrow\Var(X)=\E[X^2]-\E[X]^2=\sigma^2$$
\end{example}

\begin{claim}
    If $X$ and $Y$ are independent, then $M_{X+Y}(t)=M_X(t)M_Y(t)$.
\end{claim}
\begin{proof}
    \begin{align*}
        M_{X+Y}(t)&=\E[e^{t(X+Y)}] \\
        &=\E[e^{tX}e^{tY}] \\
        &=\E[e^{tX}]\E[e^{tY}] \tag{independence} \\
        &=M_X(t)M_Y(t)
    \end{align*}
\end{proof}

\begin{example}
    Let $X\sim\Poi(\lambda_1)$, $Y\sim\Poi(\lambda_2)$ independently. Then
    \begin{align*}
        M_{X+Y}(t)&=M_X(t)M_Y(t) \\
        &=e^{\lambda_1(e^t-1)}e^{\lambda_2(e^t-1)} \\
        &=e^{(\lambda_1+\lambda_2)(e^t-1)}
    \end{align*}
    which is the moment generating function of $\Poi(\lambda_1+\lambda_2)$.

    Therefore $X+Y\sim\Poi(\lambda_1+\lambda_2)$.
\end{example}

\begin{example}
    Let $N$ be a random variable taking non-negative integer values.
    Let $X_1,X_2,X_3,\ldots$ be i.i.d random variables. 

    Let $Y=\sum_{i=1}^NX_i$. What is the distribution of $Y$?
    \begin{align*}
        M_Y(t)&=\E[e^{tY}] \\
        &=\E\left[e^{t\sum_{i=1}^NX_i}\right]
    \end{align*}

    To evaluate this, we condition on the value of $N$:
    $$\E[e^{tY}]=\E\left[\E\left[e^{tY}|N\right]\right].$$
    Compute \begin{align*}
        \E\left[e^{tY}|N=n\right]&=\E\left[e^{t\sum_{i=1}^nX_i}\right] \\
        &=\prod_{i=1}^n\E\left[e^{tX_i}\right]^n \tag{independence} \\
        &=M_X(t)^n
    \end{align*}
    where $X_i\sim X$.

    Therefore $$M_Y(t)=\E[e^{tY}]=\E\left[M_X(t)^N\right]$$
        \begin{align*}
            \Rightarrow \E[Y]&=M_Y'(0)=\E\left[\left.\diff{t}M_X(t)^N\right|_{t=0}\right] \\
            &=\E[N\cdot \underbrace{M_X(0)^{N-1}}_{-1}\cdot\underbrace{M_X'(0)}_{\E[X]}] \\
            &=\E[N\E[X]]=\E[N]\E[X] \\
        \end{align*}
\end{example}

\chapter{Limit Theorems} \label{part::limit}

Let $X$ be a random variable, 
and let $X_1,X_2,\ldots$ be i.i.d random variables with the same distribution as $X$.

Define $S_n=\frac{1}{n}\sum_{i=1}^nX_i$. What can we say about $S_n$?

There are two types of results: \begin{itemize}
    \item ``Law of Large Numbers'':
    We might see that $S_n$ is ``close'' to $\E[X]$.
    \item ``Central Limit Theorem'':
    $S_n$ is ``close'' to a normal distribution.
\end{itemize}

\begin{theorem}[Weak Law of Large Numbers]
    Let $X_1,X_2,\ldots$ be i.i.d random variables with $\E[X_i]=\mu$ and $\Var(X_i)=\sigma^2$.

    Let $S_n=\frac{1}{n}\sum_{i=1}^nX_i$.
    Then for any $\varepsilon>0$,
    $$\P\left(\left|S_n-\mu\right|>\varepsilon\right)\to0\text{ as }n\to\infty.$$
\end{theorem}
\begin{proof}
    It is easy to see that $\E[S_n]=\mu$ by LoE.

    Also, \begin{align*}
        \Var(S_n)&=\Var\left(\frac{1}{n}\sum_{i=1}^nX_i\right) \\
        &=\frac{1}{n^2}\Var\left(\sum_{i=1}^nX_i\right) \\
        &=\frac{1}{n^2}\sum_{i=1}^n\Var\left(X_i\right)=\frac{\sigma^2}{n}
    \end{align*}

    Then by Chebychev's inequality,
    \begin{align*}
        \P\left(\left|S_n-\mu\right|>\varepsilon\right)&=\P\left(\left|S_n-\E[S_n]\right|>\varepsilon\right) \\
        &\leq\frac{\Var(S_n)}{\varepsilon^2} \\
        &=\frac{\sigma^2}{n\varepsilon^2}\to0\text{ as }n\to\infty
    \end{align*}

    This completes the proof.
\end{proof}

\begin{remark}
    In fact, we do not need the variance to be finite. (more complicated proof)
\end{remark}

\begin{theorem}[Strong Law of Large Numbers]
    Let $X_1,X_2,\ldots$ be i.i.d random variables with $\E[X_i]=\mu$ and $\E[X_i^4]=K<\infty$.

    Then if $S_n=\frac{1}{n}\sum_{i=1}^nX_i$, we have
    $$\P(S_n\to\mu\text{ as }n\to\infty)=1.$$ % $$\P\left(\lim_{n\to\infty}S_n=\mu\right)=1.$$    
\end{theorem}
\begin{proof}
    Case: $\mu=0$.

    Define $Y_n=\sum_{i=1}^{n}X_i$. Then $$Y_n^4=\left(\sum_{i=1}^{n}X_i\right)^4=\sum_{1\leq i,j,k,l\leq n}X_iX_jX_kX_l$$
    $$\E[Y_n^4]=\sum_{i,j,k,l}\E[X_iX_jX_kX_l].$$
    Observation. By independence, if one term $\E[X_iX_jX_kX_l]$ has one index of $i,j,k,l$ different from others, 
    then the term $=0$.

    Therefore the only surviving terms are $\E[X_i^4]$ and $\E[X_i^2X_j^2]$.
    Compute
    \begin{align*}
        \E[Y_n^2]&=\sum_{i=1}^{n}\E[X_i^4]+\sum_{i<j}6\E[X_i^2X_j^2] \\
        &=n\E[X_1^4]+6\binom{n}{2}\E[X_1^2]^2 \\
        &\leq n\E[X_1^4]+6\binom{n}{2}\E[X_1^4] \\
        &\leq 4n^2K
    \end{align*}

    Let $E_n$ be the event \begin{align*}
        \set{\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}\geq\frac{1}{\log n}}
        &=\set{\abs{\frac{1}{n}Y_n}\geq\frac{1}{\log n}} \\
        &=\set{\abs{Y_n}\geq\frac{n}{\log n}} \\
        &=\set{Y_n^4\geq\frac{n^4}{\log^4 n}}.
    \end{align*}

    By Markov's inequality, \begin{align*}
        \P(E_n)&=\P\left(Y_n^4\geq\frac{n^4}{\log^4 n}\right) \\
        &\leq\frac{\E[Y_n^4]}{\frac{n^4}{\log^4 n}} \\
        &\leq\frac{4Kn^2}{\frac{n^4}{\log^4 n}}=\frac{4K\log^4 n}{n^2}.
    \end{align*}

    Since $\sum_{n=1}^\infty\P(E_n)\leq\sum_{n=1}^\infty\frac{4K\log^4 n}{n^2}<\infty$,
    by Borel-Cantalli, $\P\left(\limsup_{n\to\infty}E_n\right)=0$.
    i.e., only finitely many of the events $E_n$ occur.

    Therefore, with probability 1, there is some $N\in\N$ such that for all $n\geq N$, $E_n$ does not hold.
    That is, $\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}\leq\frac{1}{\log N}$ for all $n\geq N$,
    and thus $\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}\to 0=\mu$.

    Case: general case $\mu\neq0$.

    Let $X_i'=X_i-\mu$. Then $X_1',X_2',\ldots$ are i.i.d with mean 0 and $\E[(X_i')^4]<\infty$.

    By the above case, we have $\P\left(\frac{1}{n}\sum_{i=1}^{n}X_i'\to0\right)=1$,
    and thus $\P\left(\frac{1}{n}\sum_{i=1}^{n}X_i\to\mu\right)=1$.
\end{proof}

\begin{remark}
    We only need pairwise independence in WLLN and 4-wise independence in SLLN.
\end{remark}

\begin{theorem}[Central Limit Theorem]
    Let $X_1,X_2,\ldots$ be i.i.d random variables with $\E[X_i]=\mu$ and $\Var(X_i)=\sigma^2$.
    Then $\frac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}$ (pointwisely) tends to the standard normal distribution as $n\to\infty$.

    That is, for any $a\in\R$, $$\P\left(\frac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\leq a\right)\to\Phi(a)\text{ as }n\to\infty.$$
\end{theorem}

For our proof, we will show that \begin{enumerate}
    \item if the moment generating function $M_{X_i}(t)$ exists, so does that of $\frac{X_i}{\sqrt{n}}$,
    \item use the fact that if we have a sequence of random variables $Z_i$, with mgfs $M_{Z_i}(t)$
    and cdfs $F_{Z_i}(z)$, and a random variable $Z$ with mdf $M_Z(t)$ and cdf $F_Z(z)$,
    if $M_{Z_i}(t)\to M_Z(t)$ for all $t$, then $F_{Z_i}(t)\to F_Z(t)$ where $F_Z(t)$ is continuous.
\end{enumerate}

\begin{proof}
    Case: $\mu=0$, $\sigma^2=1$.

    Let $M$ be the moment generating function for $X_i$. Then $M(t)=\E[e^{tX}]$.

    Then the moment generating function of $\frac{X_i}{\sqrt{n}}$ is $\E\left[e^{\frac{tX_i}{\sqrt{n}}}\right]=M\left(\frac{t}{\sqrt{n}}\right)$.

    Therefore the moment generating function of $\frac{X_1+X_2+\cdots+X_n}{\sqrt{n}}$ is
    $$\E\left[e^{t\cdot\frac{X_1+X_2+\cdots+X_n}{\sqrt{n}}}\right]=\E\left[e^{t\frac{X_1}{\sqrt{n}}}e^{t\frac{X_2}{\sqrt{n}}}\cdots e^{t\frac{X_n}{\sqrt{n}}}\right]=M\left(\frac{t}{\sqrt{n}}\right)^n$$

    Our goal is to show that $$M\left(\frac{t}{\sqrt{n}}\right)^n\to M_Z(t)e^{\frac{t^2}{2}}\text{ for all }t,$$
    where $M_Z(t)=e^{\frac{t^2}{2}}$ is the moment generating funtion of $Z\sim N(0,1)$.

    Taking natural logarithm, it suffices to show that $n\log M\left(\frac{t}{\sqrt{n}}\right)\to\frac{t^2}{2}$.

    Define $L(t)=\log M(t)$. We want to show that 
    $nL\left(\frac{t}{\sqrt{n}}\right)=\frac{L\left(\frac{t}{\sqrt{n}}\right)}{n^{-1}}\to\frac{t^2}{2}$.

    Note that \begin{align*}
        L(0)&=\log M(0)=\log M(1)=0 \\
        L'(t)&=\diff{t}\log M(t)=\frac{M'(t)}{M(t)} \\
        &\Rightarrow L'(0)=\frac{\mu}{1}=0 \\
        L''(0)&=\frac{M''(t)M(t)-M'(t)^2}{M(t)^2} \\
        &\Rightarrow L''(0)=\frac{(\sigma^2+\mu^2)1-\mu^2}{1}=1
    \end{align*}

    Therefore \begin{align*}
        \lim_{n\to\infty}\frac{L\left(\frac{t}{\sqrt{n}}\right)}{n^{-1}}
        &=\lim_{n\to\infty}\frac{\diff{n}L\left(\frac{t}{\sqrt{n}}\right)}{\diff{n}n^{-1}} \tag{L'Hopital} \\
        &=\lim_{n\to\infty}\frac{L'\left(\frac{t}{\sqrt{n}}\right)\cdot\frac{\frac{1}{2}t}{n^{\frac{3}{2}}}}{-n^{-2}} \\
        &=\lim_{n\to\infty}\frac{tL'\left(\frac{t}{\sqrt{n}}\right)}{2n^{-\frac{1}{2}}} \\
        &=\lim_{n\to\infty}\frac{\diff{n}tL'\left(\frac{t}{\sqrt{n}}\right)}{\diff{n}2n^{-\frac{1}{2}}} \tag{L'Hopital} \\
        &=\lim_{n\to\infty}\frac{L''\left(\frac{t}{\sqrt{n}}\right)\cdot\frac{-\frac{1}{2}t^2}{n^{\frac{3}{2}}}}{2\cdot-\frac{1}{2}n^{-\frac{3}{2}}} \\
        &=\lim_{n\to\infty}\frac{t^2L''\left(\frac{t}{\sqrt{n}}\right)}{2}=\frac{t^2}{2}
    \end{align*}

    Hence $M\left(\frac{t}{\sqrt{n}}\right)\to e^{\frac{t^2}{2}}$.
    Thus, by our face, if $\E[X_i]=0$ and $\Var(X_i)=1$, for any $a$, 
    $$\P\left(\frac{X_1+X_2+\cdots+X_n}{\sqrt{n}}\leq a\right)\to\Phi(a)\text{ as }n\to\infty.$$

    Case: general case.

    Let $Y_i=\frac{X_i-\mu}{\sigma}$. Then $\E[Y_i]=0$ and $\Var(Y_i)=1$.

    Therefore $\P\left(\frac{Y_1+Y_2+\cdots+Y_n}{\sqrt{n}}\leq a\right)\to\Phi(a)$ as $n\to\infty$
    and thus $\P\left(\frac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\leq a\right)\to\Phi(a)$ as $n\to\infty$.
\end{proof}

\begin{remark}
    \begin{enumerate}[label=(\arabic*)]
        \item We showed that for any $a\in\R$, $\P\left(\frac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\leq a\right)\to\Phi(a)$ as $n\to\infty$.
        One can show the convergence is uniform in $a$.
        \item The CLT applies more generally.
        Let $X_1,X_2,\ldots$ be independent random variables such that \begin{enumerate}[label=\arabic*.]
            \item there exists $m<\infty$ such that $\P(|X_i|\leq m)=1$ for all $i$,
            \item $\sum_{n=1}^\infty\Var(X_n)=\infty$.
        \end{enumerate}
        Then $$\frac{(X_1-\E[X_1])+(X_2-\E[X_2])+\cdots+(X_n-\E[X_n])}{\sqrt{\sum_{i=1}^n\Var(X_i)}}$$
        tends to $N(0,1)$ in distribution.
        \item The convergence is typically very fast.
    \end{enumerate}
\end{remark}

\begin{example} \label{nomral_approx}
    Normal approximation to the binomial.

    Let $X\sim\Bin(n,p)$ be a sum of $n$ i.i.d Bernoulli variables with $\E[X_i]=p$ and $\Var(X_i)=p(1-p)$.

    By the CLT, $\P\left(\frac{X_1+X_2+\cdots+X_n-np}{\sqrt{np(1-p)}}\leq a\right)\to\Phi(a)$.
    That is, $X$ is approximated by $N(np,np(1-p))$.
\end{example}

\begin{example}
    A fair die is rolled 10 times. What is the probability that the sum lies in $[30,40]$? 

    \noindent \textbf{Solution.} Let $X_i$ be the outcome of the $i$-th roll. 
    Then $X_i$'s are i.i.d random variables with $\E[X_i]=\frac{7}{2}$
    and $\Var(X_i)=\frac{35}{12}$.

    By the CLT, $\frac{X_1+X_2+\cdots+X_{10}-35}{\sqrt{\frac{350}{12}}}$ tends to $N(0,1)$.

    Therefore \begin{align*}
        \P(30\leq X_1+\cdots+X_{10}\leq 40)&=\P(29.5\leq X_1+\cdots+X_{10}\leq 40.5) \\
        &=\P\left(\frac{29.5-35}{\sqrt{\frac{350}{12}}}\leq\frac{X_1+\cdots+X_{10}-35}{\sqrt{\frac{350}{12}}}\leq\frac{40.5-35}{\sqrt{\frac{350}{12}}}\right) \\
        &\approx\Phi\left(\frac{5.5}{\sqrt{\frac{350}{12}}}\right)-\Phi\left(\frac{-5.5}{\sqrt{\frac{350}{12}}}\right) \\
        &=2\Phi\left(\frac{5.5}{\sqrt{\frac{350}{12}}}\right)-1\approx\fbox{0.692}
    \end{align*}
\end{example}

\chapter{The Probabilistic Method in Extremal Combinatorics} \label{part::prob_method}

What is Comvinatorics? It is the study of ``discrete structures''.
For example, graphs, set systems, number theory, etc.

What is extremal combinatorics? It asks the question:
How big/small can structures be if it has a certain property?

For example, how many edges can an $n$-vertex graph have if you can draw it without crossing edges?

Let $f(n)$ be $\max\set{|E(G)|:G\text{ is an $n$-vertex planar graph}}$. In fact, $f(n)=3n-6$.

\section{Sum-free Sets}

In a game, the foal is to choose a set of integers from $\set{1,2,\ldots,n}$ 
as possible without a solution to $x+y=z$.

\noindent \underline{Q.} How large can a sum-free subset of $\set{1,2,\ldots,n}$ be?

For example, $A=\set{1,3,5,7,\ldots}$ or $A=\set{\floor{\frac{n}{2}}+1,\floor{\frac{n}{2}}+2,\ldots,n}$, 
$|A|\approx\frac{n}{2}$. This gives a lower bound. We can find an upper bound:

\begin{claim}
    If $A$ is a sum-free subset of $\set{1,2,\ldots,n}$, then $|A|\leq\frac{n+1}{2}$.
\end{claim}
\begin{proof}
    Let $m=\max A$. Consider the disjoint pairs $\set{1,m-1}$, $\set{2,m-2}$, $\ldots$, $\set{\floor{\frac{m}{2}},\ceil{\frac{m}{2}}}$.

    Since $A$ is sum-free, we have at most one element from each pair.

    Therefore $A\exc\set{m}\leq$ number of pairs $=\floor{\frac{m}{2}}$,
    and thus $$|A|\leq\floor{\frac{m}{2}}+1=\ceil{\frac{m+1}{2}}\leq\ceil{\frac{n+1}{2}}.$$
\end{proof}

Now we play a REAL game. Given a set $S\inc\N$, $|S|=n$. The goal is to find the largest sum-free subset of $S$.

\begin{definition}
    Let $\sigma(S)=\max\set{|A|:A\inc S\text{ is sum-free}}$.
\end{definition}

For example, $\sigma(\set{1,2,\ldots,n})\approx\frac{n}{2}$, and $\sigma(\set{1,3,9,27,\ldots,3^{n-1}})=n$.

In a two-player game, the first player chooses a set $S$ and the second player finds a large sum-free subset $A$.
We want to find a strategy for the first player.

\begin{definition}
    Let $\sigma(n)=\min\set{\sigma(S):S\inc\N,|S|=n}$.
\end{definition}

From the above discussion, we have an upper $\sigma(n)\leq\frac{n}{2}$.

\begin{theorem}[Erd\Ho{o}s,1965]
    $\sigma(n)\geq\frac{1}{3}(n+1)$.
\end{theorem}

\noindent \underline{The probabilistic method.} Erd\Ho{o}s's brilliant insight:
To provide a lower (upper) bound for a max (min) problem,
you do not need to construct an example, you only need to prove an example exists. 

Consider a random construction. If $\P(\set{\text{construction has good properties}})>0$ then such a construction exists.

\begin{proof}
    Idea: Given a set $S\inc\N$, $|S|=n$, 
    \begin{itemize}
        \item consider a random subset $A\inc S$,
        \item show that with positive probability, \begin{itemize}
            \item $|A|\geq\frac{1}{3}(n+1)$,
            \item $A$ is sum-free.
        \end{itemize}
    \end{itemize}

    This will imply $\sigma(n)\geq\frac{1}{3}(n+1)$.

    Attempt: Choose every element $s\in S$ to be in $A$ with probability $p$.
    Then $|A|\sim\Bin(n,p)$, and thus $\E[|A|]=np$.

    Is $A$ sum-free? Let $X=\#\set{\text{sums $x+y=z$ in $A$}}$ and $X=\sum X_{x,y}$ where 
    $$X_{x,y}=\begin{cases}
        1 & \text{if }x,y\in A\text{ and }x+y\in A \\
        0 & \text{otherwise.}
    \end{cases}$$

    If $x\neq y$, then $\P(X_{x,y}=1)\leq p^3$.
    If $x=y$, then $\P(X_{x,y}=1)\leq p^2$.
    We can compute $$\E[X]=\sum\E[X_{x,y}]=\sum\P(X_{x,y}=1)=\sum_{x\neq y}\P(X_{x,y}=1)+\sum_{x}\P(X_{x,x}=1)\leq n^2p^3+np^2.$$

    \noindent Observation. If $\E[X]\leq1$, then $\P(X=0)>0$.

    We want $n^2p^3+np^2<1$, so we need $p<n^{-\frac{2}{3}}$. Then $\E[|A|]\leq np\leq n^{\frac{1}{3}}$, which is too small.
    We need a different model for a random subset.

    \noindent Observation. If $q=3k+2$ is a prime, then $\set{k+1,k+2,\ldots,2k+1}$ is sum-free in $\Z_q$.

    \begin{corollary}
        The projection $\pi:S\to\Z_q$ satisfies that $\pi^{-1}(\set{k+1,k+2,\ldots,2k+1})$ is a sum-free subset in $S$.
    \end{corollary}
    Let $A=\set{s\in S:xs\mod q\in\set{k+1,k+2,\ldots,2k+1}}$, where $x\in\Z_q\exc\set{0}$ 
    is chosen uniformly at random.
        
    \noindent Observation. $$xs\mod q=\begin{cases}
        0 & \text{if }s\equiv 0\mod q \\
        \text{uniformly in }\set{1,2,\ldots,q-1} & \text{otherwise}. \\
    \end{cases}$$

    Given $s\in S$, $s\not\equiv0\mod q$, 
    $$\P(xs\mod q\in\set{k+1,k+2,\ldots,2k+1})=\frac{|\set{k+1,\ldots,2k+1}|}{|\set{1,\ldots,3k+1}|}>\frac{1}{3}.$$

    \begin{align*}
        \E[|A|]&=\E\left[\sum_{s\in S}X_s\right]\quad\text{ where }X_s=1\text{ if }s\in A \\
        &=\sum_{s\in S}\E\left[X_s\right]=\sum_{s\in S}\P(s\in A) \\
        &=\sum_{s\in S}\P(xs\mod q\in\set{k+1,\ldots,2k+1}) \\
        &=\sum_{s\in S,s\not\equiv0\mod q}\P(xs\mod q\in\set{k+1,\ldots,2k+1}) \\
        &>\frac{1}{3}\abs{\set{s\in S:s\not\equiv0\mod q}}
    \end{align*}

    Choose $q>\max S$. Then $\E[|A|]>\frac{1}{3}|S|$.
    In particular, there is a choice of $x$ that gives $A\inc S$, $|A|>\frac{1}{3}n$.
    Thus $|A|\geq\frac{1}{3}(n+1)$ and this $A$ is guaranteed to be sum-free.
\end{proof}

History: \begin{align*}
    &\sigma(n)\geq\frac{1}{3}(n+1) &&\text{ [Erd\Ho{o}s,1965]} \\
    &\sigma(n)\geq\frac{1}{3}(n+2) &&\text{ [Bourgain,1997]} \\    
    &\sigma(n)\leq\frac{1}{2}n \\
    &\qquad\vdots \\
    &\sigma(n)\leq\left(\frac{1}{3}+o(1)\right)n &&\text{ [Eberhard, Green, Manners, 2014]}
\end{align*}

\section{Ramsey Theory}

\noindent \underline{Problem.} Given $k$, what is the largest $R(k)$
such that we can red/blue- color the edges of a complete graph on $R(k)$ vertices
without a subset of $k$ vertices where edges are all the same?

History: $R(3)=5$, $R(4)=17$, $42\leq R(5)\leq45$.
$$R(k)<\infty.\quad\text{Ramsey, 1930}$$
\begin{align*}
    c\sqrt{2}^k\overtext{$\substack{\text{Erd\Ho{o}s} \\ (1947)}$}{\leq}&R(k)\overtext{$\substack{\text{Erd\Ho{o}s, Szelens} \\ (1935)}$}{\leq}4^k \\
    2c\sqrt{2}^k\overtext{$\substack{\text{Spencer} \\ (197?)}$}{\leq}&R(k)\overtext{$\substack{\text{Campos, Coffin,} \\ \text{Moins, Schasrabudhe} \\ (2022)}$}{\leq}(4-\varepsilon)^k=3.7\ldots^k
\end{align*}

\begin{theorem}
    There exists $c>0$ such that for all $k\in\N$, $R(k)\geq c\sqrt{2}^k$.
\end{theorem}
\begin{proof}
    Idea: Take $N$ vertices, and color each edge randomly, independently.

    For every set $S$ of $k$ vertices, let $E_S$ be the event that all edges in $S$ are the same color.

    $$\P(E_S)=2\cdot\left(\frac{1}{2}\right)^{\binom{k}{2}}$$
    $$\P(\exists\text{ monocromatic $k$-set})=\P\left(\bigcup_SE_S\right)\leq\sum_S\P(E_S)$$
    \begin{align*}
        \P(\text{coloring is bad})&\leq\binom{N}{k}\cdot2\cdot\left(\frac{1}{2}\right)^{\binom{k}{2}}<2N^k\left(\frac{1}{2}\right)^{\frac{1}{2}k(k-1)} \\
        &2\cdot\left(\frac{N}{2^{\frac{k-1}{2}}}\right)^k\to0\quad\text{ as $k\to\infty$ if }N<2^{\frac{k-1}{2}}
    \end{align*}
    Therefore $\P(\text{coloring is good})\to1$ and thus good coloring exists.
\end{proof}

There are some major open problems:
\begin{enumerate}
    \item Determine the correct base of the exponent.
    \item Find a good coloring on $1.00000001^k$ vertices without using randomness.
\end{enumerate}

Property B. Situation: You are packing for a holiday.
To make sure there are no luggage mishaps, you decide to pack two suitcases of clothes.

Different categories of clothing:\\
\noindent \begin{minipage}[t]{0.3\textwidth}
\begin{itemize}[leftmargin=*]
  \item Tops
  \item Trousers
  \item Shorts
  \item Skirts
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\begin{itemize}[leftmargin=*]
  \item Underwear
  \item Formal
  \item Casual
  \item Beachwear
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\begin{itemize}[leftmargin=*]
  \item Red
  \item Blue
  \item Green\\[5pt]
\end{itemize}
\end{minipage}

Our goal is to assign clothes to suitcases in such a way that every category is represented in both suitcases.

We have one problem: we might have a category with only one item.

Assume each category has at least $k$ items.

More problems: For example, \begin{center}
    \begin{tikzpicture}[every node/.style={circle, draw, minimum size=5mm, inner sep=0pt}, node distance=1cm]
        % First row: illegal selection
        \node (a1) at (0,1.7) {};
        \node (a2) at (-1,0) {};
        \node (a3) at (1,0) {};
        \draw[thick] (-0.5,0.85) ellipse[x radius=1.8, y radius=0.8, rotate=60];
        \draw[thick] (0.5,0.85) ellipse[x radius=1.8, y radius=0.8, rotate=-60];
        \draw[thick] (0,0) ellipse[x radius=1.8, y radius=0.8];
    \end{tikzpicture}
\end{center}

\begin{definition}
    Given $k\geq2$, let $m(k)=\min\set{\substack{\text{no. of sets of size at least $k$,} 
    \\ \text{such that in any two-coloring of the elements,} \\ \text{one of the sets is monochromatic}}}$.
\end{definition}

For small $k$:
\begin{align*}
    &m(1)=1 \\
    &m(2)=3 \\
    &m(3)=7
\end{align*}

\begin{proposition}
    $m(k)\leq\binom{2k-1}{k}\approx\frac{c2^{2k}}{\sqrt{k}}$.
\end{proposition}
\begin{proof}
    Take $2k-1$ elements, and all subsets of size $k$.

    By the pigeonhole principle, in any 2-coloring, there will be $k$ elements of the same color.

    Since all $k$-sets are in our colleciton, there is a monochromatic set.
\end{proof}

\begin{theorem}[Erd\Ho{o}s, 1963]
    For all $k\geq1$, $m(k)\geq2^{k-1}$.
\end{theorem}
\begin{proof}
    Let $f$ be a collection of $m<2^{k-1}$ sets of size $k$.
    Color the elements uniformly, independently.

    For every set $F\in f$ in our collection, let $E_F$ be the (bad) events that it is monochromatic.
    $$\P(E_F)=2\cdot\left(\frac{1}{2}\right)^k$$ 
    \begin{align*}
        \P(\text{coloring is bad})&=\P\left(\bigcup_{F\in f}E_F\right) \\
        &\leq\bigcup_{F\in f}\P(E_F) \tag{union bound} \\
        &=m2^{1-k}<1 \tag{$m<2^{k-1}$}
    \end{align*}

    Therefore $\P(\text{coloring is good})>0$, and thus good coloring exists.
\end{proof}

\begin{theorem}[Erd\Ho{o}s, 1964]
    There exists $C>0$ such that for all $k\geq2$, $m(k)\leq Ck^22^k$.
\end{theorem}
\begin{proof}
    Consider a good set of $n$ elements to be determined color.

    Idea: Choose $m$ sets at random.

    Choose each set $S_i$ from the $\binom{n}{k}$ possibilities uniformly, independently.

    The goal is that every subset of $\frac{n}{2}$ elements contains one of our tandom sets,
    so in every coloring, the more popular color contains a set.

    For every set $X$ of $\frac{n}{2}$ elements, define the (bad) event 
    $$E=\set{\text{none of our $m$ random sets }S_i\inc X}.$$
    $$\P(S_i\inc X)=\frac{\binom{\frac{n}{2}}{k}}{\binom{n}{k}}\approx\frac{\frac{\left(\frac{n}{2}\right)^k}{k!}}{\frac{n^k}{k!}}=\frac{1}{2^k}.$$
    \begin{align*}
        \P(E_X)&=\P(\forall i,S_i\not\inc X) \\
        &=\prod_{i=1}^m\P(S_i\inc X) \\
        &=\left(1-\frac{\binom{\frac{n}{2}}{k}}{\binom{n}{k}}\right)^m \\
        &\approx\left(1-\frac{1}{2^k}\right)^m\approx e^{-\frac{m}{2^k}}
    \end{align*}
    
    Therefore \begin{align*}
        \P\left(\substack{\text{there is a 2-coloring of the elements} \\ \text{with no $S_i$ monochromatic}}\right)
        &\leq\P\left(\bigcup_XE_X\right) \\
        &\leq\sum_X\P(E_X)=\binom{n}{\frac{n}{2}}\left(1-\frac{\binom{\frac{n}{2}}{k}}{\binom{n}{k}}\right)^m \\
        &\leq 2^ne^{-\frac{m}{2^k}}<1\quad\text{ if }m>2^kn\ln 2
    \end{align*}

    Thus if $m>2^kn\ln 2$, $\P(\text{exists 2-coloring})>0$.
    Hence there exists a collection of $m$ sets with no 2-coloring, and $m(k)\leq m$.

    If we do the colorlatices coefectly, we need $n\geq Ck^2$.
\end{proof}

Now we have $2^{k-1}\leq m(k)\leq Ck^22^k$. The lower bound was improved:

\begin{theorem}[Beck]
    $m(k)\geq ck^{\frac{1}{3}}2^k$.
\end{theorem}
\begin{theorem}[Radhakrishnan, Srinivasan (2000)]
    $m(k)\geq ck^{\frac{1}{2}}2^k$.
\end{theorem}

In 1992, an algorithm is developed:
\begin{theorem}[Pluhár (1992)]
    $m(k)\geq ck^{\frac{1}{4}}2^k$ via a random greedy algorithm.
\end{theorem}
\begin{theorem}[Cherkashin, Kozak (2015)]
    $m(k)\geq ck^{\frac{1}{2}}2^k$ (same algorithm).
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}